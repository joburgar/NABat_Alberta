---
title: "Alberta's North American Bat Monitoring Program (NABat) Annual Report"
author: "Created by Joanna Burgar"
date: "Report generated on `r format(Sys.time(), '%d %B, %Y')`"
output: word_document
---

```{r EPFU image, out.width="0.3\\linewidth", include=TRUE, fig.align="center", echo=FALSE}
knitr::include_graphics("./Input/JasonHeadley-LittleBrownBat2.jpg")
```

Prepared by: Joanna M. Burgar, PhD RPBio
 joburgar@gmail.com 

Prepared for: Alberta Environment & Parks

Suggested Citation: Burgar, J.M. 2024. North American Bat Monitoring Program 2023. Alberta Environment & Parks, Edmonton, Alberta. 

Cover Illustration: Myotis lucifigus - Little Brown Bat Â© Jason Headley

```{r setup, echo=F, include=F}

#Load Packages
list.of.packages <- c("data.table", "leaflet", "tidyverse", "lunar", "zoo", "colortools", "lubridate", "camtrapR", "circular", "RColorBrewer", "Cairo", "viridis", "knitr", "sf","osmdata", "ggspatial", "ggmap","gridExtra", "grid")
# Check you have them and load them
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

lapply(list.of.packages, require, character.only = TRUE)

# Timezone 
tz <- "MST7MDT"

# Set a single catagorical variable of interest from station covariates (`sta`) for summary graphs. If you do not have and appropriate category use "Project.ID".
category <- "GRTS.Cell.ID"

# Define a colour from the R options to base the colourscheme
colour <- "lightseagreen"

# # Define the GRTS.Cell.ID and Year of interest if subsetting for year, studyarea
# GRTS_interest <- c("72071","203655")

Year_interest <- year(as.Date("2023-01-01"))

# Check the listing of output files
# fs::dir_ls(path="./Input/NABat_ProcessedFiles", recurse = TRUE)

# Import count files
# dat_count <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles",GRTS_interest,sep="/"),
#                           regexp = "\\counts.csv$", recurse = TRUE)

# # Import count files if selecting only GRTS.Cell.ID of interest
# make the changes below to add in GRTS_interest and/or Year_interest for other files as appropriate
# dat_count <- fs::dir_ls(path="./Input/NABat_ProcessedFiles",regexp = "\\counts.csv$", recurse = TRUE) %>%
#   map_dfr(read_csv, col_types = cols(.default = 'c'), .id = "source") %>% 
#   select(-`...3`) %>%
#   type_convert() %>%
#   mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
#   mutate(Year = year(SurveyNight), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
#   filter(Year==Year_interest)
# 
# dat_count$Orig.Name <- as.factor(paste(dat_count$Site, dat_count$Detector, sep="_"))
# dat_count$GRTS.Cell.ID <- as.factor(word(dat_count$source,4,sep = "\\/"))
# dat_count$Location.Name <- as.factor(word(dat_count$source,5,sep = "\\/"))
# dat_count$Deployment.ID <- as.factor(word(dat_count$source,6,sep = "\\/"))
# dat_count$Classification <- as.factor(dat_count$Classification %>% 
#                                         recode(EPFU.LANO = "EPFU-LANO", LABO.MYLU = "LABO-MYLU",
#                                                My_40k = "Myotis 40k", MYEV.MYSE = "MYEV-MYSE"))
# dat_count$Classification <- factor(dat_count$Classification,
#                                     levels = c("EPFU", "EPFU-LANO", "LANO", "LACI", "LABO", "LABO-MYLU", "MYLU",
#                                                "MYCA", "MYCI", "MYEV","MYEV-MYSE", "MYSE", "MYVO", "Myotis 40k",
#                                                "unknown", "noise"))
# # dat_count <- dat_count %>% filter(GRTS.Cell.ID==GRTS_interest)

# Import summary files
dat_summary <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles/"), 
                            regexp = "\\_summary.csv$", recurse = TRUE) %>%
  map_dfr(~read_csv(.x, col_types = cols(.default = "c")), .id="source") %>% # in as character to deal with potential mismatch file types
  dplyr::select(Site, Filename, n_calls, prob1, sp1, prob2, sp2, prob3, sp3, source,Date) %>% # only select necessary columns
  type_convert() %>% # convert back to proper formats
  mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-Date)%>%
  filter(Year==Year_interest) %>%
  rename(Orig.Name=Site)

glimpse(dat_summary)

dat_summary$GRTS.Cell.ID <- as.factor(word(dat_summary$source,4,sep = "\\/"))
dat_summary$Location.Name <- as.factor(word(dat_summary$source,5,sep = "\\/"))
dat_summary$Deployment.ID <- as.factor(word(dat_summary$source,6,sep = "\\/"))

# Create file type to differentiate zc from wav 
dat_summary$File.Type <- str_sub(dat_summary$Filename,-2,-1) %>% recode("0#" = "zc", "av"="wav", "[0-9]{1}#" = "zc")
dat_summary <- dat_summary %>% mutate(File.Type = case_when(grepl("#", File.Type) ~ "zc",
                                                            TRUE ~ as.character(File.Type)))

# Extract the time from filename and put in date format
dat_summary$Time.temp1 <- str_extract(dat_summary$Filename,"_[0-9]{6}.wav") %>% str_sub(2,7)
dat_summary$Time.temp2 <- str_extract(dat_summary$Filename,"[0-9]{2}\\-[0-9]{2}\\-[0-9]{2} [0-9]{2}\\-[0-9]{2}\\-[0-9]{2}") %>% str_sub(-8,-1)
dat_summary$Time.temp3 <- str_extract(dat_summary$Filename,"_[0-9]{6}_") %>% str_sub(2,7)
dat_summary$Time.temp4 <- str_extract(dat_summary$Filename,"_[0-9]{6}.00#.zc") %>% str_sub(2,7)
dat_summary$Time.temp5 <- str_extract(dat_summary$Filename,"_[0-9]{6}.zc") %>% str_sub(2,7)
dat_summary$Time.temp6 <- str_extract(dat_summary$Filename,"_[0-9]{6}.00#") %>% str_sub(2,7)
dat_summary$Time.temp7 <- dat_summary$Filename %>% str_sub(9,15)
dat_summary$Time.temp8 <- str_extract(dat_summary$Filename,"_[0-9]{2}\\-[0-9]{2}\\-[0-9]{2}.wav") %>% str_sub(2,9)
dat_summary$Time.temp9 <- str_extract(dat_summary$Filename,"_[0-9]{5}.wav") %>% str_sub(2,6)

dat_summary$Time.temp1p <- as.POSIXct(strptime(dat_summary$Time.temp1, "%H%M%S", tz))
dat_summary$Time.temp2p <- as.POSIXct(strptime(dat_summary$Time.temp2, "%H-%M-%S", tz))
dat_summary$Time.temp3p <- as.POSIXct(strptime(dat_summary$Time.temp3, "%H%M%S", tz))
dat_summary$Time.temp4p <- as.POSIXct(strptime(dat_summary$Time.temp4, "%H%M%S", tz))
dat_summary$Time.temp5p <- as.POSIXct(strptime(dat_summary$Time.temp5, "%H%M%S", tz))
dat_summary$Time.temp6p <- as.POSIXct(strptime(dat_summary$Time.temp6, "%H%M%S", tz))
dat_summary$Time.temp7p <- as.POSIXct(strptime(dat_summary$Time.temp7, "%H%M.%S", tz))
dat_summary$Time.temp8p <- as.POSIXct(strptime(dat_summary$Time.temp8, "%H-%M-%S", tz))
dat_summary$Time.temp9p <- as.POSIXct(strptime(dat_summary$Time.temp9, "%H%M%S", tz))

dat_summary <- dat_summary %>% mutate(Timep = coalesce(Time.temp1p, Time.temp2p, Time.temp3p, Time.temp4p, Time.temp5p,Time.temp6p, Time.temp7p, Time.temp8p, Time.temp9p)) %>% 
  select(-c(Time.temp1, Time.temp2, Time.temp3, Time.temp4, Time.temp5, Time.temp6,Time.temp7, Time.temp8, Time.temp9, Time.temp1p, Time.temp2p, Time.temp3p, Time.temp4p, Time.temp5p,Time.temp6p,Time.temp7p, Time.temp8p, Time.temp9p))
hist(dat_summary$Timep, breaks=60)

dat_summary %>% filter(is.na(Timep)) %>% select(Filename)
dat_summary$Timep <- ymd_hms(dat_summary$Timep, tz=tz)
dat_summary %>% count(GRTS.Cell.ID) # 73 GRTS cells
dat_summary %>% count(Location.Name) # 116 stations

dat_summary <- dat_summary %>% mutate(time.keep = case_when(Timep <= ymd_hms(paste(Sys.Date(),"06:00:00"), tz=tz) ~ "keep",
                                                            Timep >= ymd_hms(paste(Sys.Date(),"18:00:00"),tz=tz) ~ "keep",
                                                            TRUE ~ "cut"))

dat_summary %>% filter(Orig.Name=="OnefourPlayground") %>% count(time.keep) # most of OnefourPlayground will be cut
# 1 cut        3206
# 2 keep       1721
# cut for now, put in a note that the times are out of 'normal' bat range in the report

# dat_summary %>% filter(time.keep=="cut") %>% count(Orig.Name) %>% print(n=38)
# dat_summary %>% count(time.keep)
# dat_summary %>% group_by(time.keep) %>% count(Location.Name) %>% print(n=155)

hist(dat_summary[dat_summary$time.keep=="keep",]$Timep, breaks=60)
dat_summary <- dat_summary %>% filter(time.keep=="keep")

# changing SurveyNight to be the correct date (day before if after midnight)
dat_summary <- dat_summary %>% mutate(SurveyNight = case_when(Timep <= ymd_hms(paste(Sys.Date(),"18:00:00"),tz=tz) ~ as.Date(SurveyNight-1),
                                                            TRUE ~ as.Date(SurveyNight)))
# dat_summary %>% select(SurveyNight, Filename, Timep)

# Read deployment data csv for station covariates
eff <- read.csv(paste0("Input/NABat_Deployment_Data_",Year_interest,".csv"), header=T) 
eff <- eff %>% filter(Deployment.ID>=as.numeric(Year_interest)) #%>% filter(GRTS.Cell.ID==GRTS_interest)
nrow(eff) #124 stations surveyed in 2023 (does not include mobile)
eff %>% count(Detector.Failure.Details) # 116 good, 8 without data

eff.names <- eff %>% group_by(Location.Name) %>% count(Orig.Name)
dat.names <- dat_summary%>% group_by(Location.Name) %>% count(Orig.Name)
names.join <- as.data.frame(full_join(eff.names, dat.names, by="Location.Name"))
no.data <- names.join %>% filter(is.na(n.y)) # will show missing deployment data
names.join %>% filter(is.na(n.x)) 

eff %>% filter(Location.Name %in% no.data$Location.Name) %>% select(Location.Name, Orig.Name, Detector.Failure.Details) %>% as_tibble()
### NO DATA (i.e., equipment malfunction) for 6 sites and no metadata sent for 1 site
# Location.Name Orig.Name     Detector.Failure.Details          
# 1 2643_SW_01    "BAT-23-F-7080" "no data"                         
# 2 9626_NE_01    "Muskeg 2"      "equipment malfunction"           
# 3 148871_NE_01  "WoS"           "no data"                         
# 4 199047_NW_02  "MRNA-coulee  " "no data"                         
# 5 238163_NW_01  "BAT-23-F-7084" ""                                
# 6 313693_SW_01  "BAT-23-W-500"  "no data"                         
# 7 323478_SE_02  "BAT-23-W-502"  "no data"                         
# 8 335882_SW_01  "BAT-23-W-571"  "Have metadata, but no recordings"     

NABat.stns.to.use <- unique(eff$Location.Name)

# Read station covariates csv
sta <- read.csv("Input/NABat_Station_Covariates.csv", header=T, na.string=c("","NA", "<NA>",-1)) %>%
  dplyr::select(-GRTSCellID, -Orig.Name) %>%
  rename(Location.Name=LocName, Waterbody.Type=WB_type, Waterbody.Distance=WB.dist, Road.Type=RD_Type, Road.Distance=RD.dist, Natural.Region=NRNAME)

sta <- sta %>% filter(X2023==1)
nrow(sta) #124
sta$NP <- as.factor(sta$NP)
sta$NP <- sta$NP %>% relevel(ref="In")
sta$Land.Cover <- as.factor(sta$Land.Cover)

sta %>% filter(!Location.Name %in% NABat.stns.to.use) 
sta %>% count(Land.Cover)
# Land-Use Type (Agriculture, Barren Land, Forest, Grassland, Shrubland, Urban, Water, Wetland)
# not enough Urban, keep as "Developed"
# not enough Wetland for separate category - combine Water and Wetland
# combine 3 forest types for Forest
# if waterbody < 10 m, then change Land Use Type to Water, otherwise go with ABMI class
# Consolidate Land.Cover to Agriculture, Barren (Exposed) Land, Forest, Grassland, Shrubland, Developed (incl Urban), Water/Wetland 
sta$Land.Use.Type <- sta$Land.Cover %>% recode("Coniferous Forest" = "Forest-conifer", "Broadleaf Forest" = "Forest-deciduous", "Mixed Forest" = "Forest-mixed", "Exposed Land" = "Barren Land")
sta$Land.Use.Type <- case_when(sta$Waterbody.Distance < 10 ~ "Water",
                               TRUE ~ as.character(sta$Land.Use.Type))
sta$Land.Unit.Code <- recode(sta$LandUnitCo, "BNP"="BANP", "JNP"="JANP","LPR"="LOPR","LAR"="LOAR",     "NSR"="NOSR","RDR"="REDR","SSR"="SOSR","UAR"="UPAR","UPR"="UPPR")

sta <- sta %>% mutate(Land.Unit.Code = case_when(LUF_NAME=="Lower Athabasca" ~ "LOAR",
                                                 LUF_NAME=="Lower Peace" ~ "LOPR",
                                                 LUF_NAME=="North Saskatchewan" ~ "NOSR",
                                                 LUF_NAME=="Red Deer" ~ "REDR",
                                                 LUF_NAME=="South Saskatchewan" ~ "SOSR",
                                                 LUF_NAME=="Upper Athabasca" ~ "UPAR",
                                                 LUF_NAME=="Upper Peace" ~ "UPPR",
                                                  TRUE ~ as.character(Land.Unit.Code)))

eff <- left_join(eff %>% select(-Land.Unit.Code), 
                 sta %>% dplyr::select(Location.Name, Land.Unit.Code, Natural.Region, NP, Land.Use.Type))
eff %>% filter(Deployment.ID=="2023") %>% count(GRTS.Cell.ID) # 74 GRTS cells surveyed
nrow(eff %>% filter(Deployment.ID=="2023")) # 124 sites

sta.sites <- sta %>% select(Location.Name); nrow(sta.sites) #124

eff.sites <- eff %>% select(Location.Name, Orig.Name); nrow(eff.sites) #124
names.join <- as.data.frame(full_join(sta.sites, eff.sites))
eff.sites %>% filter(!Location.Name %in% sta.sites$Location.Name) # if 0 then all good to proceed

# Read distance matrix csv
dist.mat <- read.csv("DistanceMatrixTable_2023.csv", header=T)
dim(dist.mat)
dist.mat <- na_if(dist.mat, 0)
colnames(dist.mat)[1:nrow(sta)] <- sta$Location.Name
dist.mat$Location.Name <- sta$Location.Name

dist.mat$NP <- sta$NP[match(dist.mat$Location.Name, sta$Location.Name,)]
dist.mat$Natural.Region <- sta$Natural.Region[match(dist.mat$Location.Name, sta$Location.Name,)]
dist.mat$Land.Unit.Code <- sta$Land.Unit.Code[match(dist.mat$Location.Name, sta$Location.Name,)]
# dist.mat <- dist.mat[complete.cases(dist.mat),] # removes any entries without corresponding sta data

# Read covariate proportion csv
cov.prop <- read.csv("Input/Cov.area.csv", header=T)

##############################################################
##### DATA TESTS #############################################
##############################################################

# This code will not work unless the data passes the following checks

# 1) All dates must be in YYYY-MM-DD in 'eff' and YYYY-MM-DD HH:MM:SS in 'dat' 
# If either of the following return NA, the formatting needs to change
strptime(dat_summary$SurveyNight[1], "%Y-%m-%d", tz="UTC")

# 2) Ensure Deployment.ID (year) is the same as SurveyNight year
dat_summary$unique <- paste(dat_summary$Location.Name, dat_summary$Year, sep="_")

dat_summary <- dat_summary %>%  
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-unique)

dat_summary %>% group_by(Deployment.ID) %>% count(Year) # no discrepancy between SurveyNight and Deployment.ID
# as.data.frame(dat_summary %>% filter(is.na(Year)))

# 3a) Do you have all stations represented in both the deployment data and station covariates? If yes, the value should be 0
# If length > 0, then you have some data missing!
length(setdiff(unique(eff$Location.Name), unique(sta$Location.Name))) # 0
missing.eff.sta <- left_join(eff, sta, by="Location.Name") #%>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.eff.sta[is.na(missing.eff.sta$Orig.Name),] # will show missing station covariates

# 3b) Do you have all data represented in the station covariates? If yes, the value should be 0
length(setdiff(unique(sta$Location.Name), unique(dat_summary$Location.Name))) # 7
missing.dat.sta <- left_join(sta, dat_summary, by="Location.Name") %>% dplyr::select(Location.Name, Orig_Name, Orig.Name)
missing.data.names <- missing.dat.sta[is.na(missing.dat.sta$Orig.Name),]  # will show missing count data (may just not have data for certain years) # same 8 as above

sta %>% filter(Location.Name %in% missing.data.names$Location.Name) %>% select(Location.Name, Orig_Name)

sta %>% count(Location.Name) #124 stations (7 with malfunctions or no files sent)
sta %>% count(GRTS.Cell.ID) # 74

# if number of GRTS differ between sta and eff check using the code below
# sta.GRTS <- sta %>% count(GRTS.Cell.ID) %>% select(GRTS.Cell.ID)
# sort(sta.GRTS$GRTS.Cell.ID)
# eff.GRTS <- eff %>% count(GRTS.Cell.ID) %>% select(GRTS.Cell.ID)
# sort(eff.GRTS$GRTS.Cell.ID)
# sta %>% filter(GRTS.Cell.ID %in% !eff.GRTS$GRTS.Cell.ID)
# eff %>% filter(GRTS.Cell.ID %in% !sta.GRTS$GRTS.Cell.ID)

# 4c) Do you have all data represented in the deployment data? If yes, the value should be 0
length(setdiff(unique(sta$Location.Name), unique(dat_summary$Location.Name))) # 8
missing.dat.sta <- left_join(sta, dat_summary, by="Location.Name") %>% dplyr::select(Location.Name, Orig_Name, Orig.Name)
missing.dat.sta[is.na(missing.dat.sta$Orig.Name),] # will show missing deployment data covariates (same as above)

sta %>% filter(!Location.Name %in% missing.data.names$Location.Name) %>% count(Location.Name) # 117 stations with data
sta %>% filter(!Location.Name %in% missing.data.names$Location.Name) %>% count(GRTS.Cell.ID) # 72 grid cells with data

# 5) Remove dates outside survey range (i.e., ARUs turned on but not deployed or deployed well past NABat guidelines)
# eff %>% group_by(Deployment.ID) %>% summarise(min(Survey.Start.Time), max(Survey.End.Time))
# remove all dates before May 01 and after Sept 10, using Julian date
yday("2023-05-01"); yday("2023-09-10")

dat_summary %>% group_by(Deployment.ID) %>% count(Year) 
nrow(dat_summary)
dat_summary <- dat_summary %>% filter(between(jDay, 121, 253))

# 6) # check to make sure pulling times from start and end of day
dat_summary %>% group_by(File.Type) %>% summarise(min(Timep, na.rm=T), max(Timep, na.rm=T))
# sum(is.na(dat_summary$Timep)) / nrow(dat_summary) # almost 20% having issues with filename (only 3 % in 2019; 0.7% in 2021, 0% in 2023)

# hist(dat_summary$Timep, breaks=50)
# hist(dat_summary[dat_summary$File.Type=="zc",]$Timep, breaks=60)
# will have some NAs in dat_summary as not all filenames contained time, but if small proportion then ok to go ahead

# If all of the above is satisfied -> press 'Knit' above ^
```

```{r non-adjustable options, echo=F, include=F}
# SE function
se <- function(x) sqrt(var(x)/length(x))

dist.mat$stn_dist_mean_km <- (rowMeans(dist.mat[1:nrow(sta)], na.rm = T))/1000
dist.mat$stn_dist_min_km <- (apply(dist.mat[1:nrow(sta)], 1, FUN=min, na.rm = T))/1000
dist.mat$stn_dist_max_km <- (apply(dist.mat[1:nrow(sta)], 1, FUN=max, na.rm = T))/1000
dist.mat %>% filter(stn_dist_min_km <1) %>% select(Location.Name)

dist.mat %>% summarise(min(stn_dist_min_km), mean(stn_dist_min_km), max(stn_dist_min_km))
dist.mat %>% summarise(se(stn_dist_mean_km), se(stn_dist_min_km),se(stn_dist_max_km))

# Count the total number of detector stations and GRTS cells
n.stat <- length(unique(sta$Location.Name))
n.GRTS <- length(unique(sta$GRTS.Cell.ID))

# Find mean and SE detectors per GRTS cell and number of detectors per quadrant
det.per.GRTS <- sta %>% group_by(GRTS.Cell.ID) %>% count(GRTS.Cell.ID)
det.per.quad <- sta %>% count(Stn_Num)
det.per.GRTS %>% ungroup() %>% count(n)

# Count the total number of stations sampled for NABat
# names(sta)
NABat.smp <- sta %>% count(NABat_Samp)
NABat.smp.yes <- NABat.smp[NABat.smp$NABat_Samp=="Yes",]$n
NABat.smp.no <- NABat.smp[NABat.smp$NABat_Samp=="No",]$n

# Find mean and SE for years surveyed per GRTS cell
yrs.surveyed.GRTS <- sta %>% group_by(GRTS.Cell.ID) %>% summarise(GRTS.yrs.srvyd = max(num.years))
yrs.surveyed.GRTS %>% summarise(min(GRTS.yrs.srvyd), mean(GRTS.yrs.srvyd), max(GRTS.yrs.srvyd), se(GRTS.yrs.srvyd))

# ARUs by NR, LUC and LUT
ARUs.by.NP <- sta %>% filter(NP=="In") %>% count(Land.Unit.Code, sort = TRUE)
ARUs.by.NR <- sta %>% count(Natural.Region, sort = TRUE)
ARUs.by.LUC <- sta %>% filter(NP=="Out") %>% count(Land.Unit.Code, sort = TRUE)
ARUs.by.LUT <- sta %>% count(Land.Cover, sort = TRUE)

# Proportion of stations vs spatial covariates
sum(ARUs.by.LUC$n) # 97
sum(ARUs.by.NP$n) # 27
sum(ARUs.by.NR$n) # 123
sum(ARUs.by.LUT$n) # 123
colnames(cov.prop)[5] <- "prop.area"
cov.prop$NP <- as.factor(ifelse(grepl("NP",cov.prop$Name), "In", "Out"))
#cov.prop <- cov.prop %>% arrange(NP, Cov, Name)
cov.prop <- left_join(cov.prop,ARUs.by.LUC,by=c("Name"="Land.Unit.Code"))
cov.prop <- left_join(cov.prop,ARUs.by.NP,by=c("Name"="Land.Unit.Code"))
cov.prop <- left_join(cov.prop,ARUs.by.NR,by=c("Name"="Natural.Region"))
cov.prop <- left_join(cov.prop,ARUs.by.LUT,by=c("Name"="Land.Cover"))

cov.prop$Num.Stn <- rowSums(cov.prop[7:10],na.rm=T)
cov.prop$prop.stn <- ifelse(cov.prop$Cov=="Land.Unit.Code" & cov.prop$NP=="Out", cov.prop$Num.Stn / sum(ARUs.by.LUC$n),
                            ifelse(cov.prop$Cov=="Land.Unit.Code" & cov.prop$NP=="In", cov.prop$Num.Stn / sum(ARUs.by.NP$n),
                                   ifelse(cov.prop$Cov=="Natural.Region", cov.prop$Num.Stn / sum(ARUs.by.NR$n), 
                                          ifelse(cov.prop$Cov=="Land.Use.Type", cov.prop$Num.Stn / sum(ARUs.by.LUT$n),NA))))
cov.prop <- cov.prop %>% select(-c(Area_m2,n.x, n.y, n.x.x, n.y.y))

# Generate colours to display the category levels - R needs them as a factor
names(sta)
sta[,category] <- factor(sta[,category])
col.cat <- wheel(colour, num = length(levels(sta[,category])))
sta$Cols <- col.cat[sta[,category]]

sta[,"Natural.Region"] <- factor(sta[,"Natural.Region"])
col.catNR <- wheel(colour, num = length(unique(sta[,"Natural.Region"])))
sta$ColsNR <- col.catNR[sta[,"Natural.Region"]]

# sta[,"Land.Unit.Code"] <- factor(sta[,"Land.Unit.Code"])
# col.catLUC <- wheel(colour, num = length(unique(sta[,"Land.Unit.Code"])))
# sta$ColsLUC <- col.catLUC[sta[,"Land.Unit.Code"]]

sta[,"LandUnitCo"] <- factor(sta[,"LandUnitCo"])
col.catLUC <- wheel(colour, num = length(unique(sta[,"LandUnitCo"])))
sta$ColsLUC <- col.catLUC[sta[,"LandUnitCo"]]

sta[,"Land.Use.Type"] <- factor(sta[,"Land.Use.Type"])
col.catLUT <- wheel(colour, num = length(unique(sta[,"Land.Use.Type"])))
col.catLUT <- col.catLUT[order(col.catLUT, decreasing = TRUE)] # have Water as blue
sta$ColsLUT <- col.catLUT[sta[,"Land.Use.Type"]]

col.catYr <- wheel(colour, num = 6)
col.catYr <- col.catYr[order(col.catYr, decreasing = TRUE)]

###--- add covariates to dat objects
# dat_count$NP <- sta$NP[match(dat_count$Location.Name, sta$Location.Name,)]
# dat_count$Natural.Region <- sta$Natural.Region[match(dat_count$Location.Name, sta$Location.Name,)]
# dat_count$Land.Unit.Code <- sta$Land.Unit.Code[match(dat_count$Location.Name, sta$Location.Name,)]
# dat_count$Land.Use.Type <- sta$Land.Use.Type[match(dat_count$Location.Name, sta$Location.Name,)]

###--- create volancy covariate
July10 <- yday("2023-07-10") # 191 = July 10 (general date of volancy)
# if using one date for entire province, relevel with "Pre" as first category

# dat_summary$Volancy <- "Post-volancy"
dat_summary$Volancy <- as.factor(ifelse(dat_summary$jDay<July10, "Pre-volancy","Post-volancy")) %>% relevel("Pre-volancy")
dat_summary %>% group_by(Volancy, Deployment.ID) %>% summarise(min(jDay), max(jDay))

col.catVol <- as.character(c("#2028B2","#B2AA20"))

sta %>% filter(Location.Name %in% missing.data.names$Location.Name)  # 13 stations without data - some summary stats

sta %>% filter(Location.Name %in% missing.data.names$Location.Name) %>% group_by(LandUnitCo) %>% count(NP, Land.Use.Type, Natural.Region, GRTS.Cell.ID) # 13 stations without data - some summary stats

```

``` {r temporal summaries, include=F}

# remove call files without time
# dat_summary %>% filter(is.na(Timep)) %>% count(Location.Name)
# sta %>% filter(GRTS.Cell.ID=="171651")
# dat_summaryT %>% filter(Location.Name=="171651_NW_01")
# 563/(4734+563)
# 10% of 171651_NW_01 removed due to time stamp issues (563 files removed)

dat_summaryT <- dat_summary[complete.cases(dat_summary$Timep),]

# now use Alberta eBat criteria to classify species
dat_sum_sub <- dat_summaryT[c("GRTS.Cell.ID","Location.Name","SurveyNight","Year","Month","jDay","Volancy","Timep",
                              "Filename","n_calls","prob1","sp1","prob2","sp2","prob3","sp3")]
nrow(dat_sum_sub) # 243,178 files
dat_sum_sub$Location.Name.Year <- paste(dat_sum_sub$Location.Name, dat_sum_sub$Year, sep="_")

# check for erroneous time stamps and remove entire survey
Timepdate <- date(Sys.time())
Timepdatetime1 <- as.POSIXct(paste(Timepdate,"08:00:00"), tz)
Timepdatetime2 <- as.POSIXct(paste(Timepdate,"18:00:00"),tz)

# 3 stations and 39 surveys with calls between 8 am and 6 pm - remove these survey periods from the temporal analysis
timestamp.error <- as.data.frame(dat_sum_sub %>% filter(Timep >Timepdatetime1 & Timep<Timepdatetime2) %>% filter(sp1!="noise") %>% group_by(Location.Name.Year) %>% summarise(min(SurveyNight), max(SurveyNight)))
unique(timestamp.error$Location.Name.Year)
unique(timestamp.error$Location.Name)

dat_sum_sub %>% filter(Location.Name.Year %in% timestamp.error$Location.Name.Year) %>% count(GRTS.Cell.ID)
sta %>% filter(GRTS.Cell.ID %in% c(27242))

# keep in these 3 surveys as only some of the times are off...maybe just picked up some noise calls throughout the day or had some early flying bats (seems like most are around 5 pm)
# dat_sum_sub <- dat_sum_sub %>% filter(!Location.Name.Year %in% timestamp.error$Location.Name.Year)

# subset data to one month pre and one month post volancy (July 10)
# dat_sum_sub <- dat_sum_sub %>% filter(between(jDay, 161,222))
# nrow(dat_sum_sub) # now down to 123837

# create thresholds for noise and bat classifications
threshold_noise <- 0.8; threshold_bat <- 0.5
# start with all call sequences as "unknown"
dat_sum_sub$category <- "unknown"

# Index cases to be categorized as noise
index_noise <- with(dat_sum_sub, (sp1 == "noise") & (prob1 > threshold_noise)) 
# Set the indexed categories to "noise" 
dat_sum_sub$category[index_noise] <- "noise"

# Index the noise values to be filtered out
index_remove_noise <- with(dat_sum_sub, (category == "unknown") & (sp1 == "noise")) 

# Note that there are 126 `NA` values
sum(is.na(index_remove_noise)) 
# These `NA` values are due to an `NA` in the `sp1` column which is the result of a tie in the random forest probabilities
dat_sum_sub[is.na(index_remove_noise),]

# Set any `NA` value to `FALSE` (i.e. not a noise value to be filtered out)
index_remove_noise[is.na(index_remove_noise)] <- FALSE 
# For the noise values to be filtered out, get the corresponding probabilities
prob_noise <- dat_sum_sub[index_remove_noise, "prob1"] 
# Record the state of the data frame before the filtering out of noise
dat_sum_sub_before_noise_removal <- dat_sum_sub

dat_sum_sub[index_remove_noise, "sp1"] <- dat_sum_sub[index_remove_noise, "sp2"]
dat_sum_sub[index_remove_noise,"sp2"] <- dat_sum_sub[index_remove_noise, "sp3"] 
dat_sum_sub[index_remove_noise, "prob1"] <- dat_sum_sub[index_remove_noise, "prob2"] 
dat_sum_sub[index_remove_noise,"prob2"] <- dat_sum_sub[index_remove_noise, "prob3"] 

# Now that the `prob3` value has been shifted to the `prob2` column, replace the `prob3` column with `NA` values, for the cases where noise has been filtered out
dat_sum_sub[index_remove_noise, "prob3"] <- NA 
dat_sum_sub[index_remove_noise, "sp3"] <- NA

dat_sum_sub[index_remove_noise, "prob1"] <- dat_sum_sub[index_remove_noise, "prob1"] / (1 - prob_noise) 
dat_sum_sub[index_remove_noise, "prob2"] <- dat_sum_sub[index_remove_noise, "prob2"] / (1 - prob_noise) 
# Compare "sp1" columns before and after filtering out noise
table(dat_sum_sub_before_noise_removal$sp1)
table(dat_sum_sub$sp1)

index_bat_species <- with(dat_sum_sub, (category == "unknown") & (n_calls >= 3) & (prob2 / prob1 <= 0.80)) 

index_bat_species[is.na(index_bat_species)] <- FALSE # to deal with the NA values

dat_sum_sub$category[index_bat_species] <- dat_sum_sub$sp1[index_bat_species]
# dat_sum_sub$sp1 <- dat_sum_sub$sp1 %>% replace_na("unknown")
# dat_sum_sub$sp2 <- dat_sum_sub$sp2 %>% replace_na("unknown")

eBat_param <- read.csv("./Input/eBat_param.csv")
dim(eBat_param)
tmp <- as.data.frame(lapply(eBat_param[,3:13], function(y) gsub("Â±.*$", "", y)))
param <- eBat_param$Parameter
eBat_param <- tmp %>% mutate_at(vars(1:11), as.numeric) # make sure all call data is numeric
eBat_param <- as.data.frame(t(eBat_param))
colnames(eBat_param) <- param
eBat_param %>% arrange(Fmin) %>% select(Fmin)
eBat_param$sp <- rownames(eBat_param)
eBat_param <- eBat_param %>% filter(sp!="noise")

# https://pubs.usgs.gov/of/2018/1068/ofr20181068.pdf
# HighF = min freq >30
# 40k = min freq 35-45
# Myotis40k = Myotis 35-40 min freq
# LowF = min freq <30
# 25k = min freq 15-25

Q25_sp <- as.character(eBat_param %>% filter(Fmin<25) %>% select(sp))
Q40_sp <- as.character(unlist(eBat_param %>% filter(Fmin>=35 & Fmin<=45) %>% select(sp)))
LowF_sp <- as.character(unlist(eBat_param %>% filter(Fmin<=30) %>% select(sp)))
HighF_sp <- as.character(unlist(eBat_param %>% filter(Fmin>30) %>% select(sp)))

Q25_index <- (dat_sum_sub$sp1 %in% Q25_sp) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob1 > 0.2)
Q40_index <- (dat_sum_sub$sp1 %in% Q40_sp) & (dat_sum_sub$sp2 %in% Q40_sp) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)
LowF_index <- (dat_sum_sub$sp1 %in% LowF_sp) & (dat_sum_sub$sp2 %in% LowF_sp) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)
HighF_index <- (dat_sum_sub$sp1 %in% HighF_sp) & (dat_sum_sub$sp2 %in% HighF_sp) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

LABO.MYLU_index <- (dat_sum_sub$sp1 %in% c("LABO", "MYLU")) & (dat_sum_sub$sp2 %in% c("LABO", "MYLU")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

EPFU.LANO_index <- (dat_sum_sub$sp1 %in% c("EPFU", "LANO")) & (dat_sum_sub$sp2 %in% c("EPFU", "LANO")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

MYEV.MYSE_index <- (dat_sum_sub$sp1 %in% c("MYEV", "MYSE")) & (dat_sum_sub$sp2 %in% c("MYEV", "MYSE")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

My_40k_index <- (dat_sum_sub$sp1 %in% c("MYCI", "MYLU", "MYVO")) & (dat_sum_sub$sp2 %in% c("MYCI", "MYLU", "MYVO")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)


#' Check the number of call sequences belonging to each category:
sum(HighF_index, na.rm=T)
sum(Q40_index, na.rm=T)
sum(My_40k_index, na.rm=T)
sum(LABO.MYLU_index, na.rm=T)
sum(MYEV.MYSE_index, na.rm=T)

sum(LowF_index, na.rm=T)
sum(Q25_index, na.rm=T)
sum(EPFU.LANO_index, na.rm=T)

#' Define a new data frame:  
dat_sum_sub_df <- dat_sum_sub
dat_sum_sub_df$category <- dat_sum_sub_df$sp1

# for all species with the "unknown" classification
# reclassify going from coarser to finer resolution of ID
dat_sum_sub_df %>% count(category)

dat_sum_sub_df$category[dat_sum_sub_df$n_calls < 3 | 
                          dat_sum_sub_df$prob1 < threshold_bat & dat_sum_sub_df$sp1 != "noise"] <- "unknown"

dat_sum_sub_df$category[HighF_index] <- "HighF"
dat_sum_sub_df$category[Q40_index] <- "40k"
dat_sum_sub_df$category[My_40k_index] <- "My_40k"

dat_sum_sub_df$category[LABO.MYLU_index] <- "LABO.MYLU"
dat_sum_sub_df$category[MYEV.MYSE_index] <- "MYEV.MYSE"

dat_sum_sub_df$category[LowF_index] <- "LowF"
dat_sum_sub_df$category[Q25_index] <- "25k"
dat_sum_sub_df$category[EPFU.LANO_index] <- "EPFU.LANO"

# dat_sum_sub_df <- dat_sum_sub_df %>% filter(n_calls<3)


###--- Visualization of overlapping species detection data
# use overlap and circular R packages

# create new df with select columns, remove noise files and all classifications with <3 calls, and add in some covariates
# nrow(dat_sum_sub_df) - nrow(dat_time) # 48,393

dat_time <- dat_sum_sub_df %>% filter(n_calls>2) %>% select(-n_calls:-sp3) %>% filter(category!="noise")
# unique(dat_time$category)
# dat_time %>% count(category)


dat_time$Classification <- as.factor(dat_time$category %>% 
                                         recode(EPFU.LANO = "EPFU-LANO", LABO.MYLU = "LABO-MYLU",
                                                My_40k = "Myotis 40k", MYEV.MYSE = "MYEV-MYSE"))

# levels(dat_time$Classification)
dat_time$Classification <- factor(dat_time$Classification,
                                     levels = c("EPFU", "EPFU-LANO", "LANO", "LACI", "LABO", "LABO-MYLU", "MYLU",
                                                "MYCA", "MYCI", "MYEV","MYEV-MYSE", "MYSE", "MYVO", "Myotis 40k",
                                                "40k","25k","HighF","LowF","unknown"))

dat_time <- as.data.frame(dat_time) 
dat_time$Time <- format(dat_time$Timep, format = "%H:%M:%S")

# covariates for overlap
dat_time$Classification_Volancy <- as.factor(paste(dat_time$Classification, dat_time$Volancy))
dat_time$NP <- sta$NP[match(dat_time$Location.Name, sta$Location.Name,)]
dat_time$Land.Use.Type <- sta$Land.Cover[match(dat_time$Location.Name, sta$Location.Name,)]
dat_time$Natural.Region <- sta$Natural.Region[match(dat_time$Location.Name, sta$Location.Name,)]


dat_time %>% count(Classification)
nrow(dat_time) # 229510
# difference is that dat_time has filtered out all "calls" with <3 pulses
# inclined to go with dat_time as splits unknown down a bit

dat_time_agg <- dat_time %>% group_by(GRTS.Cell.ID, Location.Name, Volancy, SurveyNight) %>% count(Classification)
colnames(dat_time_agg)[6] <- "Count"
dat_time_agg$Count <- as.numeric(dat_time_agg$Count)
dat_time_agg$Year <- as.factor(year(dat_time_agg$SurveyNight))
dat_time_agg$Month <- month(dat_time_agg$SurveyNight)
dat_time_agg$jDay <- yday(dat_time_agg$SurveyNight)
dat_time_agg$NP <- sta$NP[match(dat_time_agg$Location.Name, sta$Location.Name,)]
dat_time_agg$Natural.Region <- sta$Natural.Region[match(dat_time_agg$Location.Name, sta$Location.Name,)]
dat_time_agg$Land.Unit.Code <- sta$LandUnitCo[match(dat_time_agg$Location.Name, sta$Location.Name,)]
dat_time_agg$Land.Use.Type <- sta$Land.Cover[match(dat_time_agg$Location.Name, sta$Location.Name,)]


dat_time_agg %>% ungroup() %>% summarise(min(SurveyNight), max(SurveyNight))
dat_time_agg %>% ungroup() %>% count(Classification)
dat_time_agg %>% ungroup() %>% summarise(sum(Count))
dat_time_agg %>% ungroup() %>% count(GRTS.Cell.ID)
unique(dat_time_agg$Location.Name)
length(unique(dat_time_agg$SurveyNight))

# glimpse(dat_time_agg)
# summary(dat_time_agg)
# dat_time_agg %>% ungroup() %>% summarise(sum(Count)) 

# Create a call df without the noise files
call_count <- dat_time_agg %>% ungroup () # no noise files but keeping this to keep the same code below, to use with dat_tmime_agg
call_sum_class <- call_count %>% group_by(Classification) %>% summarise(Total = sum(Count))
arrange(call_sum_class, Total)
call_count %>%  summarise(sum(Count))
call_count %>% group_by(Volancy)%>% summarise(sum(Count))
(316359 + 346745)  / 663104

316359 / 663104
unknown.calls <- call_count %>% filter(Classification=="unknown") %>% summarise(sum(Count)) / sum(call_count$Count)

MYLU.calls <- call_count %>% filter(Classification=="MYLU") %>% summarise(sum(Count)) / sum(call_count$Count)

# Determine total effort (survey nights)
total.effort <- call_count %>% group_by(Location.Name, Year) %>% summarise_at(c("SurveyNight"), list(Min = min, Max=max))
total.effort$Diff <- (total.effort$Max - total.effort$Min)+1
sum(total.effort$Diff) # 9912 survey nights from 2015 - 2022
# total.effort %>% group_by(Volancy) %>% summarise(sum(Diff))
# 2011 / 3347 
total.effort %>% ungroup()  %>% summarise(min(Min), max(Max), min(Diff), max(Diff), mean(Diff)) 
# min(Min)` `max(Max)` `min(Diff)` `max(Diff)` `mean(Diff)` 
# 1 2015-06-09 2022-09-10 1 days      133 days    15.43925 days

### temporal summaries
# create activity overlap plots for all bats in/out of parks 

# create function to produce activity overlap for all classifications pre and post volancy 
volancy_overlap.fn <- function(Classification=Classification){
  overlap <- activityOverlap(recordTable = dat_time,
                             speciesA = paste(Classification,"Pre-volancy"),
                             speciesB = paste(Classification,"Post-volancy"),
                             speciesCol = "Classification_Volancy",
                             recordDateTimeCol = "Time",
                             recordDateTimeFormat = "%H:%M:%S",
                             xcenter="midnight",
                             overlapEstimator = "Dhat4",
                             plotR = TRUE,
                             writePNG = TRUE,
                             plotDirectory = ("./Output"),
                             addLegend = TRUE,
                             legendPosition = "topleft",
                             pngMaxPix   = 1000,
                             linecol     = c("black", "blue"),
                             linewidth   = c(3,3),
                             linetype    = c(1, 2),
                             olapcol     = "darkgrey",
                             add.rug     = TRUE,
                             extend      = "lightgrey",
                             ylim = c(0,0.35),
                             main = paste(Classification,"Pre and Post Volancy"))
  wwt <- watson.wheeler.test(list(overlap$densityA, overlap$densityB))
  return(wwt)
}


# save.image("NABat_Annual_Report_201522.RDS")
save.image(paste("NABat_Annual_Report_",Year_interest,".RDS", sep=""))
# load("NABat_Annual_Report_2022.RDS")

# run through each classification, except "unknown" and any classification with <100 calls
unknown.call.types <- c("unknown","25k","40k","LowF","HighF")
class.list <- as.data.frame(dat_time %>% filter(!Classification %in% unknown.call.types) %>% count(Classification, sort=TRUE) %>% 
                              filter(n>100) %>% select(Classification))
class.list <- sort(class.list$Classification)
EPFU.WWT <- volancy_overlap.fn(Classification=class.list[1])
EPFU.LANO.WWT <- volancy_overlap.fn(Classification=class.list[2])
LANO.WWT <- volancy_overlap.fn(Classification=class.list[3])
LACI.WWT <- volancy_overlap.fn(Classification=class.list[4])
LABO.WWT <- volancy_overlap.fn(Classification=class.list[5])
LABO.MYLU.WWT <- volancy_overlap.fn(Classification=class.list[6])
MYLU.WWT <- volancy_overlap.fn(Classification=class.list[7])
MYCA.WWT <- volancy_overlap.fn(Classification=class.list[8])
Myotis40k.WWT <- volancy_overlap.fn(Classification=class.list[9])

```
### Introduction


```{r WNS spread png map, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap="White-nose syndrome spread up to 2021", echo=FALSE}
knitr::include_graphics("./Input/WNS_spread_map_2022-02-26.png")

```

##### Figure 1. The spread of White-Nose Syndrome up to the spring of 2022; downloaded from https://www.whitenosesyndrome.org/where-is-wns.



#### Summary Statistics
All analyses were conducted in R (version 4.1.2; 2021), within RStudio (version 1.2.5033) using rmarkdwon (version 2.11; Allaire et al. 2020) for reproducible results. Unless otherwise noted, results are reported as the mean Â± 1 SE. 

### Results â Survey Representation
In `r Year_interest` there were passive acoustic surveys at `r n.stat` unique locations (i.e., stations) within `r n.GRTS` NABat grid cells (Figure 1). On average, each grid cell had `r round(mean(det.per.GRTS$n),1)` Â± `r round(se(det.per.GRTS$n),1)` (1 SE) stations, with between `r min(det.per.quad[,1])` (`r det.per.quad[det.per.quad$Stn_Num==1,]$n` stations) and `r max(det.per.quad[,1])` (`r det.per.quad[det.per.quad$Stn_Num==4,]$n` station) stations per quadrat. The average minimum distance between stations was `r round(mean(dist.mat$stn_dist_min_km),1)` km Â± `r round(se(dist.mat$stn_dist_min_km),1)`. Neighbouring stations ranged from  `r round(min(dist.mat[dist.mat$stn_dist_min_km!=0,]$stn_dist_min_km),1)` km to `r round(max(dist.mat[dist.mat$stn_dist_min_km!=0,]$stn_dist_min_km),1)` km apart.  While most (i.e., `r NABat.smp.yes` or `r round(NABat.smp.yes/nrow(sta)*100,0)`%) stations were surveyed specifically for NABat monitoring, some (i.e., `r NABat.smp.no` or `r round(NABat.smp.no/nrow(sta)*100,0)`%) stations were surveyed following similar NABat protocols but not for NABat monitoring per se (e.g., outside of the recommended survey temporal window or for other objectives such as monitoring migratory routes [REDR] or to collect WNS baseline data [BANP, WLNP]).

```{r Table 1 bat survey location, echo=F}

sta.count <- sta %>% count(NP, Natural.Region, Land.Unit.Code)

sta.count2 <- sta %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% count(GRTS.Cell.ID)
sta.count2$GRTS.count <- 1
sta.count3 <- as.data.frame(sta.count2 %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% summarise(sum(GRTS.count)))

sta.count4 <- sta %>% filter(NABat_Samp=="Yes") %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% count(NABat_Samp)

sta.NP <-sta %>% filter(NP=="In") %>% count(Land.Unit.Code)
sta.NP$prop <- sta.NP$n / sum(sta.NP$n)
sta.NP <- arrange(sta.NP, n)

sta.NR <-sta %>% count(Natural.Region)
sta.NR$prop <- sta.NR$n / sum(sta.NR$n)
sta.NR <- arrange(sta.NR, n)

sta.count$Count.Grid.Cells <- as.vector(sta.count3[,4])
sta.count <- left_join(sta.count, sta.count4 %>% select(-NABat_Samp), 
                  by = c("NP" = "NP", "Natural.Region" = "Natural.Region", "Land.Unit.Code" = "Land.Unit.Code"))

dist.mat.LUC <- dist.mat %>% group_by(Land.Unit.Code) %>% summarise(`Min Stn Dist (km)` = min(stn_dist_min_km),
                                                                    `Mean Stn Dist (km)` = mean(stn_dist_min_km), 
                                                                    `Max Stn Dist (km)` = max(stn_dist_min_km))

sta.count <- left_join(sta.count, dist.mat.LUC)
colnames(sta.count)[1:6] <- c("National Park", "Natural Region", "Land Unit", "Total Stns", "Total Grid Cells", "NABat Stns")
sta.count <- sta.count[,c("National Park", "Natural Region", "Land Unit","Total Grid Cells","Total Stns","NABat Stns","Min Stn Dist (km)","Mean Stn Dist (km)", "Max Stn Dist (km)")]
# names(sta.count)

opts <- options(knitr.kable.NA = "0", digits=1)

knitr::kable(sta.count,
             caption="The number of stations and NABat grid cells in Natural Regions and Land Units (i.e., National Parks or Land-use Framework Regions). Land Unit abbreviations are as follows: BANP = Banff National Park, EINP = Elk Island National Park, JANP = Jasper National Park, WBNP = Wood Buffalo National Park, WLNP = Waterton Lakes National Park, LOAR = Lower Athabasca Region, LOPR = Lower Peace Region, NOSR = North Saskatchewan Region, REDR = Red Deer Region, SOSR = South Saskatchewan Region, UPAR = Upper Athabasca Region, and UPPR = Upper Peace Region.",
             align = "lllrrrrrr")
```


```{r Provincial png map, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap="Bat Survey Locations 2015-2021", echo=FALSE}
knitr::include_graphics("./Output/Fig_provincial.plot.png")
```

##### Figure 1. Bats were acoustically surveyed, following NABat protocols, at `r n.stat` stations across Alberta in `r Year_interest`.


```{r nightly survey effort NP, echo=F, fig.height=6, fig.width=7}

calls.per.night.locn <- dat_time_agg %>% group_by(Year,jDay, SurveyNight,NP, Natural.Region, Land.Unit.Code, Location.Name) %>%
  filter(Classification!="noise") %>%
  summarise(Call.Count =sum(Count))
calls.per.night.locn$Night.Count <- 1
calls.per.night.locn %>% ungroup() %>% summarise(mean(Call.Count), se(Call.Count), min(Call.Count), max(Call.Count))
calls.per.night.locn %>% ungroup() %>% filter(Call.Count>1000)

calls.per.night.locn <- calls.per.night.locn %>% ungroup()
calls.per.night.locn %>% filter(is.na(Land.Unit.Code))

NightLUC.hist <- ggplot(data = calls.per.night.locn, aes(x = jDay, y = Night.Count, fill=Land.Unit.Code)) + 
  geom_vline(xintercept = July10, linetype="dotted", color = "gray", size=0.5)+
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=unique(sta$ColsLUC))+
  scale_x_continuous(breaks = c(153, 183, 214, 245),
  label = c("Jun-01", "Jul-01", "Aug-01","Sep-01"))+
  theme_classic() + ylab("Number of Stations Surveyed") +
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.title.x = element_blank()) +
  theme(axis.title.y = element_text(size = 14))+
  facet_wrap(~Year, ncol=1)

NightLUC.hist

# Cairo(file="Output/NightPA.hist.PNG",
#       type="png",
#       width=3400,
#       height=2600,
#       pointsize=15,
#       bg="white",
#       dpi=300)
# NightLUC.hist
# dev.off()

# tmp1 <- calls.per.night.locn %>% group_by(Location.Name) %>% summarise(min(SurveyNight), max(SurveyNight))
# colnames(tmp1) <- c("Location.Name", "Survey.Start", "Survey.End")
# tmp1$nights_surveyed <- tmp1$Survey.End - tmp1$Survey.Start+1
# tmp1 <- as.data.frame(tmp1)
# min(tmp1$nights_surveyed) #1
# max(tmp1$nights_surveyed) #133
# mean(tmp1$nights_surveyed) #19
# se(tmp1$nights_surveyed) #2
# sum(tmp1$nights_surveyed)

```

###### Figure 2. Number of bat survey locations sampled per night from `r min(eff$Deployment.ID)` to `r max(eff$Deployment.ID)` by year within and outside of National Parks (NP).

#### Nightly Overall Bat Call Summaries

```{r mean nightly call by NR, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count.NR <- call_count %>% group_by(Natural.Region, Year, SurveyNight, Volancy) %>% summarise(Count = sum(Count))
# call_count.NR %>% group_by(Natural.Region) %>% summarise(sum(Count))
# call_count.NR %>% ungroup() %>% summarise(sum(Count))

# summary(call_count.NR)

# subset data for overall mean nightly bat calls by year for each Natural Region
NR.Calls.Yr <- call_count.NR  %>% group_by(Natural.Region, Year, Volancy) %>%
  summarise(Mean = mean(Count), SE = se(Count))

call_count.NR %>% ungroup() %>% group_by(SurveyNight) %>% summarise(mean(Count))
summary(call_count.NR)

call_count.NR %>% ungroup() %>% group_by(Natural.Region) %>% summarise(mean(Count), se(Count))

NR.surveyed <- call_count %>% group_by(Year, Natural.Region, Location.Name, SurveyNight, Volancy) %>% summarise(Count =sum(Count))
NR.surveyed %>% ungroup() %>% summarise(mean(Count), min(Count), max(Count), se(Count))
NR.surveyed %>% arrange(Count) %>% tail()
NR.surveyed %>% filter(Count>999) %>% count(Location.Name)
NR.surveyed %>% group_by(Natural.Region) %>% summarise(mean(Count), se(Count))
NR.Calls.Yr <- NR.surveyed %>% group_by(Year,Natural.Region, Volancy) %>% summarise(Mean = mean(Count), SE = se(Count))


fcalls.NR <- NR.Calls.Yr %>%
  ggplot(aes(x = Natural.Region, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean Â± 1 SE Nightly Bat Calls"))) +
  #ylim(c(0,60))+
  geom_linerange(aes(Natural.Region, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  facet_wrap(~Year, ncol=1)
fcalls.NR

# Cairo(file="Output/fcalls.NR.2022.PNG",
#       type="png",
#       width=2000,
#       height=1800,
#       pointsize=15,
#       bg="white",
#       dpi=300)
# fcalls.NR
# dev.off()

```
##### Figure 8. Mean nightly bat calls for each Natural Region, by year and volancy period.

```{r mean nightly call by LUT, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count.LUT <- call_count %>% group_by(Land.Use.Type, Year, Volancy)
call_count.LUT <- call_count.LUT[complete.cases(call_count.LUT$Land.Use.Type),]

# subset data for overall mean nightly bat calls by year for each LUT
call_count.LUT %>% group_by(Land.Use.Type) %>% summarise(Mean = mean(Count), SE = se(Count))

LUT.Calls.Yr <- call_count.LUT %>% group_by(Land.Use.Type, Year, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))
LUT.levels <- c("Agriculture","Barren Land", "Developed" ,"Forest-conifer", "Forest-deciduous","Forest-mixed",  "Grassland" ,"Shrubland", "Water")
LUT.Calls.Yr$Land.Use.Type <- factor(LUT.Calls.Yr$Land.Use.Type , levels = LUT.levels)
LUT.Calls.Yr$Land.Use.Type

LUT.surveyed <- call_count %>% group_by(Year, Land.Use.Type, Location.Name, SurveyNight, Volancy) %>% summarise(Count =sum(Count))
LUT.surveyed %>% ungroup() %>% summarise(mean(Count), min(Count), max(Count), se(Count))
LUT.surveyed %>% arrange(Count) %>% tail()
LUT.surveyed %>% filter(Count>999) %>% count(Location.Name)
LUT.surveyed %>% group_by(Land.Use.Type) %>% summarise(Mean = mean(Count), SE = se(Count)) %>% arrange(Mean)
LUT.Calls.Yr <- LUT.surveyed %>% group_by(Year,Land.Use.Type, Volancy) %>% summarise(Mean = mean(Count), SE = se(Count))

fcalls.LUT <- LUT.Calls.Yr %>%
  ggplot(aes(x = Land.Use.Type, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean Â± 1 SE Nightly Bat Calls"))) +
  geom_linerange(aes(Land.Use.Type, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  facet_wrap(~Year)
fcalls.LUT

# Cairo(file="Output/fcalls.LUT.2022.PNG",
#       type="png",
#       width=2000,
#       height=1800,
#       pointsize=15,
#       bg="white",
#       dpi=300)
# fcalls.LUT
# dev.off()

```

```{r mean nightly call by GRTS, echo=F, fig.height=6, fig.width=7, warning=FALSE}

# Reorder GRTS to be NP, Land.Unit.Code then GRTS
GRTS.Order <- call_count %>% arrange(Land.Unit.Code, GRTS.Cell.ID) %>% 
  group_by(Land.Unit.Code) %>% count(GRTS.Cell.ID)

GRTS.Order$Order <- row.names(GRTS.Order)
GRTS.Order <- fct_reorder(GRTS.Order$GRTS.Cell.ID, GRTS.Order$Order, min)

sta %>% filter(GRTS.Cell.ID=="179795")

# subset data for overall mean nightly bat calls by year for each GRTS
GRTS.surveyed <- call_count %>% group_by(Year, Land.Unit.Code, GRTS.Cell.ID, Location.Name, SurveyNight) %>% summarise(Count =sum(Count))
# GRTS.surveyed %>% ungroup() %>% summarise(mean(Count), min(Count), max(Count), se(Count))
# GRTS.surveyed %>% arrange(Count) %>% tail()
# GRTS.surveyed %>% filter(Count>999) %>% count(Location.Name)
# GRTS.surveyed %>% group_by(GRTS.Cell.ID) %>% summarise(Mean = mean(Count), SE = se(Count)) %>% arrange(Mean)
GRTS.Calls.Yr <- GRTS.surveyed %>% group_by(Year,Land.Unit.Code, GRTS.Cell.ID) %>% summarise(Mean = mean(Count), SE = se(Count))

# GRTS.Calls.Yr$GRTS.Cell.ID <- factor(GRTS.Calls.Yr$GRTS.Cell.ID, levels = GRTS.Order)
GRTS.Calls.Yr %>% filter(Mean >200) %>% arrange(Mean) %>% print(n=21)
GRTS.Calls.Yr %>% filter(Mean <5)

call_count %>% group_by(Volancy) %>% summarise(sum(Count))
98666/(98666+125714)

summary(GRTS.Calls.Yr$Mean)
fcalls.GRTS <- GRTS.Calls.Yr %>%
  ggplot(aes(x = GRTS.Cell.ID, Mean))+
  geom_point(colour="white", shape=21, size=4,aes(fill=Land.Unit.Code))+
  scale_fill_manual(values=unique(sta$ColsLUC)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean Â± 1 SE Nightly Bat Calls"))) +
  xlab(expression("NABat GRTS Cell ID"))+
  geom_linerange(aes(GRTS.Cell.ID, ymin = Mean-SE, ymax = Mean+SE)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 8)) +
  theme(legend.position = "bottom", legend.title=element_blank()) +
  facet_wrap(~Year)

fcalls.GRTS

# Cairo(file="Output/fcalls.GRTS.2022.PNG",
#       type="png",
#       width=2000,
#       height=1800,
#       pointsize=15,
#       bg="white",
#       dpi=300)
# fcalls.GRTS
# dev.off()

```

##### Figure 10. Mean nightly bat calls for each NABat grid cell by year.

#### Nightly Bat Call Summaries by Species / Species Group
There were `r sprintf("%.0f", sum(call_count$Count))` bat calls recorded at `r n.stat` bat survey locations between `r min(eff$Deployment.ID)` and `r max(eff$Deployment.ID)`, over `r sum(as.numeric(total.effort$Diff))` survey nights. Of these calls, `r round(unknown.calls*100,0)`% were classified as unknown.

##### Table 2. Bat call count
``` {r overall call table, echo=F, results="asis"}
# create a table for the paper
# Determine call count overall, percentage and by effort for each species
Table.Calls <- call_count %>% group_by(Classification) %>% summarise(Call.Count = sum(Count))
Table.Calls$Per.Known <- as.data.frame(round(Table.Calls$Call.Count/sum(Table.Calls$Call.Count)*100,0))[,1]
Table.Calls$Occupancy <- as.data.frame(round(Table.Calls$Call.Count/sum(as.numeric(total.effort$Diff)),2))[,1] 
colnames(Table.Calls) <- c("Species / Species Group", "Call Count", "% of Calls","Calls per Night")
# as.data.frame(Table.Calls)

knitr::kable(Table.Calls, 
             caption=paste("Overall bat call count, percentage and call per night for ",n.stat," stations surveyed in ",Year_interest,".", sep=""),
             align = "lrrr",
             format.args = list(scientific = FALSE))

options(scipen = 100)

call_count %>% filter(Classification=="MYLU") %>% group_by(Natural.Region) %>% summarise(sum(Count))
51227/99973
# head(call_count)
# head(dat_time)
# bat_count <- call_count %>% filter(Classification!="unknown")
# call_count %>% group_by(Classification) %>% summarise(sum(Count))
# dat_time %>% count(Classification)
# nrow(dat_time)
# 
# sum(call_count$Count)
# sum(bat_count$Count)
# bat_count %>% group_by(Classification) %>% summarise(sum(Count))
# 
# 12878  / sum(bat_count$Count)

# call_count %>% group_by(Natural.Region, Classification) %>% summarise(Call.Count = sum(Count))
# NR_xtabs <- as.data.frame(table(dat_time$Classification,dat_time$Natural.Region))
# colnames(NR_xtabs)[1:2] <- c("Classification","Natural.Region")
# sum(NR_xtabs$Freq)
# NR_xtabs <- NR_xtabs %>% pivot_wider(names_from = Natural.Region, values_from = Freq)
# NR_xtabs <- NR_xtabs %>% rowwise() %>% mutate(Class.Sum = sum(c_across(where(is.numeric))))
# write.csv(NR_xtabs, "NR_xtabs.csv")
eff %>% filter(GRTS.Cell.ID=="336731")
call_count %>% ungroup() %>% filter(Classification=="MYEV-MYSE") %>% group_by(Location.Name, Natural.Region,Land.Unit.Code) %>% summarise(sum(Count))

call_count %>% ungroup() %>% filter(Classification=="MYCA") %>% filter(Natural.Region=="Boreal") %>% group_by(Land.Unit.Code) %>% summarise(count = sum(Count)) %>% arrange(desc(count))
94/1030

```


```{r sp hist NR, echo=F, fig.height=6, fig.width=7}
# Generate colours to display the species levels
call_count$Classification <- droplevels(call_count$Classification)

Sp.hist.data <- call_count %>% group_by(Year, Natural.Region, Land.Use.Type, Classification) %>% summarise(n = sum(Count))

Sp.hist.NR <- ggplot(data = Sp.hist.data, aes(x = Classification, y = n, fill= Natural.Region)) + 
  geom_bar(stat = "identity") + 
  # scale_fill_manual(values=unique(sta$ColsNR))+
  theme_classic() + xlab("Species / Species Group") + ylab("Total Bat Calls") + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(axis.title.y = element_text(size = 14)) +   theme(axis.title.x = element_blank())+
  facet_wrap(~Year)

Sp.hist.NR

```

##### Figure 11. Total bat calls for species / species groups by year and Natural Region


```{r sp hist LUT, echo=F, fig.height=6, fig.width=7}

Sp.hist.LUT <- ggplot(data = Sp.hist.data %>% filter(!is.na(Land.Use.Type)), aes(x = Classification, y = n, fill= Land.Use.Type)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette = "Paired", direction=-1)+
  theme_classic() + xlab("Species / Species Group") + ylab("Total Bat Calls") + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(axis.title.y = element_text(size = 14)) +   theme(axis.title.x = element_blank())+
  facet_wrap(~Year, ncol=1, scales="free_y")

Sp.hist.LUT
```

##### Figure 12. Total bat calls for species / species groups by year and Land Use Type.


```{r mean species call by year and volancy, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count.Sp <- call_count %>% group_by(Classification, Year, Volancy)
# only graph species / species groups with >100 calls
sp.to.use <- call_count.Sp %>% group_by(Classification) %>% summarise(Sum = sum(Count))

# group data for overall mean nightly bat calls by year with volancy
Sp.Calls.Yr <- call_count.Sp%>% group_by(Classification, Year, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

Sp.Calls.Yr %>% arrange(Mean) %>% print(n=29)

fcalls.Sp <- Sp.Calls.Yr %>% filter(Classification %in% sp.to.use[sp.to.use$Sum>100,]$Classification)%>%
  ggplot(aes(x = Classification, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean Â± 1 SE Nightly Bat Calls"))) +
  geom_linerange(aes(Classification, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank())+
  facet_wrap(~Year)
fcalls.Sp

unique(eff$Contact)

```

##### Figure 13. Mean nightly bat calls for each species / species group by year and volancy period.

Species / species group specific 

```{r EFPU-LANO activity overlap, out.width="33%", out.height="20%", fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c(paste("./Output/activity_overlap_EPFU Pre-volancy-EPFU Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_EPFU-LANO Pre-volancy-EPFU-LANO Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_LANO Pre-volancy-LANO Post-volancy_",Sys.Date(),".png",sep="")))
``` 

##### Figure 15. Nightly activity overlap one month pre and post volancy for EPFU, LANO and EPFU-LANO.

```{r LABO-MYLU activity overlap, out.width="33%", out.height="20%", fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c(paste("./Output/activity_overlap_LABO Pre-volancy-LABO Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_LABO-MYLU Pre-volancy-LABO-MYLU Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_MYLU Pre-volancy-MYLU Post-volancy_",Sys.Date(),".png",sep="")))
``` 

##### Figure 16. Nightly activity overlap one month pre and post volancy for LABO, MYLU and LABO-MYLU. 

```{r LACO-Myotis 40k activity overlap, out.width="33%", out.height="20%", fig.show='hold', fig.align='center', echo=FALSE}
knitr::include_graphics(c(paste("./Output/activity_overlap_LACI Pre-volancy-LACI Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_Myotis 40k Pre-volancy-Myotis 40k Post-volancy_",Sys.Date(),".png",sep="")))
``` 

##### Figure 17. Nightly activity overlap one month pre and post volancy for LACI and Myotis 40k.

### Acknowledgements

Thank you to Lisa Wilkinson for having the foresight and enthusiasm to start NABat monitoring in Alberta and her continued support of the program, to Jason Headley for providing the cover photo, and to all of the biologists who generously provided their time and energy in gathering and providing bat acoustic data: Barb Johnston, Brenda Shepard, Cami Hurtado, Cory Olson, Courtney Hughes, David Bruinsma, Erin Bayne, Greg Brooke, Greg Horne, Geoffrey Prophet, Helena Mahoney, Jason Unruh, Jennifer Carpenter, Lisa Wilkinson, Matina Kalcounis-Rueppell, Natalka Melnycky, Rolanda Steenweg, Saakje Hazenberg, Sandi Robertson, and Sharon Irwin. Lastly, thanks to Brandon Aubie for responding so quickly to, and troubleshooting any, issues with Alberta eBat. NEED TO ADD IN REMAINING PEOPLE


