---
title: "Alberta's North American Bat Monitoring Program (NABat) Annual Report"
author: "Created by Joanna Burgar"
date: "Report generated on `r format(Sys.time(), '%d %B, %Y')`"
output: word_document
---

```{r EPFU image, out.width="0.3\\linewidth", include=TRUE, fig.align="center", echo=FALSE}
knitr::include_graphics("./Input/JasonHeadley-LittleBrownBat2.jpg")
```

Prepared by: Joanna M. Burgar, PhD RPBio
 joburgar@gmail.com 

Prepared for: Alberta Environment & Parks

Suggested Citation: Burgar, J.M. 2022. North American Bat Monitoring Program 2020. Alberta Environment & Parks, Edmonton, Alberta. 

Cover Illustration: Myotis lucifigus - Little Brown Bat Â© Jason Headley

```{r setup, echo=F, include=F}

#Load Packages
list.of.packages <- c("data.table", "leaflet", "tidyverse", "lunar", "zoo", "colortools", "lubridate", "camtrapR", "circular", "RColorBrewer", "Cairo", "viridis", "knitr", "sf","osmdata", "ggspatial", "ggmap","gridExtra", "grid","weathercan")
# Check you have them and load them
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

lapply(list.of.packages, require, character.only = TRUE)

# Timezone 
tz <- "MST7MDT"

# Set a single catagorical variable of interest from station covariates (`sta`) for summary graphs. If you do not have and appropriate category use "Project.ID".
category <- "GRTS.Cell.ID"

# Define a colour from the R options to base the colourscheme
colour <- "lightseagreen"

# # Define the GRTS.Cell.ID and Year of interest if subsetting for year, studyarea
# GRTS_interest <- c("72071","203655")

Year_interest <- year(as.Date("2021-01-01"))

# Check the listing of output files
# fs::dir_ls(path="./Input/NABat_ProcessedFiles", recurse = TRUE)

# Import count files
# dat_count <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles",GRTS_interest,sep="/"),
#                           regexp = "\\counts.csv$", recurse = TRUE)

# # Import count files if selecting only GRTS.Cell.ID of interest
# make the changes below to add in GRTS_interest and/or Year_interest for other files as appropriate
dat_count <- fs::dir_ls(path="./Input/NABat_ProcessedFiles",regexp = "\\counts.csv$", recurse = TRUE) %>%
  map_dfr(read_csv, col_types = cols(.default = 'c'), .id = "source") %>% 
  select(-`...3`) %>%
  type_convert() %>%
  mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
  mutate(Year = year(SurveyNight), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  filter(Year==Year_interest)

dat_count$Orig.Name <- as.factor(paste(dat_count$Site, dat_count$Detector, sep="_"))
dat_count$GRTS.Cell.ID <- as.factor(word(dat_count$source,4,sep = "\\/"))
dat_count$Location.Name <- as.factor(word(dat_count$source,5,sep = "\\/"))
dat_count$Deployment.ID <- as.factor(word(dat_count$source,6,sep = "\\/"))
dat_count$Classification <- as.factor(dat_count$Classification %>% 
                                        recode(EPFU.LANO = "EPFU-LANO", LABO.MYLU = "LABO-MYLU",
                                               My_40k = "Myotis 40k", MYEV.MYSE = "MYEV-MYSE"))
dat_count$Classification <- factor(dat_count$Classification,
                                    levels = c("EPFU", "EPFU-LANO", "LANO", "LACI", "LABO", "LABO-MYLU", "MYLU",
                                               "MYCA", "MYCI", "MYEV","MYEV-MYSE", "MYSE", "MYVO", "Myotis 40k",
                                               "unknown", "noise"))
# dat_count <- dat_count %>% filter(GRTS.Cell.ID==GRTS_interest)

# Import summary files
dat_summary <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles/"), 
                            regexp = "\\_summary.csv$", recurse = TRUE) %>%
  map_dfr(~read_csv(.x, col_types = cols(.default = "c")), .id="source") %>% # issues with min_wind so read in as character
  type_convert() %>% # convert back to proper formats, min_wind still problematic so only use max or mean wind if available
  mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  filter(Year==Year_interest) %>%
  select(-Date)%>%
  rename(Orig.Name=Site)

dat_summary$GRTS.Cell.ID <- as.factor(word(dat_summary$source,4,sep = "\\/"))
dat_summary$Location.Name <- as.factor(word(dat_summary$source,5,sep = "\\/"))
dat_summary$Deployment.ID <- as.factor(word(dat_summary$source,6,sep = "\\/"))

# dat_summary <- dat_summary %>% filter(GRTS.Cell.ID==GRTS_interest)

# Create file type to differentiate zc from wav 
dat_summary$File.Type <- str_sub(dat_summary$Filename,-2,-1) %>% recode("0#" = "zc", "av"="wav")
# nrow(dat_summary)
# glimpse(dat_summary)
# summary(dat_summary)
# dat_summary$Filename[grepl("wav",dat_summary$Filename)]

# Extract the time from filename and put in date format
dat_summary$Time.temp1 <- str_extract(dat_summary$Filename,"_[0-9]{6}.wav") %>% str_sub(2,7)
dat_summary$Time.temp2 <- str_extract(dat_summary$Filename,"[0-9]{2}\\-[0-9]{2}\\-[0-9]{2} [0-9]{2}\\-[0-9]{2}\\-[0-9]{2}") %>% str_sub(-8,-1)
dat_summary$Time.temp3 <- str_extract(dat_summary$Filename,"_[0-9]{6}_") %>% str_sub(2,7)
dat_summary$Time.temp4 <- str_extract(dat_summary$Filename,"_[0-9]{6}.00#.zc") %>% str_sub(2,7)

dat_summary$Time.temp1p <- as.POSIXct(strptime(dat_summary$Time.temp1, "%H%M%S", tz))
dat_summary$Time.temp2p <- as.POSIXct(strptime(dat_summary$Time.temp2, "%H-%M-%S", tz))
dat_summary$Time.temp3p <- as.POSIXct(strptime(dat_summary$Time.temp3, "%H%M%S", tz))
dat_summary$Time.temp4p <- as.POSIXct(strptime(dat_summary$Time.temp4, "%H%M%S", tz))
dat_summary <- dat_summary %>% mutate(Timep = coalesce(Time.temp1p, Time.temp2p, Time.temp3p, Time.temp4p)) %>% 
  select(-c(Time.temp1, Time.temp2, Time.temp3, Time.temp4, Time.temp1p, Time.temp2p, Time.temp3p, Time.temp4p))
hist(dat_summary$Timep, breaks=60)
summary(dat_summary$Timep)
dat_summary %>% filter(is.na(Timep)) %>% count(Orig.Name) #issues with one of the NSR sites (0.2% overall)

# Create environmental covariate
# convert wind to numeric
wind.columns <- names(dat_summary)
wind.columns <- wind.columns[grepl("wind",wind.columns)]
dat_summary[c(wind.columns,"min_temp")] <- sapply(dat_summary[c(wind.columns,"min_temp")],as.numeric)

nightly.max.env.cov <- dat_summary %>% group_by(Location.Name,SurveyNight) %>% dplyr::summarise_at(c("max_temp","max_hum","max_wind"), list(Mean = mean))
nightly.max.env.cov <- nightly.max.env.cov %>% rename(Max.Temp = "max_temp_Mean", Max.Hum = "max_hum_Mean", Max.Wind = "max_wind_Mean")
summary(nightly.max.env.cov)
glimpse(nightly.max.env.cov)

# Read deployment data csv for station covariates
eff <- read.csv("Input/NABat_Deployment_Data.csv", header=T) %>%
  mutate(Survey.Start.Time = ymd(Survey.Start.Time), Survey.End.Time = ymd(Survey.End.Time))
eff$Survey.End.Time - eff$Survey.Start.Time
eff <- eff %>% filter(Deployment.ID>=as.numeric(Year_interest)) #%>% filter(GRTS.Cell.ID==GRTS_interest)
as.data.frame(eff %>% group_by(Deployment.ID) %>% count(GRTS.Cell.ID)) # 108 NABat cells surveyed

eff.names <- eff %>% group_by(Location.Name) %>% count(Orig.Name)
dat.names <- dat_count%>% group_by(Location.Name) %>% count(Orig.Name)
dat.names.sum <- dat_summary %>% group_by(Location.Name) %>% count(Orig.Name)
names.join <- as.data.frame(full_join(eff.names, dat.names, by="Location.Name"))
names.join <- as.data.frame(full_join(eff.names, dat.names.sum, by="Location.Name"))
no.data <- names.join %>% filter(is.na(n.y)) # will show missing deployment data
eff %>% filter(Location.Name %in% no.data$Location.Name) %>% select(Location.Name, Orig.Name, Detector.Failure.Details)

### NO DATA (i.e., equipment malfunction) for 12 sites and no data sent for 1 site
# 132435_SW_01 - errors on card (malfunction)
# 14234_SE_01 - malfunction in 2021
# 14234_SW_01 - malfunction in 2021
# 152915_NE_01 - no data in 2021
# 160106_SE_01 - no data in 2021
# 3667_NW_01 - no submission in 2021
# 41322_SE_01 - malfunction in 2021
# 90474_NW_01 - malfunction in 2021
# 922 - malfunction in 2021 (all 4 sites)
# 9626_NE_01 - all noise in 2021 (malfunction?)

NABat.stns.to.use <- unique(eff$Location.Name)

# Read station covariates csv
sta <- read.csv("Input/NABat_Station_Covariates.csv", header=T, na.string=c("","NA", "<NA>",-1)) %>%
  # dplyr::select(OBJECTID, LandUnitCo, GRTS.Cell.ID, LocName, Cardinal_D, Stn_Num, Orig_Name, NABat_Samp, YrsSurveye, LC_class, NSRNAME, NRNAME,
  #               WTRBODY_TY, DIST_WTRBD, STREAM_TYP, Dist_Strea, RoadType, DistRoad_M, HF_TYPE, Dist_to_HF, Latitude, Longitude) %>%
  rename(GRTS.Cell.ID=GRTSCellID, Location.Name=LocName, Waterbody.Type=WB_type, Waterbody.Distance=WB.dist, Road.Type=RD_Type, Road.Distance=RD.dist, Natural.Region=NRNAME)

# sta <- sta %>% filter(GRTS.Cell.ID %in% GRTS_interest)

sta$NP <- as.factor(sta$NP)
sta$NP <- sta$NP %>% relevel(ref="In")
sta$Land.Cover <- as.factor(sta$Land.Cover)

# sta %>% filter(GRTS.Cell.ID=="336731")
# levels(sta$Land.Cover)
# sta$Land.Cover <- sta$Land.Cover %>% recode("20"="Water", "33"="Exposed Land", "34"="Developed", "50"="Shrubland", "110"="Grassland","120"="Agriculture", "210"="Coniferous Forest", "220"="Broadleaf Forest", "230"="Mixed Forest")

sta %>% filter(!Location.Name %in% NABat.stns.to.use) %>% filter(Surveyed2021=="yes")
sta <- sta %>% filter(Surveyed2021=="yes")

# Land-Use Type (Agriculture, Barren Land, Forest, Grassland, Shrubland, Urban, Water, Wetland)
# not enough Urban, keep as "Developed"
# not enough Wetland for separate category - combine Water and Wetland
# combine 3 forest types for Forest
# if waterbody < 10 m, then change Land Use Type to Water, otherwise go with ABMI class
# Consolidate Land.Cover to Agriculture, Barren (Exposed) Land, Forest, Grassland, Shrubland, Developed (incl Urban), Water/Wetland 

sta %>% filter(Land.Cover=="Water")

sta$Land.Use.Type <- sta$Land.Cover %>% recode("Coniferous Forest" = "Forest-conifer", "Broadleaf Forest" = "Forest-deciduous", "Mixed Forest" = "Forest-mixed", "Exposed Land" = "Barren Land")
# sta$Land.Use.Type <- case_when(sta$Waterbody.Distance < 10 ~ "Water",
#                                # grepl("URBAN", sta$Human.Footprint.Type) ~ "Urban",
#                                TRUE ~ as.character(sta$Land.Use.Type))

sta$Land.Unit.Code <- recode(sta$LandUnitCo, "BNP"="BANP", "JNP"="JANP","LPR"="LOPR","LAR"="LOAR",
                                         "NSR"="NOSR","RDR"="REDR","SSR"="SOSR","UAR"="UPAR","UPR"="UPPR")


eff <- left_join(eff %>% select(-Land.Unit.Code), 
                 sta %>% dplyr::select(Location.Name, Land.Unit.Code, Natural.Region, NP, Land.Use.Type))
eff %>% filter(Deployment.ID=="2021") %>% count(GRTS.Cell.ID) # 108 GRTS cells surveyed
nrow(eff %>% filter(Deployment.ID=="2021")) # 172 sites

sta.sites <- sta %>% filter(Surveyed2021=="yes") %>% select(Location.Name); nrow(sta.sites) #172
eff.sites <- eff %>% filter(Deployment.ID=="2021") %>% select(Location.Name, Orig.Name); nrow(eff.sites) #172
names.join <- as.data.frame(full_join(sta.sites, eff.sites))
eff.sites %>% filter(!Location.Name %in% sta.sites$Location.Name)

# Read distance matrix csv
dist.mat <- read.csv("DistanceMatrixTable_2021.csv", header=T, row.names=1)
dim(dist.mat)
colnames(dist.mat)[1:nrow(sta)] <- sta$Location.Name
dist.mat$Location.Name <- sta$Location.Name
# sta %>% filter(Surveyed2021=="no")
# nrow(sta)
# nrow(dist.mat)
dist.mat$NP <- sta$NP[match(dist.mat$Location.Name, sta$Location.Name,)]
dist.mat$Natural.Region <- sta$Natural.Region[match(dist.mat$Location.Name, sta$Location.Name,)]
dist.mat$Land.Unit.Code <- sta$Land.Unit.Code[match(dist.mat$Location.Name, sta$Location.Name,)]
# dist.mat <- dist.mat[complete.cases(dist.mat),] # removes any entries without corresponding sta data
summary(dist.mat)

# Read covariate proportion csv
cov.prop <- read.csv("Input/Cov.area.csv", header=T)

##############################################################
##### DATA TESTS #############################################
##############################################################

# This code will not work unless the data passes the following checks

# 1) All dates must be in YYYY-MM-DD in 'eff' and YYYY-MM-DD HH:MM:SS in 'dat' 
# If either of the following return NA, the formatting needs to change
# strptime(eff$Survey.Start.Time[1], "%Y-%m-%d", tz="UTC")
strptime(dat_count$SurveyNight[1], "%Y-%m-%d", tz="UTC")
strptime(dat_summary$SurveyNight[1], "%Y-%m-%d", tz="UTC")

# 2) ensure all deployment starting dates are before deployment retreival dates
# Logic = are the stations active for 0 or more days -> all should read TRUE
# didn't input this year (2021), will use first and last date bat recorded instead
table((strptime(eff$Survey.End.Time, "%Y-%m-%d", tz="UTC")-strptime(eff$Survey.Start.Time, "%Y-%m-%d", tz="UTC"))>=0)

# 3) Ensure Deployment.ID (year) is the same as SurveyNight year
dat_count %>% group_by(Deployment.ID) %>% count(Year)
dat_count$unique <- paste(dat_count$Location.Name, dat_count$Year, sep="_")
# dat_count$SurveyNight <- case_when(dat_count$unique=="139715_SW_02_2000" ~ dat_count$SurveyNight + 7456,
#                                TRUE ~ as.Date(dat_count$SurveyNight))
dat_count <- dat_count %>%  
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-unique)

dat_count %>% group_by(Deployment.ID) %>% count(Year) 
# no discrepancy between SurveyNight and Deployment.ID

dat_summary$unique <- paste(dat_summary$Location.Name, dat_summary$Year, sep="_")
# dat_summary$SurveyNight <- case_when(dat_summary$unique=="139715_SW_02_2000" ~ dat_summary$SurveyNight + 7456,
#                                TRUE ~ as.Date(dat_summary$SurveyNight))
dat_summary <- dat_summary %>%  
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-unique)

dat_summary %>% group_by(Deployment.ID) %>% count(Year) # no discrepancy between SurveyNight and Deployment.ID
as.data.frame(dat_summary %>% filter(is.na(Year)))

# 4a) Do you have all stations represented in both the deployment data and station covariates? If yes, the value should be 0
# If length > 0, then you have some data missing!
length(setdiff(unique(eff$Location.Name), unique(sta$Location.Name))) # 0
missing.eff.sta <- left_join(eff, sta, by="Location.Name") #%>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.eff.sta[is.na(missing.eff.sta$Orig.Name),] # will show missing station covariates

# 4b) Do you have all data represented in the station covariates? If yes, the value should be 0
length(setdiff(unique(sta$Location.Name), unique(dat_count$Location.Name))) # 13
missing.dat.sta <- left_join(sta, dat_count, by="Location.Name") %>% dplyr::select(Location.Name, Orig_Name, Orig.Name)
missing.data.names <- missing.dat.sta[is.na(missing.dat.sta$Orig.Name),]  # will show missing count data (may just not have data for certain years)

eff %>% count(Location.Name) #172 stations (13 with malfunctions or no files sent)
eff %>% count(GRTS.Cell.ID) # 108

# 4c) Do you have all data represented in the deployment data? If yes, the value should be 0
length(setdiff(unique(eff$Location.Name), unique(dat_count$Location.Name))) # 13
missing.dat.eff <- left_join(eff, dat_count, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.dat.eff[is.na(missing.dat.eff$Orig.Name.y),] # will show missing deployment data covariates


eff %>% filter(!Location.Name %in% missing.data.names$Location.Name) %>% count(Location.Name) # 159 stations with data
eff %>% filter(!Location.Name %in% missing.data.names$Location.Name) %>% count(GRTS.Cell.ID) # 102 grid cells with data

# For 2021, Fenlands data was not sent - first time catching this so will call it a "no files sent and ask Barb for files for submission"

# 5) Remove dates outside survey range (i.e., ARUs turned on but not deployed or deployed well past NABat guidelines)
eff %>% group_by(Deployment.ID) %>% summarise(min(Survey.Start.Time), max(Survey.End.Time))
# remove all dates before May 01 and after Sept 10, using Julian date
yday("2021-05-01"); yday("2021-09-10")

dat_count %>% group_by(Deployment.ID) %>% count(Year) 
dat_count <- dat_count %>% filter(between(jDay, 121, 253))

dat_summary %>% group_by(Deployment.ID) %>% count(Year) 
dat_summary <- dat_summary %>% filter(between(jDay, 121, 253))

# 6) # check to make sure pulling times from start and end of day
dat_summary %>% group_by(File.Type) %>% summarise(min(Timep, na.rm=T), max(Timep, na.rm=T))
# sum(is.na(dat_summary$Timep)) / nrow(dat_summary) # almost 20% having issues with filename (only 3 % in 2019; 0.7% in 2021)

# hist(dat_summary$Timep, breaks=50)
# hist(dat_summary[dat_summary$File.Type=="zc",]$Timep, breaks=60)
# will have some NAs in dat_summary as not all filenames contained time, but if small proportion then ok to go ahead

# If all of the above is satisfied -> press 'Knit' above ^
```

```{r non-adjustable options, echo=F, include=F}
# SE function
se <- function(x) sqrt(var(x)/length(x))

# Determine distance between bat survey locations
# commented out code using ArcGIS generated (old) code - use new script below for R generated distance matrix
# dist.mat$count <- 1 # to count number of survey locations within x distance of original survey location
# dist.mat <- as.data.frame(dist.mat %>%
#                                  group_by(NP, Natural.Region, Land.Unit.Code, Location.Name) %>% 
#                                  summarise_at(c("Link.Distance..km."), list(Mean = mean, SE = se, Min = min, Max = max)))
# dist.mat.100km <- left_join(dist.mat.100km, dist.mat %>% filter(Link.Distance..km.<=10) %>% group_by(ORIGIN_Location.Name) %>% count(count))
# dist.mat.100km <- dist.mat.100km %>% select(-count)
# dist.mat.100km$n <- dist.mat.100km$n %>% replace_na(0) # the number of survey locations within 10 km
# mean(dist.mat.100km$n); se(dist.mat.100km$n) # the mean and se of survey locations wtihin 10 km
# dist.mat.100km %>% filter(n==0) %>% nrow() # 19 stations with next nearest station > 10 km away

dist.mat$stn_dist_mean_km <- (rowMeans(dist.mat[1:nrow(sta)], na.rm = T))/1000
dist.mat$stn_dist_min_km <- (apply(dist.mat[1:nrow(sta)], 1, FUN=min, na.rm = T))/1000
dist.mat$stn_dist_max_km <- (apply(dist.mat[1:nrow(sta)], 1, FUN=max, na.rm = T))/1000
dist.mat %>% filter(stn_dist_min_km <1) %>% select(Location.Name)

dist.mat %>% summarise(min(stn_dist_min_km), mean(stn_dist_min_km), max(stn_dist_min_km))
dist.mat %>% summarise(se(stn_dist_mean_km), se(stn_dist_min_km),se(stn_dist_max_km))


# Count the total number of detector stations and GRTS cells
n.stat <- length(unique(sta$Location.Name))
n.GRTS <- length(unique(sta$GRTS.Cell.ID))

# Find mean and SE detectors per GRTS cell and number of detectors per quadrant
det.per.GRTS <- sta %>% group_by(GRTS.Cell.ID) %>% count(GRTS.Cell.ID)
det.per.quad <- sta %>% count(Stn_Num)

det.per.GRTS %>% ungroup() %>% count(n)

# Count the total number of stations sampled for NABat
# names(sta)
NABat.smp <- sta %>% count(NABat_Samp)
NABat.smp.yes <- NABat.smp[NABat.smp$NABat_Samp=="Yes",]$n
NABat.smp.no <- NABat.smp[NABat.smp$NABat_Samp=="No",]$n

# Find mean and SE for years surveyed per GRTS cell
yrs.surveyed.GRTS <- sta %>% group_by(GRTS.Cell.ID) %>% count(Surveyed2021)
se(sta$YrsSurveyed)

# ARUs by NR, LUC and LUT
ARUs.by.NP <- sta %>% filter(NP=="In") %>% count(Land.Unit.Code, sort = TRUE)
ARUs.by.NR <- sta %>% count(Natural.Region, sort = TRUE)
ARUs.by.LUC <- sta %>% filter(NP=="Out") %>% count(Land.Unit.Code, sort = TRUE)
ARUs.by.LUT <- sta %>% count(Land.Cover, sort = TRUE)

# Proportion of stations vs spatial covariates
sum(ARUs.by.LUC$n) # 146
sum(ARUs.by.NP$n) # 25
sum(ARUs.by.NR$n) # 172
sum(ARUs.by.LUT$n) # 172
colnames(cov.prop)[5] <- "prop.area"
cov.prop$NP <- as.factor(ifelse(grepl("NP",cov.prop$Name), "In", "Out"))
#cov.prop <- cov.prop %>% arrange(NP, Cov, Name)
cov.prop <- left_join(cov.prop,ARUs.by.LUC,by=c("Name"="Land.Unit.Code"))
cov.prop <- left_join(cov.prop,ARUs.by.NP,by=c("Name"="Land.Unit.Code"))
cov.prop <- left_join(cov.prop,ARUs.by.NR,by=c("Name"="Natural.Region"))
cov.prop <- left_join(cov.prop,ARUs.by.LUT,by=c("Name"="Land.Cover"))

cov.prop$Num.Stn <- rowSums(cov.prop[7:10],na.rm=T)
cov.prop$prop.stn <- ifelse(cov.prop$Cov=="Land.Unit.Code" & cov.prop$NP=="Out", cov.prop$Num.Stn / sum(ARUs.by.LUC$n),
                            ifelse(cov.prop$Cov=="Land.Unit.Code" & cov.prop$NP=="In", cov.prop$Num.Stn / sum(ARUs.by.NP$n),
                                   ifelse(cov.prop$Cov=="Natural.Region", cov.prop$Num.Stn / sum(ARUs.by.NR$n), 
                                          ifelse(cov.prop$Cov=="Land.Use.Type", cov.prop$Num.Stn / sum(ARUs.by.LUT$n),NA))))
cov.prop <- cov.prop %>% select(-c(n.x, n.y, n.x.x, n.y.y))

# Generate colours to display the category levels - R needs them as a factor
names(sta)
sta[,category] <- factor(sta[,category])
col.cat <- wheel(colour, num = length(levels(sta[,category])))
sta$Cols <- col.cat[sta[,category]]

col.catNR <- wheel(colour, num = length(unique(sta[,"Natural.Region"])))
sta$ColsNR <- col.catNR[sta[,"Natural.Region"]]

sta[,"Land.Unit.Code"] <- factor(sta[,"Land.Unit.Code"])
col.catLUC <- wheel(colour, num = length(unique(sta[,"Land.Unit.Code"])))
sta$ColsLUC <- col.catLUC[sta[,"Land.Unit.Code"]]

sta[,"Land.Use.Type"] <- factor(sta[,"Land.Use.Type"])
col.catLUT <- wheel(colour, num = length(unique(sta[,"Land.Use.Type"])))
col.catLUT <- col.catLUT[order(col.catLUT, decreasing = TRUE)] # have Water as blue
sta$ColsLUT <- col.catLUT[sta[,"Land.Use.Type"]]

col.catYr <- wheel(colour, num = 6)
col.catYr <- col.catYr[order(col.catYr, decreasing = TRUE)]

###--- add covariates to dat objects
dat_count$NP <- sta$NP[match(dat_count$Location.Name, sta$Location.Name,)]
dat_count$Natural.Region <- sta$Natural.Region[match(dat_count$Location.Name, sta$Location.Name,)]
dat_count$Land.Unit.Code <- sta$Land.Unit.Code[match(dat_count$Location.Name, sta$Location.Name,)]
dat_count$Land.Use.Type <- sta$Land.Use.Type[match(dat_count$Location.Name, sta$Location.Name,)]

###--- create volancy covariate
July10 <- yday("2021-07-10") # 191 = July 10 (general date of volancy)
# if using one date for entire province, relevel with "Pre" as first category
# dat_count$Volancy <- "Post-volancy"
dat_count$Volancy <- as.factor(ifelse(dat_count$jDay<July10, "Pre-volancy","Post-volancy")) %>% relevel("Pre-volancy")
dat_count %>% group_by(Volancy, Deployment.ID) %>% summarise(min(jDay), max(jDay))

# dat_summary$Volancy <- "Post-volancy"
dat_summary$Volancy <- as.factor(ifelse(dat_summary$jDay<July10, "Pre-volancy","Post-volancy")) %>% relevel("Pre-volancy")
dat_summary %>% group_by(Volancy, Deployment.ID) %>% summarise(min(jDay), max(jDay))

col.catVol <- as.character(c("#2028B2","#B2AA20"))

# Create a call df without the noise files
call_count <- dat_count %>% filter(Classification!="noise")
unknown.calls <- call_count %>% filter(Classification=="unknown") %>% summarise(sum(Count)) / sum(call_count$Count)

# Determine total effort (survey nights)
total.effort <- call_count %>% group_by(Location.Name, Deployment.ID) %>% summarise_at(c("SurveyNight"), list(Min = min, Max=max))
total.effort$Diff <- (total.effort$Max - total.effort$Min)+1
sum(total.effort$Diff) # 3047 survey nights


sta %>% filter(Location.Name %in% missing.data.names$Location.Name)  # 13 stations without data - some summary stats

sta %>% filter(Location.Name %in% missing.data.names$Location.Name) %>% group_by(LandUnitCo) %>% count(NP, Land.Use.Type, Natural.Region, GRTS.Cell.ID) # 13 stations without data - some summary stats

```

``` {r temporal summaries, include=F}

# remove call files without time
# dat_summary %>% filter(is.na(Timep)) %>% count(Location.Name)
# sta %>% filter(GRTS.Cell.ID=="171651")
# dat_summaryT %>% filter(Location.Name=="171651_NW_01")
# 563/(4734+563)
# 10% of 171651_NW_01 removed due to time stamp issues (563 files removed)

dat_summaryT <- dat_summary[complete.cases(dat_summary$Timep),]

# now use Alberta eBat criteria to classify species
dat_sum_sub <- dat_summaryT[c("GRTS.Cell.ID","Location.Name","SurveyNight","Year","Month","jDay","Volancy","Timep",
                              "Filename","n_calls","prob1","sp1","prob2","sp2","prob3","sp3")]
nrow(dat_sum_sub) # 290,775 files
dat_sum_sub$Location.Name.Year <- paste(dat_sum_sub$Location.Name, dat_sum_sub$Year, sep="_")

# check for erroneous time stamps and remove entire survey
Timepdate <- date(Sys.time())
Timepdatetime1 <- as.POSIXct(paste(Timepdate,"08:00:00"), tz)
Timepdatetime2 <- as.POSIXct(paste(Timepdate,"18:00:00"),tz)

# 3 stations and 39 surveys with calls between 8 am and 6 pm - remove these survey periods from the temporal analysis
timestamp.error <- as.data.frame(dat_sum_sub %>% filter(Timep >Timepdatetime1 & Timep<Timepdatetime2) %>% filter(sp1!="noise") %>% group_by(Location.Name.Year) %>% summarise(min(SurveyNight), max(SurveyNight)))
unique(timestamp.error$Location.Name.Year)
unique(timestamp.error$Location.Name)

dat_sum_sub %>% filter(Location.Name.Year %in% timestamp.error$Location.Name.Year) %>% count(GRTS.Cell.ID)
sta %>% filter(GRTS.Cell.ID %in% c(220611, 32195))

# keep in these 3 surveys as only some of the times are off...maybe just picked up som noise calls throughout the day or had some early flying bats (seems like most are around 5 pm)
# dat_sum_sub <- dat_sum_sub %>% filter(!Location.Name.Year %in% timestamp.error$Location.Name.Year)

# subset data to one month pre and one month post volancy (July 10)
# dat_sum_sub <- dat_sum_sub %>% filter(between(jDay, 161,222))
# nrow(dat_sum_sub) # now down to 123837

# create thresholds for noise and bat classificaitons
threshold_noise <- 0.8; threshold_bat <- 0.5
# start with all call sequences as "unknown"
dat_sum_sub$category <- "unknown"

# Index cases to be categorized as noise
index_noise <- with(dat_sum_sub, (sp1 == "noise") & (prob1 > threshold_noise)) 
# Set the indexed categories to "noise" 
dat_sum_sub$category[index_noise] <- "noise"

# Index the noise values to be filtered out
index_remove_noise <- with(dat_sum_sub, (category == "unknown") & (sp1 == "noise")) 

# Note that there are 126 `NA` values
sum(is.na(index_remove_noise)) 
# These `NA` values are due to an `NA` in the `sp1` column which is the result of a tie in the random forest probabilities
dat_sum_sub[is.na(index_remove_noise),]

# Set any `NA` value to `FALSE` (i.e. not a noise value to be filtered out)
index_remove_noise[is.na(index_remove_noise)] <- FALSE 
# For the noise values to be filtered out, get the corresponding probabilities
prob_noise <- dat_sum_sub[index_remove_noise, "prob1"] 
# Record the state of the data frame before the filtering out of noise
dat_sum_sub_before_noise_removal <- dat_sum_sub

dat_sum_sub[index_remove_noise, "sp1"] <- dat_sum_sub[index_remove_noise, "sp2"]
dat_sum_sub[index_remove_noise,"sp2"] <- dat_sum_sub[index_remove_noise, "sp3"] 
dat_sum_sub[index_remove_noise, "prob1"] <- dat_sum_sub[index_remove_noise, "prob2"] 
dat_sum_sub[index_remove_noise,"prob2"] <- dat_sum_sub[index_remove_noise, "prob3"] 

# Now that the `prob3` value has been shifted to the `prob2` column, replace the `prob3` column with `NA` values, for the cases where noise has been filtered out
dat_sum_sub[index_remove_noise, "prob3"] <- NA 
dat_sum_sub[index_remove_noise, "sp3"] <- NA

dat_sum_sub[index_remove_noise, "prob1"] <- dat_sum_sub[index_remove_noise, "prob1"] / (1 - prob_noise) 
dat_sum_sub[index_remove_noise, "prob2"] <- dat_sum_sub[index_remove_noise, "prob2"] / (1 - prob_noise) 
# Compare "sp1" columns before and after filtering out noise
table(dat_sum_sub_before_noise_removal$sp1)
table(dat_sum_sub$sp1)

index_bat_species <- with(dat_sum_sub, (category == "unknown") & (n_calls >= 3) & (prob2 / prob1 <= 0.80)) 

index_bat_species[is.na(index_bat_species)] <- FALSE # to deal with the NA values

dat_sum_sub$category[index_bat_species] <- dat_sum_sub$sp1[index_bat_species]
# dat_sum_sub$sp1 <- dat_sum_sub$sp1 %>% replace_na("unknown")
# dat_sum_sub$sp2 <- dat_sum_sub$sp2 %>% replace_na("unknown")

eBat_param <- read.csv("./Input/eBat_param.csv")
dim(eBat_param)
tmp <- as.data.frame(lapply(eBat_param[,3:13], function(y) gsub("Â±.*$", "", y)))
param <- eBat_param$Parameter
eBat_param <- tmp %>% mutate_at(vars(1:11), as.numeric) # make sure all call data is numeric
eBat_param <- as.data.frame(t(eBat_param))
colnames(eBat_param) <- param
eBat_param %>% arrange(Fmin) %>% select(Fmin)
eBat_param$sp <- rownames(eBat_param)
eBat_param <- eBat_param %>% filter(sp!="noise")

# https://pubs.usgs.gov/of/2018/1068/ofr20181068.pdf
# HighF = min freq >30
# 40k = min freq 35-45
# Myotis40k = Myotis 35-40 min freq
# LowF = min freq <30
# 25k = min freq 15-25

Q25_sp <- as.character(eBat_param %>% filter(Fmin<25) %>% select(sp))
Q40_sp <- as.character(unlist(eBat_param %>% filter(Fmin>=35 & Fmin<=45) %>% select(sp)))
LowF_sp <- as.character(unlist(eBat_param %>% filter(Fmin<=30) %>% select(sp)))
HighF_sp <- as.character(unlist(eBat_param %>% filter(Fmin>30) %>% select(sp)))

Q25_index <- (dat_sum_sub$sp1 %in% Q25_sp) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob1 > 0.2)
Q40_index <- (dat_sum_sub$sp1 %in% Q40_sp) & (dat_sum_sub$sp2 %in% Q40_sp) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)
LowF_index <- (dat_sum_sub$sp1 %in% LowF_sp) & (dat_sum_sub$sp2 %in% LowF_sp) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)
HighF_index <- (dat_sum_sub$sp1 %in% HighF_sp) & (dat_sum_sub$sp2 %in% HighF_sp) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

LABO.MYLU_index <- (dat_sum_sub$sp1 %in% c("LABO", "MYLU")) & (dat_sum_sub$sp2 %in% c("LABO", "MYLU")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

EPFU.LANO_index <- (dat_sum_sub$sp1 %in% c("EPFU", "LANO")) & (dat_sum_sub$sp2 %in% c("EPFU", "LANO")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

MYEV.MYSE_index <- (dat_sum_sub$sp1 %in% c("MYEV", "MYSE")) & (dat_sum_sub$sp2 %in% c("MYEV", "MYSE")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

My_40k_index <- (dat_sum_sub$sp1 %in% c("MYCI", "MYLU", "MYVO")) & (dat_sum_sub$sp2 %in% c("MYCI", "MYLU", "MYVO")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)


#' Check the number of call sequences belonging to each category:
sum(HighF_index, na.rm=T)
sum(Q40_index, na.rm=T)
sum(My_40k_index, na.rm=T)
sum(LABO.MYLU_index, na.rm=T)
sum(MYEV.MYSE_index, na.rm=T)

sum(LowF_index, na.rm=T)
sum(Q25_index, na.rm=T)
sum(EPFU.LANO_index, na.rm=T)

#' Define a new data frame:  
dat_sum_sub_df <- dat_sum_sub
dat_sum_sub_df$category <- dat_sum_sub_df$sp1

# for all species with the "unknown" classification
# reclassify going from coarser to finer resolution of ID
dat_sum_sub_df %>% count(category)

dat_sum_sub_df$category[dat_sum_sub_df$n_calls < 3 | 
                          dat_sum_sub_df$prob1 < threshold_bat & dat_sum_sub_df$sp1 != "noise"] <- "unknown"

dat_sum_sub_df$category[HighF_index] <- "HighF"
dat_sum_sub_df$category[Q40_index] <- "40k"
dat_sum_sub_df$category[My_40k_index] <- "My_40k"

dat_sum_sub_df$category[LABO.MYLU_index] <- "LABO.MYLU"
dat_sum_sub_df$category[MYEV.MYSE_index] <- "MYEV.MYSE"

dat_sum_sub_df$category[LowF_index] <- "LowF"
dat_sum_sub_df$category[Q25_index] <- "25k"
dat_sum_sub_df$category[EPFU.LANO_index] <- "EPFU.LANO"

# dat_sum_sub_df <- dat_sum_sub_df %>% filter(n_calls<3)


###--- Visualization of overlapping species detection data
# use overlap and circular R packages

# create new df with select columns, remove noise files and all classifications with <3 calls, and add in some covariates
# nrow(dat_sum_sub_df) - nrow(dat_time) # 48,393

dat_time <- dat_sum_sub_df %>% filter(n_calls>2) %>% select(-n_calls:-sp3) %>% filter(category!="noise")
# unique(dat_time$category)
# dat_time %>% count(category)


dat_time$Classification <- as.factor(dat_time$category %>% 
                                         recode(EPFU.LANO = "EPFU-LANO", LABO.MYLU = "LABO-MYLU",
                                                My_40k = "Myotis 40k", MYEV.MYSE = "MYEV-MYSE"))

# levels(dat_time$Classification)
dat_time$Classification <- factor(dat_time$Classification,
                                     levels = c("EPFU", "EPFU-LANO", "LANO", "LACI", "LABO", "LABO-MYLU", "MYLU",
                                                "MYCA", "MYCI", "MYEV","MYEV-MYSE", "MYSE", "MYVO", "Myotis 40k",
                                                "40k","25k","HighF","LowF","unknown"))

dat_time <- as.data.frame(dat_time) 
dat_time$Time <- format(dat_time$Timep, format = "%H:%M:%S")

# covariates for overlap
dat_time$Classification_Volancy <- as.factor(paste(dat_time$Classification, dat_time$Volancy))
dat_time$NP <- sta$NP[match(dat_time$Location.Name, sta$Location.Name,)]
dat_time$Land.Use.Type <- sta$Land.Use.Type[match(dat_time$Location.Name, sta$Location.Name,)]
dat_time$Natural.Region <- sta$Natural.Region[match(dat_time$Location.Name, sta$Location.Name,)]


dat_time %>% count(Classification)
nrow(dat_time) # 242994
dat_count %>% group_by(Classification) %>% summarise(sum(Count))
dat_count %>% filter(Classification!="noise") %>% summarise(sum(Count)) # 264317
# difference is that dat_time has filtered out all "calls" with <3 pulses
# inclined to go with dat_time as splits unknown down a bit

# glimpse(dat_count)
# names(dat_time)

dat_time_agg <- dat_time %>% group_by(GRTS.Cell.ID, Location.Name, Volancy, SurveyNight) %>% count(Classification)
colnames(dat_time_agg)[6] <- "Count"
dat_time_agg$Count <- as.numeric(dat_time_agg$Count)
dat_time_agg$Year <- as.factor(year(dat_time_agg$SurveyNight))
dat_time_agg$Month <- month(dat_time_agg$SurveyNight)
dat_time_agg$jDay <- yday(dat_time_agg$SurveyNight)
dat_time_agg$NP <- sta$NP[match(dat_time_agg$Location.Name, sta$Location.Name,)]
dat_time_agg$Natural.Region <- sta$Natural.Region[match(dat_time_agg$Location.Name, sta$Location.Name,)]
dat_time_agg$Land.Unit.Code <- sta$Land.Unit.Code[match(dat_time_agg$Location.Name, sta$Location.Name,)]
dat_time_agg$Land.Use.Type <- sta$Land.Use.Type[match(dat_time_agg$Location.Name, sta$Location.Name,)]


dat_time_agg %>% ungroup() %>% summarise(min(SurveyNight), max(SurveyNight))
dat_time_agg %>% ungroup() %>% count(Classification)
dat_time_agg %>% ungroup() %>% summarise(sum(Count))
dat_time_agg %>% ungroup() %>% count(GRTS.Cell.ID)
unique(dat_time_agg$Location.Name)
length(unique(dat_time_agg$SurveyNight))

# glimpse(dat_time_agg)
# summary(dat_time_agg)
# dat_time_agg %>% ungroup() %>% summarise(sum(Count)) 

# Create a call df without the noise files
call_count <- dat_time_agg %>% ungroup () # no noise files but keeping this to keep the same code below, to use with dat_tmime_agg
unknown.calls <- call_count %>% filter(Classification=="unknown") %>% summarise(sum(Count)) / sum(call_count$Count)

### temporal summaries
# create activity overlap plots for all bats in/out of parks 

# create function to produce activity overlap for all classifications pre and post volancy 
volancy_overlap.fn <- function(Classification=Classification){
  overlap <- activityOverlap(recordTable = dat_time,
                             speciesA = paste(Classification,"Pre-volancy"),
                             speciesB = paste(Classification,"Post-volancy"),
                             speciesCol = "Classification_Volancy",
                             recordDateTimeCol = "Time",
                             recordDateTimeFormat = "%H:%M:%S",
                             xcenter="midnight",
                             overlapEstimator = "Dhat4",
                             plotR = TRUE,
                             writePNG = TRUE,
                             plotDirectory = ("./Output"),
                             addLegend = TRUE,
                             legendPosition = "topleft",
                             pngMaxPix   = 1000,
                             linecol     = c("black", "blue"),
                             linewidth   = c(3,3),
                             linetype    = c(1, 2),
                             olapcol     = "darkgrey",
                             add.rug     = TRUE,
                             extend      = "lightgrey",
                             ylim = c(0,0.35),
                             main = paste(Classification,"Pre and Post Volancy"))
  wwt <- watson.wheeler.test(list(overlap$densityA, overlap$densityB))
  return(wwt)
}


save.image(paste("NABat_Annual_Report_",Year_interest,".RDS", sep=""))

# run through each classification, except "unknown" and any classification with <100 calls
unknown.call.types <- c("unknown","25k","40k","LowF","HighF")
class.list <- as.data.frame(dat_time %>% filter(!Classification %in% unknown.call.types) %>% count(Classification, sort=TRUE) %>% 
                              filter(n>100) %>% select(Classification))
class.list <- sort(class.list$Classification)
EPFU.WWT <- volancy_overlap.fn(Classification=class.list[1])
EPFU.LANO.WWT <- volancy_overlap.fn(Classification=class.list[2])
LANO.WWT <- volancy_overlap.fn(Classification=class.list[3])
LACI.WWT <- volancy_overlap.fn(Classification=class.list[4])
LABO.WWT <- volancy_overlap.fn(Classification=class.list[5])
LABO.MYLU.WWT <- volancy_overlap.fn(Classification=class.list[6])
MYLU.WWT <- volancy_overlap.fn(Classification=class.list[7])
Myotis40k.WWT <- volancy_overlap.fn(Classification=class.list[8])

```
### Introduction


```{r WNS spread png map, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap="White-nose syndrome spread up to 2021", echo=FALSE}
knitr::include_graphics("./Input/WNS_spread_map_2022-02-26.png")

```

##### Figure 1. The spread of White-Nose Syndrome up to the spring of 2021; downloaded from https://www.whitenosesyndrome.org/where-is-wns.



#### Summary Statistics
All analyses were conducted in R (version 4.1.2; 2021), within RStudio (version 1.2.5033) using rmarkdwon (version 2.11; Allaire et al. 2020) for reproducible results. Unless otherwise noted, results are reported as the mean Â± 1 SE. 

### Results â Survey Representation
In `r Year_interest` there were passive acoustic surveys at `r n.stat` unique locations (i.e., stations) within `r n.GRTS` NABat grid cells (Figure 1). On average, each grid cell had `r round(mean(det.per.GRTS$n),1)` Â± `r round(se(det.per.GRTS$n),1)` (1 SE) stations, with between `r min(det.per.quad[,1])` (`r det.per.quad[det.per.quad$Stn_Num==1,]$n` stations) and `r max(det.per.quad[,1])` (`r det.per.quad[det.per.quad$Stn_Num==4,]$n` station) stations per quadrat. The average minimum distance between stations was `r round(mean(dist.mat$stn_dist_min_km),1)` km Â± `r round(se(dist.mat$stn_dist_min_km),1)`. Neighbouring stations ranged from  `r round(min(dist.mat[dist.mat$stn_dist_min_km!=0,]$stn_dist_min_km),1)` km to `r round(max(dist.mat[dist.mat$stn_dist_min_km!=0,]$stn_dist_min_km),1)` km apart.  While most (i.e., `r NABat.smp.yes` or `r round(NABat.smp.yes/nrow(sta)*100,0)`%) stations were surveyed specifically for NABat monitoring, some (i.e., `r NABat.smp.no` or `r round(NABat.smp.no/nrow(sta)*100,0)`%) stations were surveyed following similar NABat protocols but not for NABat monitoring per se (e.g., outside of the recommended survey temporal window or for other objectives such as monitoring migratory routes [REDR] or to collect WNS baseline data [BANP, WLNP]).

```{r Table 1 bat survey location, echo=F}

sta.count <- sta %>% count(NP, Natural.Region, Land.Unit.Code)

sta.count2 <- sta %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% count(GRTS.Cell.ID)
sta.count2$GRTS.count <- 1
sta.count3 <- as.data.frame(sta.count2 %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% summarise(sum(GRTS.count)))

sta.count4 <- sta %>% filter(NABat_Samp=="Yes") %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% count(NABat_Samp)

sta.NP <-sta %>% filter(NP=="In") %>% count(Land.Unit.Code)
sta.NP$prop <- sta.NP$n / sum(sta.NP$n)
sta.NP <- arrange(sta.NP, n)

sta.NR <-sta %>% count(Natural.Region)
sta.NR$prop <- sta.NR$n / sum(sta.NR$n)
sta.NR <- arrange(sta.NR, n)

sta.count$Count.Grid.Cells <- as.vector(sta.count3[,4])
sta.count <- left_join(sta.count, sta.count4 %>% select(-NABat_Samp), 
                  by = c("NP" = "NP", "Natural.Region" = "Natural.Region", "Land.Unit.Code" = "Land.Unit.Code"))

dist.mat.LUC <- dist.mat %>% group_by(Land.Unit.Code) %>% summarise(`Min Stn Dist (km)` = min(stn_dist_min_km),
                                                                    `Mean Stn Dist (km)` = mean(stn_dist_min_km), 
                                                                    `Max Stn Dist (km)` = max(stn_dist_min_km))

sta.count <- left_join(sta.count, dist.mat.LUC)
colnames(sta.count)[1:6] <- c("National Park", "Natural Region", "Land Unit", "Total Stns", "Total Grid Cells", "NABat Stns")
sta.count <- sta.count[,c("National Park", "Natural Region", "Land Unit","Total Grid Cells","Total Stns","NABat Stns","Min Stn Dist (km)","Mean Stn Dist (km)", "Max Stn Dist (km)")]
# names(sta.count)

opts <- options(knitr.kable.NA = "0", digits=1)

knitr::kable(sta.count,
             caption="The number of stations and NABat grid cells in Natural Regions and Land Units (i.e., National Parks or Land-use Framework Regions). Land Unit abbreviations are as follows: BANP = Banff National Park, EINP = Elk Island National Park, JANP = Jasper National Park, WBNP = Wood Buffalo National Park, WLNP = Waterton Lakes National Park, LOAR = Lower Athabasca Region, LOPR = Lower Peace Region, NOSR = North Saskatchewan Region, REDR = Red Deer Region, SOSR = South Saskatchewan Region, UPAR = Upper Athabasca Region, and UPPR = Upper Peace Region.",
             align = "lllrrrrrr")
```


```{r Provincial png map, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap="Bat Survey Locations 2015-2021", echo=FALSE}
knitr::include_graphics("./Output/Fig_provincial.plot.png")
```

##### Figure 1. Bats were acoustically surveyed, following NABat protocols, at `r n.stat` stations across Alberta in `r Year_interest`.


```{r nightly survey effort NP, echo=F, fig.height=6, fig.width=7}

calls.per.night.locn <- dat_time_agg %>% group_by(Year,jDay, SurveyNight,NP, Natural.Region, Land.Unit.Code, Location.Name) %>%
  filter(Classification!="noise") %>%
  summarise(Call.Count =sum(Count))
calls.per.night.locn$Night.Count <- 1
calls.per.night.locn %>% ungroup() %>% summarise(mean(Call.Count), se(Call.Count), min(Call.Count), max(Call.Count))
calls.per.night.locn %>% ungroup() %>% filter(Call.Count>1000)

calls.per.night.locn <- calls.per.night.locn %>% ungroup()

NightLUC.hist <- ggplot(data = calls.per.night.locn, aes(x = jDay, y = Night.Count, fill=Land.Unit.Code)) + 
  geom_vline(xintercept = July10, linetype="dotted", color = "gray", size=0.5)+
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=unique(sta$ColsLUC))+
  scale_x_continuous(breaks = c(153, 183, 214, 245),
  label = c("Jun-01", "Jul-01", "Aug-01","Sep-01"))+
  theme_classic() + ylab("Number of Stations Surveyed") +
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.title.x = element_blank()) +
  theme(axis.title.y = element_text(size = 14))+
  facet_wrap(~Year, ncol=1)

NightLUC.hist

# Cairo(file="Output/NightPA.hist.PNG",
#       type="png",
#       width=3400,
#       height=2600,
#       pointsize=15,
#       bg="white",
#       dpi=300)
# NightLUC.hist
# dev.off()

# tmp1 <- calls.per.night.locn %>% group_by(Location.Name) %>% summarise(min(SurveyNight), max(SurveyNight))
# colnames(tmp1) <- c("Location.Name", "Survey.Start", "Survey.End")
# tmp1$nights_surveyed <- tmp1$Survey.End - tmp1$Survey.Start+1
# tmp1 <- as.data.frame(tmp1)
# min(tmp1$nights_surveyed) #1
# max(tmp1$nights_surveyed) #133
# mean(tmp1$nights_surveyed) #19
# se(tmp1$nights_surveyed) #2
# sum(tmp1$nights_surveyed)

```

###### Figure 2. Number of bat survey locations sampled per night from `r min(eff$Deployment.ID)` to `r max(eff$Deployment.ID)` by year within and outside of National Parks (NP).

#### Nightly Overall Bat Call Summaries

```{r mean nightly call by NR, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count.NR <- call_count %>% group_by(Natural.Region, Year, Volancy)
# summary(call_count.NR)

# subset data for overall mean nightly bat calls by year for each Natural Region
NR.Calls.Yr <- call_count.NR%>% group_by(Natural.Region, Year, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

call_count.NR %>% ungroup() %>% group_by(Natural.Region) %>% summarise(mean(Count), se(Count))

fcalls.NR <- NR.Calls.Yr %>%
  ggplot(aes(x = Natural.Region, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean Â± 1 SE Nightly Bat Calls"))) +
  #ylim(c(0,60))+
  geom_linerange(aes(Natural.Region, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  facet_wrap(~Year, ncol=1)
fcalls.NR

```
##### Figure 8. Mean nightly bat calls for each Natural Region, by year and volancy period.

```{r mean nightly call by LUT, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count.LUT <- call_count %>% group_by(Land.Use.Type, Year, Volancy)
call_count.LUT <- call_count.LUT[complete.cases(call_count.LUT$Land.Use.Type),]

# subset data for overall mean nightly bat calls by year for each LUT
call_count.LUT %>% group_by(Land.Use.Type) %>% summarise(Mean = mean(Count), SE = se(Count))

LUT.Calls.Yr <- call_count.LUT %>% group_by(Land.Use.Type, Year, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))
LUT.levels <- c("Agriculture","Barren Land", "Developed" ,"Forest-conifer", "Forest-deciduous","Forest-mixed",  "Grassland" ,"Shrubland", "Water")
LUT.Calls.Yr$Land.Use.Type <- factor(LUT.Calls.Yr$Land.Use.Type , levels = LUT.levels)
LUT.Calls.Yr$Land.Use.Type


fcalls.LUT <- LUT.Calls.Yr %>%
  ggplot(aes(x = Land.Use.Type, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean Â± 1 SE Nightly Bat Calls"))) +
  geom_linerange(aes(Land.Use.Type, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  facet_wrap(~Year)
fcalls.LUT

```

```{r mean nightly call by GRTS, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count2 <- call_count %>% group_by(Land.Unit.Code, Year, GRTS.Cell.ID)

# Reorder GRTS to be NP, Land.Unit.Code then GRTS
GRTS.Order <- call_count %>% arrange(Land.Unit.Code, GRTS.Cell.ID) %>% 
  group_by(Land.Unit.Code) %>% count(GRTS.Cell.ID)

GRTS.Order$Order <- row.names(GRTS.Order)
GRTS.Order <- fct_reorder(GRTS.Order$GRTS.Cell.ID, GRTS.Order$Order, min)

# subset data for overall mean nightly bat calls by year for each GRTS
GRTS.Calls.Yr <- call_count2 %>% group_by(Land.Unit.Code, Year, GRTS.Cell.ID) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

GRTS.Calls.Yr$GRTS.Cell.ID <- factor(GRTS.Calls.Yr$GRTS.Cell.ID, levels = GRTS.Order)
GRTS.Calls.Yr %>% filter(Mean >50)
as.data.frame(GRTS.Calls.Yr %>% filter(Mean <2))

call_count2 %>% group_by(Land.Unit.Code, Year, GRTS.Cell.ID) %>%  summarise(sum = sum(Count)) %>% filter(sum<20)
call_count2 %>% group_by(Volancy) %>% summarise(sum(Count))
112745/(112745+135809)

summary(GRTS.Calls.Yr$Mean)
fcalls.GRTS <- GRTS.Calls.Yr %>%
  ggplot(aes(x = GRTS.Cell.ID, Mean))+
  geom_point(colour="white", shape=21, size=4,aes(fill=Land.Unit.Code))+
  scale_fill_manual(values=unique(sta$ColsLUC)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean Â± 1 SE Nightly Bat Calls"))) +
  xlab(expression("NABat GRTS Cell ID"))+
  geom_linerange(aes(GRTS.Cell.ID, ymin = Mean-SE, ymax = Mean+SE)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 8)) +
  theme(legend.position = "bottom", legend.title=element_blank()) +
  facet_wrap(~Year)

fcalls.GRTS

```

##### Figure 10. Mean nightly bat calls for each NABat grid cell by year.

#### Nightly Bat Call Summaries by Species / Species Group
There were `r sprintf("%.0f", sum(call_count$Count))` bat calls recorded at `r n.stat` bat survey locations between `r min(eff$Deployment.ID)` and `r max(eff$Deployment.ID)`, over `r sum(as.numeric(total.effort$Diff))` survey nights. Of these calls, `r round(unknown.calls*100,0)`% were classified as unknown.

##### Table 2. Bat call count
``` {r overall call table, echo=F, results="asis"}
# create a table for the paper
# Determine call count overall, percentage and by effort for each species
Table.Calls <- call_count %>% group_by(Classification) %>% summarise(Call.Count = sum(Count))
Table.Calls$Per.Known <- as.data.frame(round(Table.Calls$Call.Count/sum(Table.Calls$Call.Count)*100,0))[,1]
Table.Calls$Occupancy <- as.data.frame(round(Table.Calls$Call.Count/sum(as.numeric(total.effort$Diff)),2))[,1] 
colnames(Table.Calls) <- c("Species / Species Group", "Call Count", "% of Calls","Calls per Night")
# as.data.frame(Table.Calls)

knitr::kable(Table.Calls, 
             caption=paste("Overall bat call count, percentage and call per night for ",n.stat," stations surveyed in ",Year_interest,".", sep=""),
             align = "lrrr",
             format.args = list(scientific = FALSE))

options(scipen = 100)
# head(call_count)
# head(dat_time)
# bat_count <- call_count %>% filter(Classification!="unknown")
# call_count %>% group_by(Classification) %>% summarise(sum(Count))
# dat_time %>% count(Classification)
# nrow(dat_time)
# 
# sum(call_count$Count)
# sum(bat_count$Count)
# bat_count %>% group_by(Classification) %>% summarise(sum(Count))
# 
# 12878  / sum(bat_count$Count)

# call_count %>% group_by(Natural.Region, Classification) %>% summarise(Call.Count = sum(Count))
# NR_xtabs <- as.data.frame(table(dat_time$Classification,dat_time$Natural.Region))
# colnames(NR_xtabs)[1:2] <- c("Classification","Natural.Region")
# sum(NR_xtabs$Freq)
# NR_xtabs <- NR_xtabs %>% pivot_wider(names_from = Natural.Region, values_from = Freq)
# NR_xtabs <- NR_xtabs %>% rowwise() %>% mutate(Class.Sum = sum(c_across(where(is.numeric))))
# write.csv(NR_xtabs, "NR_xtabs.csv")
eff %>% filter(GRTS.Cell.ID=="336731")
dat_time_agg %>% ungroup() %>% filter(Classification=="MYEV-MYSE") %>% group_by(Location.Name, Natural.Region) %>% summarise(sum(Count))

dat_summary %>% ungroup() %>% filter(sp1=="MYCA") %>% filter(GRTS.Cell.ID=="336731") %>% select(Filename)
eBat_param

```


```{r sp hist NR, echo=F, fig.height=6, fig.width=7}
# Generate colours to display the species levels
call_count$Classification <- droplevels(call_count$Classification)

Sp.hist.data <- call_count %>% group_by(Year, Natural.Region, Land.Use.Type) %>% count(Classification)

Sp.hist.NR <- ggplot(data = Sp.hist.data, aes(x = Classification, y = n, fill= Natural.Region)) + 
  geom_bar(stat = "identity") + 
  # scale_fill_manual(values=unique(sta$ColsNR))+
  theme_classic() + xlab("Species / Species Group") + ylab("Total Bat Calls") + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(axis.title.y = element_text(size = 14)) +   theme(axis.title.x = element_blank())+
  facet_wrap(~Year)

Sp.hist.NR

```

##### Figure 11. Total bat calls for species / species groups by year and Natural Region


```{r sp hist LUT, echo=F, fig.height=6, fig.width=7}

Sp.hist.LUT <- ggplot(data = Sp.hist.data %>% filter(!is.na(Land.Use.Type)), aes(x = Classification, y = n, fill= Land.Use.Type)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette = "Paired", direction=-1)+
  theme_classic() + xlab("Species / Species Group") + ylab("Total Bat Calls") + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(axis.title.y = element_text(size = 14)) +   theme(axis.title.x = element_blank())+
  facet_wrap(~Year, ncol=1, scales="free_y")

Sp.hist.LUT
```

##### Figure 12. Total bat calls for species / species groups by year and Land Use Type.


```{r mean species call by year and volancy, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count.Sp <- call_count %>% group_by(Classification, Year, Volancy)
# only graph species / species groups with >100 calls
sp.to.use <- call_count.Sp %>% group_by(Classification) %>% summarise(Sum = sum(Count))

# group data for overall mean nightly bat calls by year with volancy
Sp.Calls.Yr <- call_count.Sp%>% group_by(Classification, Year, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

fcalls.Sp <- Sp.Calls.Yr %>% filter(Classification %in% sp.to.use[sp.to.use$Sum>100,]$Classification)%>%
  ggplot(aes(x = Classification, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean Â± 1 SE Nightly Bat Calls"))) +
  geom_linerange(aes(Classification, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank())+
  facet_wrap(~Year)
fcalls.Sp

```

##### Figure 13. Mean nightly bat calls for each species / species group by year and volancy period.

Species / species group specific 

```{r EFPU-LANO activity overlap, out.width="33%", out.height="20%", fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c(paste("./Output/activity_overlap_EPFU Pre-volancy-EPFU Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_EPFU-LANO Pre-volancy-EPFU-LANO Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_LANO Pre-volancy-LANO Post-volancy_",Sys.Date(),".png",sep="")))
``` 

##### Figure 15. Nightly activity overlap one month pre and post volancy for EPFU, LANO and EPFU-LANO.

```{r LABO-MYLU activity overlap, out.width="33%", out.height="20%", fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c(paste("./Output/activity_overlap_LABO Pre-volancy-LABO Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_LABO-MYLU Pre-volancy-LABO-MYLU Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_MYLU Pre-volancy-MYLU Post-volancy_",Sys.Date(),".png",sep="")))
``` 

##### Figure 16. Nightly activity overlap one month pre and post volancy for LABO, MYLU and LABO-MYLU. 

```{r LACO-Myotis 40k activity overlap, out.width="33%", out.height="20%", fig.show='hold', fig.align='center', echo=FALSE}
knitr::include_graphics(c(paste("./Output/activity_overlap_LACI Pre-volancy-LACI Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_Myotis 40k Pre-volancy-Myotis 40k Post-volancy_",Sys.Date(),".png",sep="")))
``` 

##### Figure 17. Nightly activity overlap one month pre and post volancy for LACI and Myotis 40k.

### Acknowledgements

Thank you to Lisa Wilkinson for having the foresight and enthusiasm to start NABat monitoring in Alberta and her continued support of the program, to Jason Headley for providing the cover photo, and to all of the biologists who generously provided their time and energy in gathering and providing bat acoustic data: Barb Johnston, Brenda Shepard, Cami Hurtado, Cory Olson, Courtney Hughes, David Bruinsma, Erin Bayne, Greg Brooke, Greg Horne, Geoffrey Prophet, Helena Mahoney, Jason Unruh, Jennifer Carpenter, Lisa Wilkinson, Matina Kalcounis-Rueppell, Natalka Melnycky, Rolanda Steenweg, Saakje Hazenberg, Sandi Robertson, and Sharon Irwin. Lastly, thanks to Brandon Aubie for responding so quickly to, and troubleshooting any, issues with Alberta eBat. NEED TO ADD IN REMAINING PEOPLE


