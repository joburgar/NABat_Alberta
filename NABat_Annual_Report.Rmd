---
title: "Alberta's North American Bat Monitoring Program (NABat) Annual Report"
author: "Created by Joanna Burgar"
date: "Report generated on `r format(Sys.time(), '%d %B, %Y')`"
output: word_document
---

```{r EPFU image, out.width="0.3\\linewidth", include=TRUE, fig.align="center", echo=FALSE}
knitr::include_graphics("./Input/JasonHeadley-EPFU-1756.jpg")
```

Prepared by: Joanna M. Burgar, PhD RPBio
 joburgar@gmail.com 

Prepared for: Alberta Environment & Parks

Suggested Citation: Burgar, J.M. 2022. North American Bat Monitoring Program 2020. Alberta Environment & Parks, Edmonton, Alberta. 

Cover Illustration: Eptesicus fuscus - Big Brown Bat © Jason Headley

```{r setup, echo=F, include=F}

#Load Packages
list.of.packages <- c("data.table", "leaflet", "tidyverse", "lunar", "zoo", "colortools", "lubridate", "camtrapR", "circular", "RColorBrewer", "Cairo", "viridis", "knitr", "sf","osmdata", "ggspatial", "ggmap","gridExtra", "grid","weathercan")
# Check you have them and load them
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

lapply(list.of.packages, require, character.only = TRUE)

# Timezone 
tz <- "UTC"

# Set a single catagorical variable of interest from station covariates (`sta`) for summary graphs. If you do not have and appropriate category use "Project.ID".
category <- "GRTS.Cell.ID"

# Define a colour from the R options to base the colourscheme
colour <- "lightseagreen"

# # Define the GRTS.Cell.ID and Year of interest if subsetting for year, studyarea
# GRTS_interest <- "922"
Year_interest <- year(as.Date("2021-01-01"))

# Check the listing of output files
# fs::dir_ls(path="./Input/NABat_ProcessedFiles", recurse = TRUE)

# Import count files
# dat_count <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles",GRTS_interest,sep="/"),
#                           regexp = "\\counts.csv$", recurse = TRUE)

# # Import count files if selecting only GRTS.Cell.ID of interest
# make the changes below to add in GRTS_interest and/or Year_interest for other files as appropriate
dat_count <- fs::dir_ls(path="./Input/NABat_ProcessedFiles",regexp = "\\counts.csv$", recurse = TRUE) %>%
  map_dfr(read_csv, col_types = cols(.default = 'c'), .id = "source") %>% 
  type_convert() %>%
  mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
  mutate(Year = year(SurveyNight), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  filter(Year==Year_interest)

dat_count$Orig.Name <- as.factor(paste(dat_count$Site, dat_count$Detector, sep="_"))
dat_count$GRTS.Cell.ID <- as.factor(word(dat_count$source,4,sep = "\\/"))
dat_count$Location.Name <- as.factor(word(dat_count$source,5,sep = "\\/"))
dat_count$Deployment.ID <- as.factor(word(dat_count$source,6,sep = "\\/"))
dat_count$Classification <- as.factor(dat_count$Classification %>% 
                                        recode(EPFU.LANO = "EPFU-LANO", LABO.MYLU = "LABO-MYLU",
                                               My_40k = "Myotis 40k", MYEV.MYSE = "MYEV-MYSE"))
dat_count$Classification <- factor(dat_count$Classification,
                                    levels = c("EPFU", "EPFU-LANO", "LANO", "LACI", "LABO", "LABO-MYLU", "MYLU",
                                               "MYCA", "MYCI", "MYEV","MYEV-MYSE", "MYSE", "MYVO", "Myotis 40k",
                                               "unknown", "noise"))

# Import summary files
dat_summary <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles/"), 
                            regexp = "\\_summary.csv$", recurse = TRUE) %>%
  map_dfr(~read_csv(.x, col_types = cols(.default = "c")), .id="source") %>% # issues with min_wind so read in as character
  type_convert() %>% # convert back to proper formats, min_wind still problematic so only use max or mean wind if available
  mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  filter(Year==Year_interest) %>%
  select(-Date)%>%
  rename(Orig.Name=Site)

dat_summary$GRTS.Cell.ID <- as.factor(word(dat_summary$source,4,sep = "\\/"))
dat_summary$Location.Name <- as.factor(word(dat_summary$source,5,sep = "\\/"))
dat_summary$Deployment.ID <- as.factor(word(dat_summary$source,6,sep = "\\/"))

# Create file type to differentiate zc from wav 
dat_summary$File.Type <- str_sub(dat_summary$Filename,-2,-1) %>% recode("0#" = "zc", "av"="wav")
nrow(dat_summary)
# Extract the time from filename and put in date format
dat_summary$Time.temp1 <- str_extract(dat_summary$Filename,"_[0-9]{6}.wav") %>% str_sub(2,7)
dat_summary$Time.temp2 <- str_extract(dat_summary$Filename,"[0-9]{2}\\-[0-9]{2}\\-[0-9]{2} [0-9]{2}\\-[0-9]{2}\\-[0-9]{2}") %>% str_sub(-8,-1)
dat_summary$Time.temp3 <- str_extract(dat_summary$Filename,"_[0-9]{6}_") %>% str_sub(2,7)

dat_summary$Time.temp1p <- as.POSIXct(strptime(dat_summary$Time.temp1, "%H%M%S", tz))
dat_summary$Time.temp2p <- as.POSIXct(strptime(dat_summary$Time.temp2, "%H-%M-%S", tz))
dat_summary$Time.temp3p <- as.POSIXct(strptime(dat_summary$Time.temp3, "%H%M%S", tz))
dat_summary <- dat_summary %>% mutate(Timep = coalesce(Time.temp1p, Time.temp2p, Time.temp3p)) %>% 
  select(-c(Time.temp1, Time.temp2, Time.temp3, Time.temp1p, Time.temp2p, Time.temp3p))

# Create environmental covariate
nightly.max.env.cov <- dat_summary %>% group_by(Location.Name,SurveyNight) %>% dplyr::summarise_at(c("max_temp","max_hum","max_wind"), list(Mean = mean))
nightly.max.env.cov <- nightly.max.env.cov %>% rename(Max.Temp = "max_temp_Mean", Max.Hum = "max_hum_Mean", Max.Wind = "max_wind_Mean")

# Read deployment data csv for station covariates
eff <- read.csv("Input/NABat_Deployment_Data.csv", header=T) %>%
  mutate(Survey.Start.Time = ymd(Survey.Start.Time), Survey.End.Time = ymd(Survey.End.Time))
eff$Survey.End.Time - eff$Survey.Start.Time
eff <- eff %>% filter(Deployment.ID>as.numeric(Year_interest)-1)
as.data.frame(eff %>% group_by(Deployment.ID) %>% count(GRTS.Cell.ID)) # 63 NABat cells surveyed

eff.names <- eff %>% group_by(Location.Name) %>% count(Orig.Name)
dat.names <- dat_count%>% group_by(Location.Name) %>% count(Orig.Name) 
dat.names.sum <- dat_summary %>% group_by(Location.Name) %>% count(Orig.Name)
names.join <- as.data.frame(full_join(eff.names, dat.names.sum, by="Location.Name"))
names.join[is.na(names.join$n.y),] # will show missing deployment data

NABat.stns.to.use <- unique(eff$Location.Name)


# Read station covariates csv
names(sta)
sta <- read.csv("Input/NABat_Station_Covariates.csv", header=T, na.string=c("","NA", "<NA>",-1)) %>%
  # dplyr::select(OBJECTID, LandUnitCo, GRTSCellID, LocName, Cardinal_D, Stn_Num, Orig_Name, NABat_Samp, YrsSurveye, LC_class, NSRNAME, NRNAME,
  #               WTRBODY_TY, DIST_WTRBD, STREAM_TYP, Dist_Strea, RoadType, DistRoad_M, HF_TYPE, Dist_to_HF, Latitude, Longitude) %>%
  rename(Land.Cover=LC_type, Waterbody.Type=WB_type, Waterbody.Distance=WB.dist, Road.Type=RD_Type, Road.Distance=RD.dist)

sta$NP <- as.factor(sta$NP)
sta$NP <- sta$NP %>% relevel(ref="In")
sta$Land.Cover <- as.factor(sta$Land.Cover)
levels(sta$Land.Cover)
sta$Land.Cover <- sta$Land.Cover %>% recode("20"="Water", "33"="Exposed Land", "34"="Developed", "50"="Shrubland", "110"="Grassland","120"="Agriculture", "210"="Coniferous Forest", "220"="Broadleaf Forest", "230"="Mixed Forest")

# Land-Use Type (Agriculture, Barren Land, Forest, Grassland, Shrubland, Urban, Water, Wetland)
# not enough Urban, keep as "Developed"
# not enough Wetland for separate category - combine Water and Wetland
# combine 3 forest types for Forest
# if waterbody < 10 m, then change Land Use Type to Water, otherwise go with ABMI class
# Consolidate Land.Cover to Agriculture, Barren (Exposed) Land, Forest, Grassland, Shrubland, Developed (incl Urban), Water/Wetland 

sta %>% filter(Land.Cover=="Water")

sta$Land.Use.Type <- sta$Land.Cover %>% recode("Coniferous Forest" = "Forest-conifer", "Broadleaf Forest" = "Forest-deciduous", "Mixed Forest" = "Forest-mixed", "Exposed Land" = "Barren Land")
# sta$Land.Use.Type <- case_when(sta$Waterbody.Distance < 10 ~ "Water",
#                                grepl("URBAN", sta$Human.Footprint.Type) ~ "Urban",
#                                TRUE ~ as.character(sta$Land.Use.Type))

sta <- sta %>% filter(Location.Name %in% NABat.stns.to.use)

sta$Land.Unit.Code <- recode(sta$Land.Unit.Code, "BNP"="BANP", "JNP"="JANP","LPR"="LOPR",
                                         "NSR"="NOSR","RDR"="REDR","SSR"="SOSR","UAR"="UPAR","UPR"="UPPR")


eff <- left_join(eff %>% select(-Land.Unit.Code), 
                 sta %>% dplyr::select(Location.Name, Land.Unit.Code, Natural.Region, NP, Land.Use.Type))
eff %>% count(GRTS.Cell.ID)

# Read distance matrix csv
dist.mat <- read.csv("Input/DistanceMatrixTable.csv", header=T)

# convert FID to Location.Name and create ORIGIN and DESTINATION Location.Name columns in dist.mat
FID <- sta[c("FID","Location.Name")]
dist.mat$ORIGIN_Location.Name <- FID$Location.Name[match(dist.mat$ORIGIN_FID, FID$FID,)]
dist.mat$DESTINATION_Location.Name <- FID$Location.Name[match(dist.mat$DESTINATION_FID, FID$FID,)]

dist.mat <- dist.mat %>% filter(DESTINATION_Location.Name %in% NABat.stns.to.use)

dist.mat$NP <- sta$NP[match(dist.mat$ORIGIN_Location.Name, sta$Location.Name,)]
dist.mat$Natural.Region <- sta$Natural.Region[match(dist.mat$ORIGIN_Location.Name, sta$Location.Name,)]
dist.mat$Land.Unit.Code <- sta$Land.Unit.Code[match(dist.mat$ORIGIN_Location.Name, sta$Location.Name,)]
dist.mat <- dist.mat[complete.cases(dist.mat),] # removes any entries without corresponding sta data
summary(dist.mat)

# Read covariate proportion csv
cov.prop <- read.csv("Input/Cov.area.csv", header=T)

##############################################################
##### DATA TESTS #############################################
##############################################################

# This code will not work unless the data passes the following checks

# 1) All dates must be in YYYY-MM-DD in 'eff' and YYYY-MM-DD HH:MM:SS in 'dat' 
# If either of the following return NA, the formatting needs to change
strptime(eff$Survey.Start.Time[1], "%Y-%m-%d", tz="UTC")
strptime(dat_count$SurveyNight[1], "%Y-%m-%d", tz="UTC")
strptime(dat_summary$SurveyNight[1], "%Y-%m-%d", tz="UTC")

# 2) ensure all deployment starting dates are before deployment retreival dates
# Logic = are the stations active for 0 or more days -> all should read TRUE
table((strptime(eff$Survey.End.Time, "%Y-%m-%d", tz="UTC")-strptime(eff$Survey.Start.Time, "%Y-%m-%d", tz="UTC"))>=0)

# 3) Ensure Deployment.ID (year) is the same as SurveyNight year
dat_count %>% group_by(Deployment.ID) %>% count(Year)

dat_count$unique <- paste(dat_count$Location.Name, dat_count$Year, sep="_")
# dat_count$SurveyNight <- case_when(dat_count$unique=="139715_SW_02_2000" ~ dat_count$SurveyNight + 7456,
#                                TRUE ~ as.Date(dat_count$SurveyNight))
dat_count <- dat_count %>%  
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-unique)

dat_count %>% group_by(Deployment.ID) %>% count(Year) 
# no discrepancy between SurveyNight and Deployment.ID

dat_summary$unique <- paste(dat_summary$Location.Name, dat_summary$Year, sep="_")
# dat_summary$SurveyNight <- case_when(dat_summary$unique=="139715_SW_02_2000" ~ dat_summary$SurveyNight + 7456,
#                                TRUE ~ as.Date(dat_summary$SurveyNight))
dat_summary <- dat_summary %>%  
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-unique)

dat_summary %>% group_by(Deployment.ID) %>% count(Year) # no discrepancy between SurveyNight and Deployment.ID
as.data.frame(dat_summary %>% filter(is.na(Year)))

# 4a) Do you have all stations represented in both the deployment data and station covariates? If yes, the value should be 0
# If length > 0, then you have some data missing!
length(setdiff(unique(eff$Location.Name), unique(sta$Location.Name))) # 0
# missing.eff.sta <- left_join(eff, sta, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.eff.sta[is.na(missing.eff.sta$Orig.Name.y),] # will show missing station covariates

# 4b) Do you have all data represented in the station covariates? If yes, the value should be 0
length(setdiff(unique(sta$Location.Name), unique(dat_count$Location.Name))) # 0
missing.dat.sta <- left_join(sta, dat_count, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.dat.sta[is.na(missing.dat.sta$Orig.Name.y),]  # will show missing count data (may just not have data for certain years)

# 4c) Do you have all data represented in the deployment data? If yes, the value should be 0
length(setdiff(unique(eff$Location.Name), unique(dat_count$Location.Name))) # 0
missing.dat.eff <- left_join(eff, dat_count, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.dat.eff[is.na(missing.dat.eff$Orig.Name.y),] # will show missing deployment data covariates

# Clearwater had malfunction issues, no actual data
# EINP can be included in the NABat 2020 report but does not include station covariates (no info from Rhonda), just NP, NR, NSR groupings

# 5) Remove dates outside survey range (i.e., ARUs turned on but not deployed or deployed well past NABat guidelines)
eff %>% group_by(Deployment.ID) %>% summarise(min(Survey.Start.Time), max(Survey.End.Time))
# remove all dates before May 01 and after Sept 01, using Julian date
yday("2020-05-01"); yday("2020-09-01")

dat_count %>% group_by(Deployment.ID) %>% count(Year) 
dat_count <- dat_count %>% filter(between(jDay, 122, 245))

dat_summary %>% group_by(Deployment.ID) %>% count(Year) 
dat_summary <- dat_summary %>% filter(between(jDay, 122, 245))

# 6) # check to make sure pulling times from start and end of day
dat_summary %>% group_by(File.Type) %>% summarise(min(Timep, na.rm=T), max(Timep, na.rm=T))
sum(is.na(dat_summary$Timep)) / nrow(dat_summary) # almost 20% having issues with filename (only 3 % in 2019)

# hist(dat_summary[dat_summary$File.Type=="zc",]$Timep, breaks=60)
# will have some NAs in dat_summary as not all filenames contained time, but if small proportion then ok to go ahead

# If all of the above is satisfied -> press 'Knit' above ^
```

```{r non-adjustable options, echo=F, include=F}
# SE function
se <- function(x) sqrt(var(x)/length(x))

# Determine distance between bat survey locations
dist.mat$count <- 1 # to count number of survey locations within x distance of original survey location
dist.mat.100km <- as.data.frame(dist.mat %>%
                                 group_by(NP, Natural.Region, Land.Unit.Code, ORIGIN_Location.Name) %>% 
                                 summarise_at(c("Link.Distance..km."), list(Mean = mean, SE = se, Min = min, Max = max)))
dist.mat.100km <- left_join(dist.mat.100km, dist.mat %>% filter(Link.Distance..km.<=10) %>% group_by(ORIGIN_Location.Name) %>% count(count))
dist.mat.100km <- dist.mat.100km %>% select(-count)
dist.mat.100km$n <- dist.mat.100km$n %>% replace_na(0) # the number of survey locations within 10 km
mean(dist.mat.100km$n); se(dist.mat.100km$n) # the mean and se of survey locations wtihin 10 km
dist.mat.100km %>% filter(n==0) %>% nrow() # 19 stations with next nearest station > 10 km away

# Count the total number of detector stations and GRTS cells
n.stat <- length(unique(sta$Location.Name))
n.GRTS <- length(unique(sta$GRTS.Cell.ID))

# Find mean and SE detectors per GRTS cell and number of detectors per quadrant
det.per.GRTS <- sta %>% group_by(GRTS.Cell.ID) %>% count(GRTS.Cell.ID)
det.per.quad <- sta %>% count(Stn_Num)

# Count the total number of stations sampled for NABat
NABat.smp <- sta %>% count(NABat.Sample)
NABat.smp.yes <- NABat.smp[NABat.smp$NABat.Sample=="Yes",]$n
NABat.smp.no <- NABat.smp[NABat.smp$NABat.Sample=="No",]$n

# Find mean and SE for years surveyed per GRTS cell
yrs.surveyed.GRTS <- sta %>% group_by(GRTS.Cell.ID) %>% count(Yrs.Surveyed_2021)
se(sta$Yrs.Surveyed)

# ARUs by NR, LUC and LUT
ARUs.by.NP <- sta %>% filter(NP=="In") %>% count(Land.Unit.Code, sort = TRUE)
ARUs.by.NR <- sta %>% count(Natural.Region, sort = TRUE)
ARUs.by.LUC <- sta %>% filter(NP=="Out") %>% count(Land.Unit.Code, sort = TRUE)
ARUs.by.LUT <- sta %>% count(Land.Use.Type, sort = TRUE)

# Proportion of stations vs spatial covariates
sum(ARUs.by.LUC$n) # 87
sum(ARUs.by.NP$n) # 30
sum(ARUs.by.NR$n) # 117
sum(ARUs.by.LUT$n) # 117
colnames(cov.prop)[5] <- "prop.area"
cov.prop$NP <- as.factor(ifelse(grepl("NP",cov.prop$Name), "In", "Out"))
#cov.prop <- cov.prop %>% arrange(NP, Cov, Name)
cov.prop <- left_join(cov.prop,ARUs.by.LUC,by=c("Name"="Land.Unit.Code"))
cov.prop <- left_join(cov.prop,ARUs.by.NP,by=c("Name"="Land.Unit.Code"))
cov.prop <- left_join(cov.prop,ARUs.by.NR,by=c("Name"="Natural.Region"))
cov.prop <- left_join(cov.prop,ARUs.by.LUT,by=c("Name"="Land.Use.Type"))

cov.prop$Num.Stn <- rowSums(cov.prop[7:10],na.rm=T)
cov.prop$prop.stn <- ifelse(cov.prop$Cov=="Land.Unit.Code" & cov.prop$NP=="Out", cov.prop$Num.Stn / sum(ARUs.by.LUC$n),
                            ifelse(cov.prop$Cov=="Land.Unit.Code" & cov.prop$NP=="In", cov.prop$Num.Stn / sum(ARUs.by.NP$n),
                                   ifelse(cov.prop$Cov=="Natural.Region", cov.prop$Num.Stn / sum(ARUs.by.NR$n), 
                                          ifelse(cov.prop$Cov=="Land.Use.Type", cov.prop$Num.Stn / sum(ARUs.by.LUT$n),NA))))
cov.prop <- cov.prop %>% select(-c(n.x, n.y, n.x.x, n.y.y))

# Generate colours to display the category levels - R needs them as a factor
sta[,category] <- factor(sta[,category])
col.cat <- wheel(colour, num = length(levels(sta[,category])))
sta$Cols <- col.cat[sta[,category]]

col.catNR <- wheel(colour, num = length(levels(sta[,"Natural.Region"])))
sta$ColsNR <- col.catNR[sta[,"Natural.Region"]]

sta[,"Land.Unit.Code"] <- factor(sta[,"Land.Unit.Code"])
col.catLUC <- wheel(colour, num = length(levels(sta[,"Land.Unit.Code"])))
sta$ColsLUC <- col.catLUC[sta[,"Land.Unit.Code"]]

sta[,"Land.Use.Type"] <- factor(sta[,"Land.Use.Type"])
col.catLUT <- wheel(colour, num = length(levels(sta[,"Land.Use.Type"])))
col.catLUT <- col.catLUT[order(col.catLUT, decreasing = TRUE)] # have Water as blue
sta$ColsLUT <- col.catLUT[sta[,"Land.Use.Type"]]

col.catYr <- wheel(colour, num = 6)
col.catYr <- col.catYr[order(col.catYr, decreasing = TRUE)]

###--- add covariates to dat objects
dat_count$NP <- sta$NP[match(dat_count$Location.Name, sta$Location.Name,)]
dat_count$Natural.Region <- sta$Natural.Region[match(dat_count$Location.Name, sta$Location.Name,)]
dat_count$Land.Unit.Code <- sta$Land.Unit.Code[match(dat_count$Location.Name, sta$Location.Name,)]
dat_count$Land.Use.Type <- sta$Land.Use.Type[match(dat_count$Location.Name, sta$Location.Name,)]


###--- create volancy covariate
yday("2021-07-10") # 191 = July 10 (general date of volancy)
# if using one date for entire province, relevel with "Pre" as first category
dat_count$Volancy <- as.factor(ifelse(dat_count$jDay<191, "Pre-volancy","Post-volancy")) %>% relevel("Pre-volancy")
dat_count %>% group_by(Volancy, Deployment.ID) %>% summarise(min(jDay), max(jDay))

dat_summary$Volancy <- as.factor(ifelse(dat_summary$jDay<191, "Pre-volancy","Post-volancy")) %>% relevel("Pre-volancy")
dat_summary %>% group_by(Volancy, Deployment.ID) %>% summarise(min(jDay), max(jDay))

col.catVol <- as.character(c("#2028B2","#B2AA20"))

# Create a call df without the noise files
call_count <- dat_count %>% filter(Classification!="noise")
unknown.calls <- call_count %>% filter(Classification=="unknown") %>% summarise(sum(Count)) / sum(call_count$Count)

# Determine total effort (survey nights)
total.effort <- call_count %>% group_by(Location.Name, Deployment.ID) %>% summarise_at(c("SurveyNight"), list(Min = min, Max=max))
total.effort$Diff <- (total.effort$Max - total.effort$Min)+1
sum(total.effort$Diff) # 1688 survey nights
```

``` {r temporal summaries, include=F}

# remove call files without time
dat_summaryT <- dat_summary[complete.cases(dat_summary$Timep),]

# now use Alberta eBat criteria to classify species
dat_sum_sub <- dat_summaryT[c("GRTS.Cell.ID","Location.Name","SurveyNight","Year","Month","jDay","Volancy","Timep",
                              "Filename","n_calls","prob1","sp1","prob2","sp2","prob3","sp3")]

# the Whitemud files that came from Kaleidoscope don't have probabilities so exclude them from this analysis
dat_sum_sub <- dat_sum_sub[complete.cases(dat_sum_sub$n_calls),] # now down to 285,596 files
dat_sum_sub$Location.Name.Year <- paste(dat_sum_sub$Location.Name, dat_sum_sub$Year, sep="_")

# check for erroneous time stamps and remove entire survey
Timepdate <- date(Sys.time())
Timepdatetime1 <- as.POSIXct(paste(Timepdate,"08:00:00"), tz)
Timepdatetime2 <- as.POSIXct(paste(Timepdate,"18:00:00"),tz)

# 21 stations and 39 surveys with calls between 8 am and 6 pm - remove these survey periods from the temporal analysis
timestamp.error <- as.data.frame(dat_sum_sub %>% filter(Timep >Timepdatetime1 & Timep<Timepdatetime2) %>% filter(sp1!="noise") %>% group_by(Location.Name.Year) %>% summarise(min(SurveyNight), max(SurveyNight)))
unique(timestamp.error$Location.Name.Year)

# remove these 3 surveys from dat_sum_sub
dat_sum_sub <- dat_sum_sub %>% filter(!Location.Name.Year %in% timestamp.error$Location.Name.Year)

# subset data to one month pre and one month post volancy (July 10)
dat_sum_sub <- dat_sum_sub %>% filter(between(jDay, 161,222))
nrow(dat_sum_sub) # now down to 123837

# create thresholds for noise and bat classificaitons
threshold_noise <- 0.8; threshold_bat <- 0.5
# start with all call sequences as "unknown"
dat_sum_sub$category <- "unknown"

# Index cases to be categorized as noise
index_noise <- with(dat_sum_sub, (sp1 == "noise") & (prob1 > threshold_noise)) 
# Set the indexed categories to "noise" 
dat_sum_sub$category[index_noise] <- "noise"

# Index the noise values to be filtered out
index_remove_noise <- with(dat_sum_sub, (category == "unknown") & (sp1 == "noise")) 

# Note that there are 55 `NA` values
sum(is.na(index_remove_noise)) 
# These `NA` values are due to an `NA` in the `sp1` column which is the result of a tie in the random forest probabilities
dat_sum_sub[is.na(index_remove_noise),]

# Set any `NA` value to `FALSE` (i.e. not a noise value to be filtered out)
index_remove_noise[is.na(index_remove_noise)] <- FALSE 
# For the noise values to be filtered out, get the corresponding probabilities
prob_noise <- dat_sum_sub[index_remove_noise, "prob1"] 
# Record the state of the data frame before the filtering out of noise
dat_sum_sub_before_noise_removal <- dat_sum_sub

dat_sum_sub[index_remove_noise, "sp1"] <- dat_sum_sub[index_remove_noise, "sp2"]
dat_sum_sub[index_remove_noise,"sp2"] <- dat_sum_sub[index_remove_noise, "sp3"] 
dat_sum_sub[index_remove_noise, "prob1"] <- dat_sum_sub[index_remove_noise, "prob2"] 
dat_sum_sub[index_remove_noise,"prob2"] <- dat_sum_sub[index_remove_noise, "prob3"] 

# Now that the `prob3` value has been shifted to the `prob2` column, replace the `prob3` column with `NA` values, for the cases where noise has been filtered out
dat_sum_sub[index_remove_noise, "prob3"] <- NA 
dat_sum_sub[index_remove_noise, "sp3"] <- NA

dat_sum_sub[index_remove_noise, "prob1"] <- dat_sum_sub[index_remove_noise, "prob1"] / (1 - prob_noise) 
dat_sum_sub[index_remove_noise, "prob2"] <- dat_sum_sub[index_remove_noise, "prob2"] / (1 - prob_noise) 
# Compare "sp1" columns before and after filtering out noise
# table(dat_sum_sub_before_noise_removal$sp1)
# table(dat_sum_sub$sp1)

index_bat_species <- with(dat_sum_sub, (category == "unknown") & (n_calls >= 3) & (prob2 / prob1 <= 0.80)) 

index_bat_species[is.na(index_bat_species)] <- FALSE # to deal with the 5 NA values

dat_sum_sub$category[index_bat_species] <- dat_sum_sub$sp1[index_bat_species]
# dat_sum_sub$sp1 <- dat_sum_sub$sp1 %>% replace_na("unknown")
# dat_sum_sub$sp2 <- dat_sum_sub$sp2 %>% replace_na("unknown")
 
LABO.MYLU_index <- (dat_sum_sub$sp1 %in% c("LABO", "MYLU")) & (dat_sum_sub$sp2 %in% c("LABO", "MYLU")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

EPFU.LANO_index <- (dat_sum_sub$sp1 %in% c("EPFU", "LANO")) & (dat_sum_sub$sp2 %in% c("EPFU", "LANO")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

MYEV.MYSE_index <- (dat_sum_sub$sp1 %in% c("MYEV", "MYSE")) & (dat_sum_sub$sp2 %in% c("MYEV", "MYSE")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

My_40k_index <- (dat_sum_sub$sp1 %in% c("MYCI", "MYLU", "MYVO")) & (dat_sum_sub$sp2 %in% c("MYCI", "MYLU", "MYVO")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

#' Check the number of call sequences belonging to each category:
sum(LABO.MYLU_index, na.rm=T)
sum(EPFU.LANO_index, na.rm=T)
sum(MYEV.MYSE_index, na.rm=T)
sum(My_40k_index, na.rm=T)

#' Define a new data frame:  
dat_sum_sub_df <- dat_sum_sub
dat_sum_sub_df$category <- dat_sum_sub_df$sp1

dat_sum_sub_df$category[dat_sum_sub_df$n_calls < 3 | 
                          dat_sum_sub_df$prob1 < threshold_bat & dat_sum_sub_df$sp1 != "noise"] <- "unknown"
dat_sum_sub_df$category[LABO.MYLU_index] <- "LABO.MYLU"
dat_sum_sub_df$category[EPFU.LANO_index] <- "EPFU.LANO"
dat_sum_sub_df$category[My_40k_index] <- "My_40k"

###--- Visualization of overlapping species detection data
# use overlap and circular R packages

# create new df with select columns, remove noise files and add in some covariates
dat_time <- dat_sum_sub_df %>% select(-n_calls:-sp3) %>% filter(category!="noise")
# now down to 167,319 files

dat_time$Classification <- as.factor(dat_time$category %>% 
                                         recode(EPFU.LANO = "EPFU-LANO", LABO.MYLU = "LABO-MYLU",
                                                My_40k = "Myotis 40k", MYEV.MYSE = "MYEV-MYSE"))
dat_time$Classification <- factor(dat_time$Classification,
                                     levels = c("EPFU", "EPFU-LANO", "LANO", "LACI", "LABO", "LABO-MYLU", "MYLU",
                                                "MYCA", "MYCI", "MYEV","MYEV-MYSE", "MYSE", "MYVO", "Myotis 40k",
                                                "unknown"))
dat_time <- as.data.frame(dat_time) 
dat_time$Time <- format(dat_time$Timep, format = "%H:%M:%S")

# covariates for overlap
dat_time$Classification_Volancy <- as.factor(paste(dat_time$Classification, dat_time$Volancy))
dat_time$NP <- sta$NP[match(dat_time$Location.Name, sta$Location.Name,)]
dat_time$Land.Use.Type <- sta$Land.Use.Type[match(dat_time$Location.Name, sta$Location.Name,)]
dat_time$Natural.Region <- sta$Natural.Region[match(dat_time$Location.Name, sta$Location.Name,)]

### temporal summaries
# create activity overlap plots for all bats in/out of parks 

# create function to produce activity overlap for all classifications pre and post volancy 
volancy_overlap.fn <- function(Classification=Classification){
  overlap <- activityOverlap(recordTable = dat_time,
                             speciesA = paste(Classification,"Pre-volancy"),
                             speciesB = paste(Classification,"Post-volancy"),
                             speciesCol = "Classification_Volancy",
                             recordDateTimeCol = "Time",
                             recordDateTimeFormat = "%H:%M:%S",
                             xcenter="midnight",
                             overlapEstimator = "Dhat4",
                             plotR = TRUE,
                             writePNG = TRUE,
                             plotDirectory = ("./Output"),
                             addLegend = TRUE,
                             legendPosition = "topleft",
                             pngMaxPix   = 1000,
                             linecol     = c("black", "blue"),
                             linewidth   = c(3,3),
                             linetype    = c(1, 2),
                             olapcol     = "darkgrey",
                             add.rug     = TRUE,
                             extend      = "lightgrey",
                             ylim = c(0,0.35),
                             main = paste(Classification,"Pre and Post Volancy"))
  wwt <- watson.wheeler.test(list(overlap$densityA, overlap$densityB))
  return(wwt)
}


save.image(paste("NABat_Annual_Report_",Year_interest,".RDS", sep=""))

# run through each classification, except "unknown" and any classification with <100 calls
class.list <- as.data.frame(dat_time %>% filter(Classification!="unknown") %>% count(Classification, sort=TRUE) %>% 
                              filter(n>100) %>% select(Classification))
LACI.WWT <- volancy_overlap.fn(Classification=class.list[1,])
Myotis40k.WWT <- volancy_overlap.fn(Classification=class.list[2,])
MYLU.WWT <- volancy_overlap.fn(Classification=class.list[3,])
EPFU.LANO.WWT <- volancy_overlap.fn(Classification=class.list[4,])
LABO.MYLU.WWT <- volancy_overlap.fn(Classification=class.list[5,])
LANO.WWT <- volancy_overlap.fn(Classification=class.list[7,])
EPFU.WWT <- volancy_overlap.fn(Classification=class.list[6,])
LABO.WWT <- volancy_overlap.fn(Classification=class.list[8,])

```
### Introduction


```{r WNS spread png map, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap="White-nose syndrome spread up to 2021", echo=FALSE}
knitr::include_graphics("./Input/WNS_spread_map_2022-02-26.png")

```

##### Figure 1. The spread of White-Nose Syndrome up to the spring of 2021; downloaded from https://www.whitenosesyndrome.org/where-is-wns.



#### Summary Statistics
All analyses were conducted in R (version 4.1.2; 2021), within RStudio (version 1.2.5033) using rmarkdwon (version 2.11; Allaire et al. 2020) for reproducible results. Unless otherwise noted, results are reported as the mean ± 1 SE. 

### Results – Survey Representation
In `r Year_interest` there were passive acoustic surveys at `r n.stat` unique locations (i.e., stations) within `r n.GRTS` NABat grid cells (Figure 1). On average, each grid cell had `r round(mean(det.per.GRTS$n),1)` ± `r round(se(det.per.GRTS$n),1)` (1 SE) stations, with between `r min(det.per.quad[,1])` (`r det.per.quad[det.per.quad$Stn_Num==1,]$n` stations) and `r max(det.per.quad[,1])` (`r det.per.quad[det.per.quad$Stn_Num==4,]$n` station) stations per quadrat. The average minimum distance between stations was `r round(mean(dist.mat.100km$Min),1)` km ± `r round(se(dist.mat.100km$Min),1)`. Within 10 km of a station, there were an average of `r round(mean(dist.mat.100km$n),1)` ± `r round(se(dist.mat.100km$n),1)` neighbouring stations. There were `r dist.mat.100km %>% filter(n==0) %>% nrow` stations where the next nearest station was >10 km away (Figure 2). While most (i.e., `r NABat.smp.yes` or `r round(NABat.smp.yes/nrow(sta)*100,0)`%) stations were surveyed specifically for NABat monitoring, some (i.e., `r NABat.smp.no` or `r round(NABat.smp.no/nrow(sta)*100,0)`%) stations were surveyed following similar NABat protocols but not for NABat monitoring per se (e.g., outside of the recommended survey temporal window or for other objectives such as monitoring migratory routes [RDR] or to collect WNS baseline data [BNP, WLNP]).

```{r Table 1 bat survey location, echo=F}
sta.count <- sta %>% count(NP, Natural.Region, Land.Unit.Code)

sta.count2 <- sta %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% count(GRTS.Cell.ID)
sta.count2$GRTS.count <- 1
sta.count3 <- as.data.frame(sta.count2 %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% summarise(sum(GRTS.count)))

sta.count4 <- sta %>% filter(NABat.Sample=="Yes") %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% count(NABat.Sample)

sta.NP <-sta %>% filter(NP=="In") %>% count(Land.Unit.Code)
sta.NP$prop <- sta.NP$n / sum(sta.NP$n)
sta.NP <- arrange(sta.NP, n)

sta.NR <-sta %>% count(Natural.Region)
sta.NR$prop <- sta.NR$n / sum(sta.NR$n)
sta.NR <- arrange(sta.NR, n)

sta.count$Count.Grid.Cells <- as.vector(sta.count3[,4])
sta.count <- left_join(sta.count, sta.count4 %>% select(-NABat.Sample), 
                  by = c("NP" = "NP", "Natural.Region" = "Natural.Region", "Land.Unit.Code" = "Land.Unit.Code"))


colnames(sta.count) <- c("National Park", "Natural Region", "Land Unit", "Total Stations", "Total Grid Cells", "NABat Stations")
sta.count <- sta.count[,c("National Park", "Natural Region", "Land Unit","Total Grid Cells","Total Stations","NABat Stations")]

opts <- options(knitr.kable.NA = "0")

knitr::kable(sta.count,
             caption="The number of stations and NABat grid cells in Natural Regions and Land Units (i.e., National Parks or Land-use Framework Regions). Land Unit abbreviations are as follows: BANP = Banff National Park, EINP = Elk Island National Park, JANP = Jasper National Park, WBNP = Wood Buffalo National Park, WLNP = Waterton Lakes National Park, LOAR = Lower Athabasca Region, LOPR = Lower Peace Region, NOSR = North Saskatchewan Region, REDR = Red Deer Region, SOSR = South Saskatchewan Region, UPAR = Upper Athabasca Region, and UPPR = Upper Peace Region.",
             align = "lllrrr")
```


```{r TerraSolis pdf map, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap="Bat Survey Locations 2015-2020", echo=FALSE}
knitr::include_graphics("./Input/Bats_finaldata.pdf")
```

##### Figure 1. Bats were acoustically surveyed, following NABat protocols, at `r n.stat` stations across Alberta in `r Year_interest`. Please note that the 4 EINP stations were not included on the map as data was obtained after the map was produced.


```{r nightly survey effort NP, echo=F, fig.height=6, fig.width=7}

calls.per.night.locn <- dat_count %>% group_by(Year,jDay, SurveyNight,NP, Natural.Region, Land.Unit.Code, Location.Name) %>%
  filter(Classification!="noise") %>%
  summarise(Call.Count =sum(Count))
calls.per.night.locn$Night.Count <- 1


NightLUC.hist <- ggplot(data = calls.per.night.locn, aes(x = jDay, y = Night.Count, fill=Land.Unit.Code)) + 
  geom_vline(xintercept = 192, linetype="dotted", color = "gray", size=0.5)+
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=unique(sta$ColsLUC))+
  scale_x_continuous(breaks = c(153, 183, 214, 245),
  label = c("Jun-01", "Jul-01", "Aug-01","Sep-01"))+
  theme_classic() + ylab("Number of Stations Surveyed") +
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.title.x = element_blank()) +
  theme(axis.title.y = element_text(size = 14))+
  facet_wrap(~Year, ncol=1)

NightLUC.hist

# Cairo(file="Output/NightPA.hist.PNG",
#       type="png",
#       width=3400,
#       height=2600,
#       pointsize=15,
#       bg="white",
#       dpi=300)
# NightLUC.hist
# dev.off()

tmp1 <- calls.per.night.locn %>% group_by(Location.Name) %>% summarise(min(SurveyNight), max(SurveyNight))
colnames(tmp1) <- c("Location.Name", "Survey.Start", "Survey.End")
tmp1$nights_surveyed <- tmp1$Survey.End - tmp1$Survey.Start+1
tmp1 <- as.data.frame(tmp1)
min(tmp1$nights_surveyed)
max(tmp1$nights_surveyed)
mean(tmp1$nights_surveyed)

```

###### Figure 2. Number of bat survey locations sampled per night from `r min(eff$Deployment.ID)` to `r max(eff$Deployment.ID)` by year within and outside of National Parks (NP).

#### Nightly Overall Bat Call Summaries

```{r mean nightly call by NR, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count.NR <- call_count %>% group_by(Natural.Region, Deployment.ID, Volancy)

# subset data for overall mean nightly bat calls by year for each Natural Region
NR.Calls.Yr <- call_count.NR%>% group_by(Natural.Region, Deployment.ID, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

fcalls.NR <- NR.Calls.Yr %>%
  ggplot(aes(x = Natural.Region, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean ± 1 SE Nightly Bat Calls"))) +
  #ylim(c(0,60))+
  geom_linerange(aes(Natural.Region, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  facet_wrap(~Deployment.ID, ncol=1)
fcalls.NR

```
##### Figure 8. Mean nightly bat calls for each Natural Region, by year and volancy period.

```{r mean nightly call by LUT, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count.LUT <- call_count %>% group_by(Land.Use.Type, Deployment.ID, Volancy)
call_count.LUT <- call_count.LUT[complete.cases(call_count.LUT$Land.Use.Type),]

# subset data for overall mean nightly bat calls by year for each LUT
LUT.Calls.Yr <- call_count.LUT %>% group_by(Land.Use.Type, Deployment.ID, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

fcalls.LUT <- LUT.Calls.Yr %>%
  ggplot(aes(x = Land.Use.Type, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean ± 1 SE Nightly Bat Calls"))) +
  geom_linerange(aes(Land.Use.Type, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  facet_wrap(~Deployment.ID)
fcalls.LUT

```

```{r mean nightly call by GRTS, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count2 <- call_count %>% group_by(Land.Unit.Code, Deployment.ID, GRTS.Cell.ID)

# Reorder GRTS to be NP, Land.Unit.Code then GRTS
GRTS.Order <- call_count %>% arrange(Land.Unit.Code, GRTS.Cell.ID) %>% 
  group_by(Land.Unit.Code) %>% count(GRTS.Cell.ID)
GRTS.Order$Order <- row.names(GRTS.Order)
GRTS.Order <- fct_reorder(GRTS.Order$GRTS.Cell.ID, GRTS.Order$Order, min)

# subset data for overall mean nightly bat calls by year for each GRTS
GRTS.Calls.Yr <- call_count2 %>% group_by(Land.Unit.Code, Deployment.ID, GRTS.Cell.ID) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

GRTS.Calls.Yr$GRTS.Cell.ID <- factor(GRTS.Calls.Yr$GRTS.Cell.ID, levels = GRTS.Order)

fcalls.GRTS <- GRTS.Calls.Yr %>%
  ggplot(aes(x = GRTS.Cell.ID, Mean))+
  geom_point(colour="white", shape=21, size=4,aes(fill=Land.Unit.Code))+
  scale_fill_manual(values=unique(sta$ColsLUC)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean ± 1 SE Nightly Bat Calls"))) +
  xlab(expression("NABat GRTS Cell ID"))+
  geom_linerange(aes(GRTS.Cell.ID, ymin = Mean-SE, ymax = Mean+SE)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 8)) +
  theme(legend.position = "bottom", legend.title=element_blank()) +
  facet_wrap(~Deployment.ID)

fcalls.GRTS

```

##### Figure 10. Mean nightly bat calls for each NABat grid cell by year.

#### Nightly Bat Call Summaries by Species / Species Group
There were `r sprintf("%.0f", sum(call_count$Count))` bat calls recorded at `r n.stat` bat survey locations between `r min(eff$Deployment.ID)` and `r max(eff$Deployment.ID)`, over `r sum(as.numeric(total.effort$Diff))` survey nights. Of these calls, `r round(unknown.calls*100,0)`% were classified as unknown.

##### Table 2. Bat call count
``` {r overall call table, echo=F, results="asis"}
# create a table for the paper
# Determine call count overall, percentage and by effort for each species
Table.Calls <- call_count %>% group_by(Classification) %>% summarise(Call.Count = sum(Count))
Table.Calls$Per.Known <- as.data.frame(round(Table.Calls$Call.Count/sum(Table.Calls$Call.Count)*100,0))[,1]
Table.Calls$Occupancy <- as.data.frame(round(Table.Calls$Call.Count/sum(as.numeric(total.effort$Diff)),2))[,1] 
colnames(Table.Calls) <- c("Species / Species Group", "Call Count", "% of Calls","Calls per Night")
as.data.frame(Table.Calls)

knitr::kable(Table.Calls, 
             caption=paste("Overall bat call count, percentage and call per night for ",n.stat," stations surveyed in ",Year_interest,".", sep=""),
             align = "lrrr")

# bat_count <- call_count %>% filter(Classification!="unknown")
# sum(bat_count$Count)
# bat_count %>% group_by(Classification) %>% summarise(sum(Count))
# 
# 12878  / sum(bat_count$Count)
```


```{r sp hist NR, echo=F, fig.height=6, fig.width=7}
# Generate colours to display the species levels
call_count$Classification <- droplevels(call_count$Classification)

Sp.hist.data <- call_count %>% group_by(Deployment.ID, Natural.Region, Land.Use.Type) %>% count(Classification)

Sp.hist.NR <- ggplot(data = Sp.hist.data, aes(x = Classification, y = n, fill= Natural.Region)) + 
  geom_bar(stat = "identity") + 
  # scale_fill_manual(values=unique(sta$ColsNR))+
  theme_classic() + xlab("Species / Species Group") + ylab("Total Bat Calls") + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(axis.title.y = element_text(size = 14)) +   theme(axis.title.x = element_blank())+
  facet_wrap(~Deployment.ID)

Sp.hist.NR

```

##### Figure 11. Total bat calls for species / species groups by year and Natural Region


```{r sp hist LUT, echo=F, fig.height=6, fig.width=7}

Sp.hist.LUT <- ggplot(data = Sp.hist.data %>% filter(!is.na(Land.Use.Type)), aes(x = Classification, y = n, fill= Land.Use.Type)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette = "Paired", direction=-1)+
  theme_classic() + xlab("Species / Species Group") + ylab("Total Bat Calls") + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(axis.title.y = element_text(size = 14)) +   theme(axis.title.x = element_blank())+
  facet_wrap(~Deployment.ID, ncol=1, scales="free_y")

Sp.hist.LUT
```

##### Figure 12. Total bat calls for species / species groups by year and Land Use Type.


```{r mean species call by year and volancy, echo=F, fig.height=6, fig.width=7, warning=FALSE}
call_count.Sp <- call_count %>% group_by(Classification, Deployment.ID, Volancy)
# only graph species / species groups with >100 calls
sp.to.use <- call_count.Sp %>% group_by(Classification) %>% summarise(Sum = sum(Count))

# group data for overall mean nightly bat calls by year with volancy
Sp.Calls.Yr <- call_count.Sp%>% group_by(Classification, Deployment.ID, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

fcalls.Sp <- Sp.Calls.Yr %>% filter(Classification %in% sp.to.use[sp.to.use$Sum>100,]$Classification)%>%
  ggplot(aes(x = Classification, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean ± 1 SE Nightly Bat Calls"))) +
  geom_linerange(aes(Classification, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme_classic()+
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank())+
  facet_wrap(~Deployment.ID)
fcalls.Sp

```

##### Figure 13. Mean nightly bat calls for each species / species group by year and volancy period.

Species / species group specific 

```{r EFPU-LANO activity overlap, out.width="33%", out.height="20%", fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c(paste("./Output/activity_overlap_EPFU Pre-volancy-EPFU Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_EPFU-LANO Pre-volancy-EPFU-LANO Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_LANO Pre-volancy-LANO Post-volancy_",Sys.Date(),".png",sep="")))
``` 

##### Figure 15. Nightly activity overlap one month pre and post volancy for EPFU, LANO and EPFU-LANO.

```{r LABO-MYLU activity overlap, out.width="33%", out.height="20%", fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c(paste("./Output/activity_overlap_LABO Pre-volancy-LABO Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_LABO-MYLU Pre-volancy-LABO-MYLU Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_MYLU Pre-volancy-MYLU Post-volancy_",Sys.Date(),".png",sep="")))
``` 

##### Figure 16. Nightly activity overlap one month pre and post volancy for LABO, MYLU and LABO-MYLU. 

```{r LACO-Myotis 40k activity overlap, out.width="33%", out.height="20%", fig.show='hold', fig.align='center', echo=FALSE}
knitr::include_graphics(c(paste("./Output/activity_overlap_LACI Pre-volancy-LACI Post-volancy_",Sys.Date(),".png",sep=""),
                          paste("./Output/activity_overlap_Myotis 40k Pre-volancy-Myotis 40k Post-volancy_",Sys.Date(),".png",sep="")))
``` 

##### Figure 17. Nightly activity overlap one month pre and post volancy for LACI and Myotis 40k.

### Acknowledgements

Thank you to Lisa Wilkinson for having the foresight and enthusiasm to start NABat monitoring in Alberta and her continued support of the program, to Jason Headley for providing the cover photo, and to all of the biologists who generously provided their time and energy in gathering and providing bat acoustic data: Barb Johnston, Brenda Shepard, Cory Olson, Courtney Hughes, David Bruinsma, Erin Bayne, Greg Brooke, Greg Horne, Geoffrey Prophet, Helena Mahoney, Jason Unruh, Jennifer Carpenter, Lisa Wilkinson, Matina Kalcounis-Rueppell, Natalka Melnycky, Rolanda Steenweg, Saakje Hazenberg, Sandi Robertson, and Sharon Irwin. Thank you also to Sandra Frey for providing helpful R code and advice on temporal niche partitioning. Lastly, thanks to Alex MacPhail, Cami Hurtado, and Monica Kohler for providing access to ABMIs data, to Rhonda Connors (TerraSolis Inc.) for answering mapping and GIS questions, and to Brandon Aubie for responding so quickly to, and troubleshooting any, issues with Alberta eBat.


