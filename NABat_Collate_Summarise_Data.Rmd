---
title: "Collate and Summarise Alberta's NABat 2015-2020 Data"
author: "Created by Joanna Burgar"
date: "Report generated on `r format(Sys.time(), '%d %B, %Y')`"
output:
  word_document: default
  html_document:
    pdf_print: paged
  pdf_document: default
editor_options:
  chunk_output_type: console
always_allow_html: yes
---
Bats across Canada face unprecedented threats including white-nose syndrome (WNS), wind energy development, habitat loss and fragmentation, and climate change. However, a lack of basic information about the distribution and abundance of bats across the continent makes it difficult to evaluate the impacts of these threats.  Although bat monitoring is conducted in Alberta, no landscape-level spatial and temporal analyses have been completed. The mountain national parks have proposed an Integrated Conservation Planning project that aims to substantively advance bat conservation actions within PCA with partners across Alberta. This analysis and reporting examines monitoring and bat conservation in Alberta from 2015 - 2020, witin a provincial and landscape context. The hope was that this work would inform the Integrated Conservation Planning project.

```{r setup, echo=F, include=F}

#Load Packages
list.of.packages <- c("data.table", "leaflet", "tidyverse", "lunar", "zoo", "colortools", "mapview", "fs", "lubridate", "camtrapR", "Hmsc", "corrplot", "plotrix", "RColorBrewer", "MuMIn")
# Check you have them and load them
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

lapply(list.of.packages, require, character.only = TRUE)

# Timezone 
tz <- "UTC"

# Set a single catagorical variable of interest from station covariates (`sta`) for summary graphs. If you do not have and appropriate category use "Project.ID".
category <- "NP"

# Define a colour from the R options to base the colourscheme
colour <- "lightseagreen"

# # Define the GRTS.Cell.ID and Year of interest if subsetting for year, studyarea
# GRTS_interest <- "922"
# Year_interest <- as.Date(2018)

## README FIRST ##
#Read and run this chunk of code line by line - there are some question below which you will have to answer/ logic tests to complete. Once you are happy with this, hit 'knit' above. 

# Check the listing of output files
# fs::dir_ls(path="./Input/NABat_ProcessedFiles", recurse = TRUE)

# Import count files
dat_count <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles/"), 
                          regexp = "\\counts.csv$", recurse = TRUE) %>%
  map_dfr(read_csv, col_types = cols(.default = 'c'), .id = "source") %>% 
  type_convert() %>% 
  mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
  mutate(Year = year(SurveyNight), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-Date)

# # Import count files if selecting only GRTS.Cell.ID of interest
# make the changes below to add in GRTS_interest and/or Year_interest for other files as appropriate
# dat_count <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles",GRTS_interest,sep="/"), 
#                           regexp = "\\counts.csv$", recurse = TRUE) %>%
#   map_dfr(read_csv, .id = "source") %>% 
#   mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
#   mutate(Year = year(SurveyNight), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
#   filter(Year==Year_interest)

dat_count$Orig.Name <- as.factor(paste(dat_count$Site, dat_count$Detector, sep="_"))
dat_count$GRTS.Cell.ID <- as.factor(word(dat_count$source,4,sep = "\\/"))
dat_count$Location.Name <- as.factor(word(dat_count$source,5,sep = "\\/"))
dat_count$Deployment.ID <- as.factor(word(dat_count$source,6,sep = "\\/"))
dat_count$Classification <- as.factor(dat_count$Classification %>% 
                                        recode(EPFU.LANO = "EPFU-LANO", LABO.MYLU = "LABO-MYLU",
                                               My_40k = "Myotis 40k", MYEV.MYSE = "MYEV-MYSE"))
dat_count$Classification <- factor(dat_count$Classification,
                                    levels = c("EPFU", "EPFU-LANO", "LANO", "LACI", "LABO", "LABO-MYLU", "MYLU",
                                               "MYCA", "MYCI", "MYEV","MYEV-MYSE", "MYSE", "MYVO", "Myotis 40k",
                                               "unknown", "noise"))


# Import summary files
dat_summary <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles/"), 
                            regexp = "\\_summary.csv$", recurse = TRUE) %>%
  map_dfr(~read_csv(.x, col_types = cols(.default = "c")), .id="source") %>% # issues with min_wind so read in as character
  type_convert() %>% # convert back to proper formats, min_wind still problematic so only use max and mean wind if available
  mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-Date)%>%
  rename(Orig.Name=Site)

dat_summary$GRTS.Cell.ID <- as.factor(word(dat_summary$source,4,sep = "\\/"))
dat_summary$Location.Name <- as.factor(word(dat_summary$source,5,sep = "\\/"))
dat_summary$Deployment.ID <- as.factor(word(dat_summary$source,6,sep = "\\/"))

# Create file type to differentiate zc from wav 
dat_summary$File.Type <- str_sub(dat_summary$Filename,-2,-1) %>% recode("0#" = "zc", "av"="wav")
nrow(dat_summary)
# Extract the time from filename and put in date format
dat_summary$Time.temp1 <- str_extract(dat_summary$Filename,"_[0-9]{6}.wav") %>% str_sub(2,7)
dat_summary$Time.temp2 <- str_extract(dat_summary$Filename,"[0-9]{2}\\-[0-9]{2}\\-[0-9]{2} [0-9]{2}\\-[0-9]{2}\\-[0-9]{2}") %>% str_sub(-8,-1)
dat_summary$Time.temp3 <- str_extract(dat_summary$Filename,"_[0-9]{6}_") %>% str_sub(2,7)

dat_summary$Time.temp1p <- as.POSIXct(strptime(dat_summary$Time.temp1, "%H%M%S", tz))
dat_summary$Time.temp2p <- as.POSIXct(strptime(dat_summary$Time.temp2, "%H-%M-%S", tz))
dat_summary$Time.temp3p <- as.POSIXct(strptime(dat_summary$Time.temp3, "%H%M%S", tz))
dat_summary <- dat_summary %>% mutate(Timep = coalesce(Time.temp1p, Time.temp2p, Time.temp3p)) %>% 
  select(-c(Time.temp1, Time.temp2, Time.temp3, Time.temp1p, Time.temp2p, Time.temp3p))

# Read deployment data csv for station covariates
eff <- read.csv("Input/NABat_Deployment_Data.csv", header=T) %>%
  mutate(Survey.Start.Time = ymd(Survey.Start.Time), Survey.End.Time = ymd(Survey.End.Time))
eff$Survey.End.Time - eff$Survey.Start.Time
eff <- eff %>% filter(Deployment.ID>2014)

eff.names <- eff %>% group_by(Location.Name) %>% count(Orig.Name)
dat.names <- dat_count%>% group_by(Location.Name) %>% count(Orig.Name) 
dat.names.sum <- dat_summary %>% group_by(Location.Name) %>% count(Orig.Name)
names.join <- as.data.frame(full_join(eff.names, dat.names.sum, by="Location.Name"))
names.join[is.na(names.join$n.y),] # will show missing deployment data

# Read station covariates csv
sta <- read.csv("Input/NABat_Station_Covariates.csv", header=T, na.string=c("","NA", "<NA>",-1)) %>%
  dplyr::select(OBJECTID, LandUnitCo, GRTSCellID, LocName, Cardinal_D, Stn_Num, Orig_Name, NABat_Samp, YrsSurveye, LC_class, NSRNAME, NRNAME,
                WTRBODY_TY, DIST_WTRBD, STREAM_TYP, Dist_Strea, RoadType, DistRoad_M, HF_TYPE, Dist_to_HF, Latitude, Longitude) %>%
  rename(FID = OBJECTID, Land.Unit.Code=LandUnitCo, GRTS.Cell.ID=GRTSCellID, Location.Name=LocName,Orig.Name=Orig_Name, NABat.Sample=NABat_Samp,
         Yrs.Surveyed=YrsSurveye, Land.Cover=LC_class, Natural.Sub.Region=NSRNAME, Natural.Region=NRNAME, Waterbody.Type=WTRBODY_TY,          Waterbody.Distance=DIST_WTRBD, Stream.Type=STREAM_TYP, Stream.Distance=Dist_Strea, Road.Type=RoadType, Road.Distance=DistRoad_M,
         Human.Footprint.Type=HF_TYPE, Human.Footprint.Distance=Dist_to_HF)
sta$NP <- as.factor(ifelse(sta$Land.Unit.Code %in% c("BNP", "JNP", "WBNP", "WLNP", "EINP"), "In", "Out")) %>% relevel(ref="In")

sta$Land.Cover <- as.factor(sta$Land.Cover)
levels(sta$Land.Cover)
sta$Land.Cover <- sta$Land.Cover %>% recode("20"="Water", "33"="Exposed Land", "34"="Developed", "50"="Shrubland", "110"="Grassland",
                                            "120"="Agriculture", "210"="Coniferous Forest", "220"="Broadleaf Forest", "230"="Mixed Forest")

# Land-Use Type (Agriculture, Barren Land, Forest, Grassland, Shrubland, Urban, Water, Wetland)
# not enough Urban, keep as "Developed"
# not enough Wetland for separate category - combine Water and Wetland
# combine 3 forest types for Forest
# if waterbody < 10 m, then change Land Use Type to Water, otherwise go with ABMI class
# Consolidate Land.Cover to Agriculture, Barren (Exposed) Land, Forest, Grassland, Shrubland, Developed (incl Urban), Water/Wetland 

sta$Land.Use.Type <- sta$Land.Cover %>% recode("Coniferous Forest" = "Forest", "Broadleaf Forest" = "Forest", 
                                               "Mixed Forest" = "Forest", "Exposed Land" = "Barren Land")
sta$Land.Use.Type <- case_when(sta$Waterbody.Distance < 10 ~ "Water",
                               TRUE ~ as.character(sta$Land.Use.Type))

eff <- left_join(eff %>% select(-Land.Unit.Code), 
                 sta %>% dplyr::select(Location.Name, Land.Unit.Code, Natural.Region, NP, Land.Use.Type))

# Read distance matrix csv
dist.mat <- read.csv("Input/DistanceMatrixTable.csv", header=T)

# convert FID to Location.Name and create ORIGIN and DESTINATION Location.Name columns in dist.mat
FID <- sta[c("FID","Location.Name")]
dist.mat$ORIGIN_Location.Name <- FID$Location.Name[match(dist.mat$ORIGIN_FID, FID$FID,)]
dist.mat$DESTINATION_Location.Name <- FID$Location.Name[match(dist.mat$DESTINATION_FID, FID$FID,)]
dist.mat$NP <- sta$NP[match(dist.mat$ORIGIN_Location.Name, sta$Location.Name,)]
dist.mat$Natural.Region <- sta$Natural.Region[match(dist.mat$ORIGIN_Location.Name, sta$Location.Name,)]
dist.mat$Land.Unit.Code <- sta$Land.Unit.Code[match(dist.mat$ORIGIN_Location.Name, sta$Location.Name,)]
dist.mat <- dist.mat[complete.cases(dist.mat),] # removes any entries without corresponding sta data

# Read covariate proportion csv
cov.prop <- read.csv("Input/Cov.area.csv", header=T)

##############################################################
##### DATA TESTS #############################################
##############################################################

# This code will not work unless the data passes the following checks

# 1) All dates must be in YYYY-MM-DD in 'eff' and YYYY-MM-DD HH:MM:SS in 'dat' 
# If either of the following return NA, the formatting needs to change
strptime(eff$Survey.Start.Time[1], "%Y-%m-%d", tz="UTC")
strptime(dat_count$SurveyNight[1], "%Y-%m-%d", tz="UTC")
strptime(dat_summary$SurveyNight[1], "%Y-%m-%d", tz="UTC")

# 2) ensure all deployment starting dates are before deployment retreival dates
# Logic = are the stations active for 0 or more days -> all should read TRUE
table((strptime(eff$Survey.End.Time, "%Y-%m-%d", tz="UTC")-strptime(eff$Survey.Start.Time, "%Y-%m-%d", tz="UTC"))>=0)

# 3) Ensure Deployment.ID (year) is the same as SurveyNight year
dat_count %>% group_by(Deployment.ID) %>% count(Year)
# issues with Dipper Rowe - ARUs progammed as 20000212 when should be 20200712
dat_count$unique <- paste(dat_count$Location.Name, dat_count$Year, sep="_")
dat_count$SurveyNight <- case_when(dat_count$unique=="139715_SW_02_2000" ~ dat_count$SurveyNight + 7456,
                               TRUE ~ as.Date(dat_count$SurveyNight))
dat_count <- dat_count %>%  
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-unique)

dat_count %>% group_by(Deployment.ID) %>% count(Year) 
# no discrepancy between SurveyNight and Deployment.ID

dat_summary$unique <- paste(dat_summary$Location.Name, dat_summary$Year, sep="_")
dat_summary$SurveyNight <- case_when(dat_summary$unique=="139715_SW_02_2000" ~ dat_summary$SurveyNight + 7456,
                               TRUE ~ as.Date(dat_summary$SurveyNight))
dat_summary <- dat_summary %>%  
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-unique)

dat_summary %>% group_by(Deployment.ID) %>% count(Year) # no discrepancy between SurveyNight and Deployment.ID
as.data.frame(dat_summary %>% filter(is.na(Year)))

# 4a) Do you have all stations represented in both the deployment data and station covariates? If yes, the value should be 0
# If length > 0, then you have some data missing!
length(setdiff(unique(eff$Location.Name), unique(sta$Location.Name))) # 0
# missing.eff.sta <- left_join(eff, sta, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
# missing.eff.sta[is.na(missing.eff.sta$Orig.Name.y),] # will show missing station covariates

# 4b) Do you have all data represented in the station covariates? If yes, the value should be 0
length(setdiff(unique(sta$Location.Name), unique(dat_count$Location.Name))) # 0
missing.dat.sta <- left_join(sta, dat_count, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.dat.sta[is.na(missing.dat.sta$Orig.Name.y),]  # will show missing count data

# 4c) Do you have all data represented in the deployment data? If yes, the value should be 0
length(setdiff(unique(eff$Location.Name), unique(dat_count$Location.Name))) # 0
missing.dat.eff <- left_join(eff, dat_count, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.dat.eff[is.na(missing.dat.eff$Orig.Name.y),] # will show missing deployment data covariates

# EINP can be included in the NABat 2020 report but does not include station covariates (no info from Rhonda), just NP, NR, NSR groupings

# 5) Remove dates outside survey range (i.e., ARUs turned on but not deployed)
eff %>% group_by(Deployment.ID) %>% summarise(min(Survey.Start.Time), max(Survey.End.Time))
# remove all dates before May 01 and after Sept 01, using Julian date
yday("2020-05-01"); yday("2020-09-01")

dat_count %>% group_by(Deployment.ID) %>% count(Year) 
dat_count <- dat_count %>% filter(between(jDay, 122, 245))

dat_summary %>% group_by(Deployment.ID) %>% count(Year) 
dat_summary <- dat_summary %>% filter(between(jDay, 122, 245))

# 6) # check to make sure pulling times from start and end of day
dat_summary %>% group_by(File.Type) %>% summarise(min(Timep, na.rm=T), max(Timep, na.rm=T))
sum(is.na(dat_summary$Timep)) / nrow(dat_summary)

# hist(dat_summary[dat_summary$File.Type=="zc",]$Timep, breaks=60)
# will have some NAs in dat_summary as not all filenames contained time, but if small proportion then ok to go ahead

# If all of the above is satisfied -> press 'Knit' above ^
```

```{r non-adjustable options, echo=F, include=F}
# SE function
se <- function(x) sqrt(var(x)/length(x))

# Determine distance between bat survey locations
dist.mat$count <- 1 # to count number of survey locations within x distance of original survey location
dist.mat.100km <- as.data.frame(dist.mat %>%
                                 group_by(NP, Natural.Region, Land.Unit.Code, ORIGIN_Location.Name) %>% 
                                 summarise_at(c("Link.Distance..km."), list(Mean = mean, SE = se, Min = min, Max = max)))
dist.mat.100km <- left_join(dist.mat.100km, dist.mat %>% filter(Link.Distance..km.<=10) %>% group_by(ORIGIN_Location.Name) %>% count(count))
dist.mat.100km <- dist.mat.100km %>% select(-count)
dist.mat.100km$n <- dist.mat.100km$n %>% replace_na(0) # the number of survey locations within 10 km
mean(dist.mat.100km$n); se(dist.mat.100km$n) # the mean and se of survey locations wtihin 10 km
dist.mat.100km %>% filter(n==0) %>% nrow()

# Count the total number of detector stations and GRTS cells
n.stat <- length(unique(sta$Location.Name))
n.GRTS <- length(unique(sta$GRTS.Cell.ID))

# Find mean and SE detectors per GRTS cell and number of detectors per quadrant
det.per.GRTS <- sta %>% group_by(GRTS.Cell.ID) %>% count(GRTS.Cell.ID)
det.per.quad <- sta %>% count(Stn_Num)

# Count the total number of stations sampled for NABat
NABat.smp <- sta %>% count(NABat.Sample)
NABat.smp.yes <- NABat.smp[NABat.smp$NABat.Sample=="Yes",]$n
NABat.smp.no <- NABat.smp[NABat.smp$NABat.Sample=="No",]$n

# Find mean and SE for years surveyed per GRTS cell
yrs.surveyed.GRTS <- sta %>% group_by(GRTS.Cell.ID) %>% count(Yrs.Surveyed)
se(sta$Yrs.Surveyed)

# ARUs by NR, LUC and LUT
ARUs.by.NP <- sta %>% filter(NP=="In") %>% count(Land.Unit.Code, sort = TRUE)
ARUs.by.NR <- sta %>% count(Natural.Region, sort = TRUE)
ARUs.by.LUC <- sta %>% filter(NP=="Out") %>% count(Land.Unit.Code, sort = TRUE)
ARUs.by.LUT <- sta %>% count(Land.Use.Type, sort = TRUE)

# Proportion of stations vs spatial covariates
sum(ARUs.by.LUC$n) # 87
sum(ARUs.by.NP$n) # 30
sum(ARUs.by.NR$n) # 117
sum(ARUs.by.LUT$n) # 117
colnames(cov.prop)[5] <- "prop.area"
cov.prop$NP <- as.factor(ifelse(grepl("NP",cov.prop$Name), "In", "Out"))
#cov.prop <- cov.prop %>% arrange(NP, Cov, Name)
cov.prop <- left_join(cov.prop,ARUs.by.LUC,by=c("Name"="Land.Unit.Code"))
cov.prop <- left_join(cov.prop,ARUs.by.NP,by=c("Name"="Land.Unit.Code"))
cov.prop <- left_join(cov.prop,ARUs.by.NR,by=c("Name"="Natural.Region"))
cov.prop <- left_join(cov.prop,ARUs.by.LUT,by=c("Name"="Land.Use.Type"))

cov.prop$Num.Stn <- rowSums(cov.prop[7:10],na.rm=T)
cov.prop$prop.stn <- ifelse(cov.prop$Cov=="Land.Unit.Code" & cov.prop$NP=="Out", cov.prop$Num.Stn / sum(ARUs.by.LUC$n),
                            ifelse(cov.prop$Cov=="Land.Unit.Code" & cov.prop$NP=="In", cov.prop$Num.Stn / sum(ARUs.by.NP$n),
                                   ifelse(cov.prop$Cov=="Natural.Region", cov.prop$Num.Stn / sum(ARUs.by.NR$n), 
                                          ifelse(cov.prop$Cov=="Land.Use.Type", cov.prop$Num.Stn / sum(ARUs.by.LUT$n),NA))))
cov.prop <- cov.prop %>% select(-c(n.x, n.y, n.x.x, n.y.y))

# Generate colours to display the category levels - R needs them as a factor
sta[,category] <- factor(sta[,category])
col.cat <- wheel(colour, num = length(levels(sta[,category])))
sta$Cols <- col.cat[sta[,category]]

col.catNR <- wheel(colour, num = length(levels(sta[,"Natural.Region"])))
sta$ColsNR <- col.catNR[sta[,"Natural.Region"]]

sta[,"Land.Unit.Code"] <- factor(sta[,"Land.Unit.Code"])
col.catLUC <- wheel(colour, num = length(levels(sta[,"Land.Unit.Code"])))
sta$ColsLUC <- col.catLUC[sta[,"Land.Unit.Code"]]

sta[,"Land.Use.Type"] <- factor(sta[,"Land.Use.Type"])
col.catLUT <- wheel(colour, num = length(levels(sta[,"Land.Use.Type"])))
col.catLUT <- col.catLUT[order(col.catLUT, decreasing = TRUE)] # have Water as blue
sta$ColsLUT <- col.catLUT[sta[,"Land.Use.Type"]]

col.catYr <- wheel(colour, num = 6)
col.catYr <- col.catYr[order(col.catYr, decreasing = TRUE)]

###--- add covariates to dat objects
dat_count$NP <- sta$NP[match(dat_count$Location.Name, sta$Location.Name,)]
dat_count$Natural.Region <- sta$Natural.Region[match(dat_count$Location.Name, sta$Location.Name,)]
dat_count$Land.Unit.Code <- sta$Land.Unit.Code[match(dat_count$Location.Name, sta$Location.Name,)]
dat_count$Land.Use.Type <- sta$Land.Use.Type[match(dat_count$Location.Name, sta$Location.Name,)]


###--- create volancy covariate
yday("2020-07-10") # 192 = July 10 (general date of volancy)
# if using one date for entire province, relevel with "Pre" as first category
dat_count$Volancy <- as.factor(ifelse(dat_count$jDay<192, "Pre-volancy","Post-volancy")) %>% relevel("Pre-volancy")
dat_count %>% group_by(Volancy, Deployment.ID) %>% summarise(min(jDay), max(jDay))

dat_summary$Volancy <- as.factor(ifelse(dat_summary$jDay<192, "Pre-volancy","Post-volancy")) %>% relevel("Pre-volancy")
dat_summary %>% group_by(Volancy, Deployment.ID) %>% summarise(min(jDay), max(jDay))

col.catVol <- as.character(c("#2028B2","#B2AA20"))

# Create a call df without the noise files
call_count <- dat_count %>% filter(Classification!="noise")
unknown.calls <- call_count %>% filter(Classification=="unknown") %>% summarise(sum(Count)) / sum(call_count$Count)

# Determine total effort (survey nights)
total.effort <- call_count %>% group_by(Location.Name, Deployment.ID) %>% summarise_at(c("SurveyNight"), list(Min = min, Max=max))
total.effort$Diff <- (total.effort$Max - total.effort$Min)+1
sum(total.effort$Diff) # 2852 survey nights
```

### Bat Acoustic Survey Locations (`r min(eff$Deployment.ID)` to `r max(eff$Deployment.ID)`)

To date there have been passive ARU deployments at `r n.stat` unique locations (i.e., bat survey locations) within `r n.GRTS` NABat grid cells. On average, each grid cell had `r round(mean(det.per.GRTS$n),1)` ± `r round(se(det.per.GRTS$n),1)` (1 SE) ARUs recording, with between `r min(det.per.quad[,1])` and `r max(det.per.quad[,1])` ARUs recording per quadrat. The average minimum distance between bat survey locations was `r round(mean(dist.mat.100km$Min),1)` km ± `r round(se(dist.mat.100km$Min),1)`. Bat survey locations had an average of `r round(mean(dist.mat.100km$n),1)` ± `r round(se(dist.mat.100km$n),1)` survey locations within 10 km. There were `r dist.mat.100km %>% filter(n==0) %>% nrow` bat survey locations where the next nearest survey location was >10 km away. While most (i.e., `r NABat.smp.yes` or `r round(NABat.smp.yes/nrow(sta)*100,0)`%) locations were surveyed specifically for NABat monitoring, some (i.e., `r NABat.smp.no` or `r round(NABat.smp.no/nrow(sta)*100,0)`%) locations were surveyed following similar NABat protocols but not for NABat monitoring per se. 

```{r survey distance fig, fig.cap = "Mean distance (km) between bat survey locations within 100 km of one another.", echo=F}
dist.mat.100km <- dist.mat.100km[order(dist.mat.100km$Mean),]

fsurvey.dist = ggplot(dist.mat.100km, aes(x = reorder(ORIGIN_Location.Name, Mean), Mean))+
  geom_point(colour="white", shape=21, size=5,aes(fill=NP))+
  scale_fill_manual(values=unique(sta$Cols)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean ± 1 SE Distance (km)"))) +
  geom_linerange(aes(ORIGIN_Location.Name, ymin = Mean-SE, ymax = Mean+SE)) +
  theme(axis.title.x=element_blank(), axis.text.x=element_blank()) +
  theme(axis.text.y = element_text(size=14))+
  theme(axis.text.x = element_blank())+
  theme(legend.position = "none") 

fsurvey.dist
```
* Green denotes bat survey locations within National Parks, and red denotes locations outside of parks.


```{r Table 1 bat survey location, echo=F}
sta.count <- sta %>% count(NP, Natural.Region, Land.Unit.Code)
sta.count2 <- sta %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% count(GRTS.Cell.ID)
sta.count2$GRTS.count <- 1
sta.count3 <- sta.count2 %>% group_by(NP, Natural.Region, Land.Unit.Code) %>% summarise(sum(GRTS.count))

sta.NP <-sta %>% filter(NP=="In") %>% count(Land.Unit.Code)
sta.NP$prop <- sta.NP$n / sum(sta.NP$n)

sta.NR <-sta %>% count(Natural.Region)
sta.NR$prop <- sta.NR$n / sum(sta.NR$n)

sta.count$Count.Grid.Cells <- sta.count3[,4]
colnames(sta.count) <- c("National Park", "Natural Region", "Land Unit", "Count Bat Survey Locations", "Count NABat Grid Cells")
knitr::kable(sta.count, 
             caption="The number of bat survey locations and NABat grid cells in Natural Regions and Land Units (i.e., National Parks or Land-use Framework Regions). Land Unit abbreviations are as follows: BNP = Banff National Park, EINP = Elk Island National Park, JNP = Jasper National Park, WBNP = Wood Buffalo National Park, WLNP = Waterton Lakes National Park, LAR = Lower Athabasca Region, LPR = Lower Peace Region, NSR = North Saskatchewan Region, RDR = Red Deer Region, SSR = South Saskatchewan Region, UAR = Upper Athabasca Region, and UPR = Upper Peace Region.",
             align = "lllrr")
```

Bat survey locations were distributed across Alberta with a slight concentration in the south-western portion of the province (Figure 2). The majority (i.e., `r nrow(sta)-sum(ARUs.by.NP$n)` or `r round((nrow(sta)-sum(ARUs.by.NP$n))/nrow(sta)*100,0)`%) of bat survey locations were outside of National Parks. Of the `r sum(ARUs.by.NP$n)` (`r round(sum(ARUs.by.NP$n)/nrow(sta)*100,0)`%) survey lcoations within parks, `r sum(ARUs.by.NP[ARUs.by.NP$Land.Unit.Code %in% c("BNP","JNP", "WLNP"),]$n)` were part of the Mountain Park Network. National Parks deployed ARUs at an average of `r round(mean(ARUs.by.NP$n),1)` ± `r round(se(ARUs.by.NP$n),1)` locations; ranging from `r ARUs.by.NP[1,2]` locations within `r ARUs.by.NP[1,1]` to `r ARUs.by.NP[5,2]` locations within both `r ARUs.by.NP[5,1]` and `r ARUs.by.NP[4,1]`. Considering the area of each National Park, BNP and JNP had proportional survey representation, i.e., `r round(sta.NP %>% filter(Land.Unit.Code=="BNP") %>% summarise(sum(prop))*100,0)`% and `r round(sta.NP %>% filter(Land.Unit.Code=="JNP") %>% summarise(sum(prop))*100,0)`% of Alberta's park area and proportion of bat survey locations within the parks. In contrast, WLNP and EINP were over-represented, and WBNP was under-represented, in terms of survey locations to park area. 

Outside of National Parks, bat survey locations were within `r nrow(ARUs.by.LUC)` Land Units, with an average, 
`r round(mean(ARUs.by.LUC$n),1)` ± `r round(se(ARUs.by.LUC$n),1)` bat survey locations within each Land Unit. Most (i.e., `r ARUs.by.LUC[1,2]` or  `r round(ARUs.by.LUC[1,2]/nrow(sta)*100,0)`%) locations were within the `r ARUs.by.LUC[1,1]` and only (i.e., `r ARUs.by.LUC[7,2]` or  `r round(ARUs.by.LUC[1,2]/nrow(sta)*100,0)`%) within the `r ARUs.by.LUC[7,1]`. Bat survey locations were under-represented in LAR and LPR, over-represented in RDR and UPR and roughly proportional in the remaining Land Units. 

Bat survey locations occurred within each of the six Natural Regions, with an average `r round(mean(ARUs.by.NR$n),1)` ± `r round(se(ARUs.by.NR$n),1)` of bat survey locations within each Natural Region. Most (i.e., `r ARUs.by.NR[1,2]` or  `r round(ARUs.by.NR[1,2]/nrow(sta)*100,0)`%) locations were within the `r ARUs.by.NR[1,1]` and only `r ARUs.by.NR[6,2]` (i.e., `r round(ARUs.by.NR[6,2]/nrow(sta)*100,0)`%) within the `r ARUs.by.NR[6,1]`. For each Natural Region, bat survey locations were under-represented in the Boreal (i.e., `r round(sta.NR %>% filter(Natural.Region=="Boreal") %>% summarise(sum(prop))*100,0)`% of the survey locations) to Alberta's land area (58%).In contrast, bat survey locations were over-represented in the Foothills, Grassland and Rocky Mountain Natural Regions. The Canadian Shield and Parkland Natural Regions had roughly proportional survey to land area representation. 

Land Use Type was derived from the Alberta Biodiversity Monitoring Institute (ABMI) Wall-to-wall Land Cover Map 2010 Version 1.0 (ABMIw2wLCV2010v1.0) with the following three deviations: 1) the three forest classes were consolidated into one "Forest" class; 2) exposed land was renamed as "Barren Land"; and 3) if any bat survey location was within 10 m of water the Land-use Type was changed to "Water". Bat survey locations can be grouped into `r nrow(ARUs.by.LUT)` Land Use Types. Most (i.e., `r ARUs.by.LUT[1,2]` or  `r round(ARUs.by.LUT[1,2]/sum(ARUs.by.LUT[complete.cases(ARUs.by.LUT),]$n)*100,0)`%) bat survey locations were within `r ARUs.by.LUT[1,1]` and the fewest (i.e., `r ARUs.by.LUT[7,2]` or `r round(ARUs.by.LUT[7,2]/sum(ARUs.by.LUT[complete.cases(ARUs.by.LUT),]$n)*100,0)`%) were within `r ARUs.by.LUT[7,1]`. Bat survey locations were under-represented in Agriculture, Forest and Shrubland, while being over-represented in Developed, Grassland and Water.

```{r spatial representation fig, echo=F, fig.height=8.5, fig.width=6.5, fig.cap="Proportional representation of bat survey locations compared to land  area for Land Units, Natural Regions, and Land Use Types"}
cov.prop <- cov.prop %>% arrange(Cov, NP, Name)
cov.prop.hist.data <- gather(cov.prop,`prop.area`, `prop.stn`, key="Prop.Type", value="Proportion")
cov.prop.hist.data$Prop.Type <- recode(cov.prop.hist.data$Prop.Type, prop.area = "Area", prop.stn = "Bat Survey Locations")
cov.prop.hist.data$Cov2 <- ifelse(cov.prop.hist.data$Cov=="Land.Unit.Code" & cov.prop.hist.data$NP=="In", "Within National Park",
                                  ifelse(cov.prop.hist.data$Cov=="Land.Unit.Code" & cov.prop.hist.data$NP=="Out", "Outside National Park",
                                         ifelse(cov.prop.hist.data$Cov=="Natural.Region","Natural Region",
                                                ifelse(cov.prop.hist.data$Cov=="Land.Use.Type", "Land Use Type",NA))))

cov.prop.hist.data$Cov2 <- factor(cov.prop.hist.data$Cov2, levels=c("Within National Park", "Outside National Park", "Natural Region", "Land Use Type"))
cov.prop.hist <- ggplot(cov.prop.hist.data %>% filter(!Name %in% c("Broadleaf Forest", "Coniferous Forest", "Mixed Forest")), 
                        aes(fill=Prop.Type, y=Proportion, x=Name)) +
  geom_bar(position="dodge", stat="identity") +
  theme(legend.title = element_blank())+
  theme(legend.position="bottom") +
  theme(axis.title.y = element_text(size = 14)) +   
  theme(axis.title.x = element_blank())+
  facet_wrap(~Cov2, scales="free_x", ncol=1)
cov.prop.hist
```


```{r TerraSolis pdf map, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap="Bat Survey Locations 2015-2020", echo=FALSE}
knitr::include_graphics("./Input/Bats_finaldata.pdf")
```

```{r map NP, echo=F, fig.cap="Distribution of bat survey locations in/out of National Parks"}

### Check distribution of sites and correct spatial assignment (NP, Natural Region, Land Use Type)
#NP
# mPA <- leaflet() %>%
#   addProviderTiles(providers$Esri.WorldImagery, group="Satellite") %>%  # Add satellite data
#   addProviderTiles(providers$Esri.WorldTopoMap, group="Base") %>%     
#   addCircleMarkers(lng=sta$Longitude, lat=sta$Latitude,
#                    color=sta$Cols,
#                    popup=paste(sta$Deployment.ID, sta[,category])) %>%
#  addLegend("bottomleft", colors = col.cat,  labels = levels(sta[,category]),
#     title = category,
#     labFormat = labelFormat(prefix = "$"),
#     opacity = 1
#   ) %>%
#   # Layers control
#   addLayersControl(
#     baseGroups = c("Satellite", "Base"),
#     options = layersControlOptions(collapsed = FALSE)
#   )
# 
# mPA
```


```{r map NR, echo=F, fig.cap="Distribution of bat survey locations by Natural Region"}

# # Natural Region
# mNR <- leaflet() %>%
#   addProviderTiles(providers$Esri.WorldImagery, group="Satellite") %>%  # Add satellite data
#   addProviderTiles(providers$Esri.WorldTopoMap, group="Base") %>%     
#   addCircleMarkers(lng=sta$Longitude, lat=sta$Latitude,
#                    color=sta$ColsNR,
#                    popup=paste(sta$Deployment.ID, sta[,"Natural.Region"])) %>%
#  addLegend("bottomleft", colors = col.catNR,  labels = levels(sta[,"Natural.Region"]),
#     title = "Natural Region",
#     labFormat = labelFormat(prefix = "$"),
#     opacity = 1
#   ) %>%
#   # Layers control
#   addLayersControl(
#     baseGroups = c("Satellite", "Base"),
#     options = layersControlOptions(collapsed = FALSE)
#   )
# 
# mNR
```

```{r map LUC, echo=F, fig.cap="Distribution of bat survey locations by Land Unit"}

# Land Unit Code
# mLUC <- leaflet() %>%
#   addProviderTiles(providers$Esri.WorldImagery, group="Satellite") %>%  # Add satellite data
#   addProviderTiles(providers$Esri.WorldTopoMap, group="Base") %>%     
#   addCircleMarkers(lng=sta$Longitude, lat=sta$Latitude,
#                    color=sta$ColsLUC,
#                    popup=paste(sta$Deployment.ID, sta[,"Land.Unit.Code"])) %>%
#  addLegend("bottomleft", colors = col.catLUC,  labels = levels(sta[,"Land.Unit.Code"]),
#     title = "Land Unit",
#     labFormat = labelFormat(prefix = "$"),
#     opacity = 1
#   ) %>%
#   # Layers control
#   addLayersControl(
#     baseGroups = c("Satellite", "Base"),
#     options = layersControlOptions(collapsed = FALSE)
#   )
# 
# mLUC
```


```{r hist LUT, echo=F, warning=F, fig.width=6.5, fig.cap="Representation of bat survey locations by Land Use Type in/out of National Parks (NP)"}

# Land Use Type
LUT.NP.hist <- sta %>% group_by(NP) %>% count(Land.Use.Type, sort=TRUE)
LUT.NP.hist <- LUT.NP.hist[complete.cases(LUT.NP.hist),] # remove EINP which doesn't have associated LUT covariate

LUT.hist <- ggplot(data = LUT.NP.hist, aes(x = reorder(Land.Use.Type, -n), y = n, fill= NP)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=unique(sta$Cols))+
  theme_classic() + ylab("Number of Locations Surveyed") + 
  theme(legend.position="bottom") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(axis.title.y = element_text(size = 14)) + theme(axis.title.x = element_blank())

LUT.hist
```

### Bat Acoustic Survey Effort (`r min(eff$Deployment.ID)` to `r max(eff$Deployment.ID)`)

Bat survey locations have been surveyed between `r min(sta$Yrs.Surveyed)` and `r max(sta$Yrs.Surveyed)` times (mean = `r round(mean(sta$Yrs.Surveyed),1)` ± `r round(se(sta$Yrs.Surveyed),1)`). 

```{r annual survey effort hist, echo=F, fig.width=6.5, fig.cap="Annual bat survey effort by NABat grid cell in/out of National Parks (NP)"}
Years.Surveyed <- eff %>% group_by(NP, Natural.Region, Land.Unit.Code,Deployment.ID) %>% count(GRTS.Cell.ID)

Yr.GRTS.hist <- Years.Surveyed %>% group_by(NP) %>% count(Deployment.ID)

Yr.hist <- ggplot(data = Yr.GRTS.hist, aes(x = as.factor(Deployment.ID), y = n, fill= NP)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=unique(sta$Cols))+
  theme_classic() + xlab("Year") + ylab("Number of NABat Grid Cells Surveyed") + 
  theme(legend.position="bottom") +
  theme(axis.text.x = element_text(colour = "black")) +
  theme(axis.title.y = element_text(size = 14)) +   theme(axis.title.x = element_text(size = 14))

Yr.hist
```

### Nightly Acoustic Survey Effort (`r min(eff$Deployment.ID)` to `r max(eff$Deployment.ID)`)

```{r nightly survey effort NP, echo=F, fig.height=8.5, fig.width=6.5, fig.cap="Number of bat survey locations sampled per night from `r min(eff$Deployment.ID)` to `r max(eff$Deployment.ID)` by year in/out of National Parks (NP)"}

calls.per.night.locn <- dat_count %>% group_by(Year,jDay, SurveyNight,NP, Natural.Region, Land.Unit.Code, Location.Name) %>%
  filter(Classification!="noise") %>%
  summarise(Call.Count =sum(Count))
calls.per.night.locn$Night.Count <- 1

NightPA.hist <- ggplot(data = calls.per.night.locn, aes(x = jDay, y = Night.Count, fill=NP)) + 
  geom_vline(xintercept = 192, linetype="dotted", color = "gray", size=0.5)+
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=unique(sta$Cols))+
  scale_x_continuous(breaks = c(153, 183, 214, 245),
  label = c("Jun-01", "Jul-01", "Aug-01","Sep-01"))+
  theme_classic() + ylab("Number of Locations Surveyed") +
  theme(legend.position="bottom") +
  theme(axis.title.x = element_blank()) +
  theme(axis.title.y = element_text(size = 14))+
  facet_wrap(~Year, ncol=1)

NightPA.hist
```

```{r nightly survey effort NR, echo=F, fig.height=8.5, fig.width=6.5, fig.cap="Number of bat survey locations sampled per night from `r min(eff$Deployment.ID)` to `r max(eff$Deployment.ID)` by year in each Natural Region"}

NightNR.hist <- ggplot(data = calls.per.night.locn, aes(x = jDay, y = Night.Count, fill=Natural.Region)) + 
  geom_vline(xintercept = 192, linetype="dotted", color = "gray", size=0.5)+
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=unique(sta$ColsNR))+
  scale_x_continuous(breaks = c(153, 183, 214, 245),
  label = c("Jun-01", "Jul-01", "Aug-01","Sep-01"))+
  theme_classic() + ylab("Number Locations Surveyed") +
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.title.x = element_blank()) +
  theme(axis.title.y = element_text(size = 14))+
  facet_wrap(~Year, ncol=1)

NightNR.hist
```

### Nightly Overall Bat Call Summaries
Bat calls were analysed using Alberta eBat (albertaebat.ca) an online platfrom that automates bat species identification from acoustic records. The assumption was that each call file, regardless of call type (i.e., wav or zero-cross) contained one single bat call. For this report, one call is considered as a series of pulses, emanating from one bat and each call file was assumed to be an independent bat call. Due to the magnitude of recordings amassed over the 6 years of survey data, calls were not manually vetted to verify species identification. However, Alberta eBat species identification criteria is relatively conservative with species identification defaulting to "unknown" if certainty criteria were not met. Please see the Alberta eBat technical report for detailed methods (https://www.albertaebat.ca/static/doc/Alberta_e-bat_FinalReport.pdf).


```{r mean nightly call by NR, echo=F, fig.height=8.5, fig.width=7, warning=FALSE, fig.cap=" Mean nightly bat calls for each Natural Region by year and volancy period."}
call_count.NR <- call_count %>% group_by(Natural.Region, Deployment.ID, Volancy)

# subset data for overall mean nightly bat calls by year for each Natural Region
NR.Calls.Yr <- call_count.NR%>% group_by(Natural.Region, Deployment.ID, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

fcalls.NR <- NR.Calls.Yr %>%
  ggplot(aes(x = Natural.Region, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean ± 1 SE Nightly Bat Calls"))) +
  ylim(c(0,60))+
  geom_linerange(aes(Natural.Region, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  facet_wrap(~Deployment.ID, ncol=1)
fcalls.NR

```


```{r mean nightly call by LUT, echo=F, fig.height=8.5, fig.width=7, warning=FALSE, fig.cap=" Mean nightly bat calls for each Land Use Type in/out of National Parks by year and volancy period."}
call_count.LUT <- call_count %>% group_by(NP, Land.Use.Type, Deployment.ID, Volancy)
call_count.LUT <- call_count.LUT[complete.cases(call_count.LUT),]

# subset data for overall mean nightly bat calls by year for each LUT
LUT.Calls.Yr <- call_count.LUT %>% group_by(NP, Land.Use.Type, Deployment.ID, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

fcalls.LUT <- LUT.Calls.Yr %>%
  ggplot(aes(x = Land.Use.Type, y = Mean, fill=NP))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(sta$Cols)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean ± 1 SE Nightly Bat Calls"))) +
  ylim(c(0,100))+
  geom_linerange(aes(Land.Use.Type, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom") +
  facet_wrap(~Deployment.ID+Volancy, ncol=2, scales="free_y")
fcalls.LUT

```


```{r mean nightly call by GRTS, echo=F, fig.height=9, fig.width=7, warning=FALSE, fig.cap="Mean nightly bat calls for each NABat grid cell by year."}
call_count2 <- call_count %>% group_by(NP, Land.Unit.Code, Deployment.ID, GRTS.Cell.ID)

# Reorder GRTS to be NP, Land.Unit.Code then GRTS
GRTS.Order <- call_count %>% arrange(NP, Land.Unit.Code, GRTS.Cell.ID) %>% 
  group_by(NP, Land.Unit.Code) %>% count(GRTS.Cell.ID)
GRTS.Order$Order <- row.names(GRTS.Order)
GRTS.Order <- fct_reorder(GRTS.Order$GRTS.Cell.ID, GRTS.Order$Order, min)

# subset data for overall mean nightly bat calls by year for each GRTS
GRTS.Calls.Yr <- call_count2 %>% group_by(NP, Land.Unit.Code, Deployment.ID, GRTS.Cell.ID) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

GRTS.Calls.Yr$GRTS.Cell.ID <- factor(GRTS.Calls.Yr$GRTS.Cell.ID, levels = GRTS.Order)

fcalls.GRTS <- GRTS.Calls.Yr %>%
  ggplot(aes(x = GRTS.Cell.ID, Mean))+
  geom_point(colour="white", shape=21, size=4,aes(fill=Land.Unit.Code))+
  scale_fill_manual(values=unique(sta$ColsLUC)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean ± 1 SE Nightly Bat Calls"))) +
  ylim(c(0,150))+
  xlab(expression("NABat GRTS Cell ID"))+
  geom_linerange(aes(GRTS.Cell.ID, ymin = Mean-SE, ymax = Mean+SE)) +
  theme(axis.text.y = element_text(size=12))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 8)) +
  theme(legend.position = "bottom", legend.title=element_blank()) +
  facet_wrap(~Deployment.ID, ncol=1, scales="free_y")

fcalls.GRTS

```

``` {r annual mean nightly call by GRTS quadrant, echo=F, warning=FALSE, results='asis'}
call_count$Quadrant <- as.factor(word(call_count$Location.Name,2,sep = "\\_"))
call_count3 <- call_count %>% group_by(NP, Land.Unit.Code, GRTS.Cell.ID, Deployment.ID, Quadrant) %>%
  mutate(Nights.Surveyed=max(SurveyNight)-min(SurveyNight)+1)
call_count3$Nights.Surveyed <- as.numeric(call_count3$Nights.Surveyed)

call_count4 <- call_count3 %>% 
  group_by(NP, Land.Unit.Code, GRTS.Cell.ID, Deployment.ID, Quadrant, Nights.Surveyed) %>%
  summarise(Mean=mean(Count), SE=se(Count)) 

mean.quad.count <- call_count4  %>% select(-SE, -Nights.Surveyed) %>% 
                               pivot_wider(names_from = c("Quadrant"),  values_from = c("Mean")) %>%
                             rename(SE_mean=SE, SW_mean=SW, NE_mean=NE, NW_mean=NW )

se.quad.count <- call_count4 %>% select(-Mean, -Nights.Surveyed) %>%
                            pivot_wider(names_from = c("Quadrant"),  values_from = c("SE")) %>% 
                           rename(SE_se=SE, SW_se=SW, NE_se=NE, NW_se=NW ) 

srvynights.quad.count <- call_count4 %>% select(-Mean, -SE) %>%
                            pivot_wider(names_from = c("Quadrant"),  values_from = c("Nights.Surveyed")) %>% 
                           rename(SE_ns=SE, SW_ns=SW, NE_ns=NE, NW_ns=NW ) 

mean.se.ns.quad.count <- cbind(mean.quad.count, se.quad.count, srvynights.quad.count) %>% select(-NP1:-Deployment.ID1, -NP2:-Deployment.ID2)

# function to separate out into year tables for knitr:kable
fn.mean.se.quad.count.YR <- function(year=year){
  df.year <- mean.se.ns.quad.count %>% filter(Deployment.ID==year)
  df.year <- df.year[c("Land.Unit.Code", "GRTS.Cell.ID",
                                           "SE_mean", "SE_se", "SE_ns",
                                           'SW_mean', "SW_se", "SW_ns",
                                           'NE_mean', 'NE_se', "NE_ns",
                                           "NW_mean", "NW_se", "NW_ns")]
  
  colnames(df.year) <- c("Land Unit", "GRTS ID","SE Mean", "SE", "# Nights", "SW Mean", "SE", "# Nights", "NE Mean", "SE", "# Nights", "NW Mean", "Se", "# Nights")
  return(df.year)
  }

# create loop to run annual tables
call_year <- unique(call_count4$Deployment.ID)
for (i in 1:length(call_year)) {
  opts <- options(knitr.kable.NA = "")
  df.year <- fn.mean.se.quad.count.YR(year=call_year[i])
  print(knitr::kable(df.year, 
             caption = paste(call_year[i],"mean ± 1 SE nightly bat calls for each NABat GRTS quadrant (SE = South-east, SW = South-west, NE = North-east, and NW = North-west) surveyed. # Nights refers to the number of nights the quadrant was surveyed.", sep=" "), 
             digits=1))
  cat('\n\n')
  options(opts)
}

```

#### Nightly Bat Call Summaries by Species / Species Group
There were `r sprintf("%.0f", sum(call_count$Count))` bat calls recorded at `r n.stat` bat survey locations between `r min(eff$Deployment.ID)` and `r max(eff$Deployment.ID)`, over `r sum(as.numeric(total.effort$Diff))` survey nights. Of these calls, `r round(unknown.calls*100,0)`% were classified as unknown.

``` {r overall call table, echo=F, results="asis"}
# create a table for the paper
# Determine call count overall, percentage and by effort for each species
Table.Calls <- call_count %>% group_by(Classification) %>% summarise(Call.Count = sum(Count))
Table.Calls$Per.Known <- as.data.frame(round(Table.Calls$Call.Count/sum(Table.Calls$Call.Count)*100,0))[,1]
Table.Calls$Occupancy <- as.data.frame(round(Table.Calls$Call.Count/sum(as.numeric(total.effort$Diff)),2))[,1] 
colnames(Table.Calls) <- c("Species / Species Group", "Call Count", "% of Calls","Calls per Night")

knitr::kable(replace(Table.Calls, Table.Calls==0, ""), 
             caption=paste("Overall bat calls and naive occupancy for ",n.stat," bat survey locations from ",min(eff$Deployment.ID),"-", max(eff$Deployment.ID),".", sep=""),
             align = "lrrr")
```


```{r sp hist NR, echo=F, fig.height=8.5, fig.width=7, fig.cap="Total bat calls for species / species groups by year and Natural Region"}
# Generate colours to display the species levels
call_count$Classification <- droplevels(call_count$Classification)

Sp.hist.data <- call_count %>% group_by(Deployment.ID, Natural.Region, Land.Use.Type) %>% count(Classification)

Sp.hist.NR <- ggplot(data = Sp.hist.data, aes(x = Classification, y = n, fill= Natural.Region)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=col.catNR)+
  theme_classic() + xlab("Species / Species Group") + ylab("Total Bat Calls") + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(axis.title.y = element_text(size = 14)) +   theme(axis.title.x = element_blank())+
  facet_wrap(~Deployment.ID, ncol=1, scales="free_y")

Sp.hist.NR

```


```{r sp hist LUT, echo=F, fig.height=8.5, fig.width=7, fig.cap="Total bat calls for species / species groups by year and Land Use Type"}

Sp.hist.LUT <- ggplot(data = Sp.hist.data %>% filter(!is.na(Land.Use.Type)), aes(x = Classification, y = n, fill= Land.Use.Type)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=col.catLUT)+
  theme_classic() + xlab("Species / Species Group") + ylab("Total Bat Calls") + 
  theme(legend.position="bottom", legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(axis.title.y = element_text(size = 14)) +   theme(axis.title.x = element_blank())+
  facet_wrap(~Deployment.ID, ncol=1, scales="free_y")

Sp.hist.LUT
```


```{r mean species call by year and volancy, echo=F, fig.height=8.5, fig.width=7, warning=FALSE, fig.cap=" Mean nightly bat calls for each species / species group by year and volancy period."}
call_count.Sp <- call_count %>% group_by(Classification, Deployment.ID, Volancy)

# subset data for overall mean nightly bat calls by year for each Natural Region
Sp.Calls.Yr <- call_count.Sp%>% group_by(Classification, Deployment.ID, Volancy) %>% 
  summarise(Mean = mean(Count), SE = se(Count))

fcalls.Sp <- Sp.Calls.Yr %>% filter(Classification!="unknown")%>%
  ggplot(aes(x = Classification, y = Mean, fill=Volancy))+
  geom_point(colour="white", shape=21, size=4, position=position_dodge(width=0.5))+
  scale_fill_manual(values=unique(col.catVol)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ylab(expression(paste("Mean ± 1 SE Nightly Bat Calls"))) +
  #ylim(c(0,60))+
  geom_linerange(aes(Classification, ymin = Mean-SE, ymax = Mean+SE), position=position_dodge(width=0.5)) +
  theme(axis.text.y = element_text(size=12), axis.title.x = element_blank())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 10)) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  facet_wrap(~Deployment.ID, ncol=1)
fcalls.Sp

```

``` {r mean species call for each year and volancy period by Land Unit, echo=F, warning=FALSE, results='asis'}
call_count.LUC.Sp <- call_count %>% group_by(Land.Unit.Code, Deployment.ID, Volancy, Classification) %>% summarise(Mean=mean(Count), SE=se(Count)) 

call_count.LUC.Sp$Deployment.ID <- factor(call_count.LUC.Sp$Deployment.ID, levels=c("2015", "2016", "2017", "2018", "2019", "2020"))
levels(call_count.LUC.Sp$Deployment.ID)

mean.LUCSp.count <- call_count.LUC.Sp  %>% arrange(Land.Unit.Code, Classification, Volancy) %>% select(-SE) %>% 
                               pivot_wider(names_from = c("Deployment.ID"),  values_from = c("Mean")) %>%
                             rename(mean2015=`2015`, mean2016=`2016`, mean2017=`2017`, mean2018=`2018`, mean2019=`2019`, mean2020=`2020`)

se.LUCsp.count <- call_count.LUC.Sp %>% arrange(Land.Unit.Code, Classification, Volancy) %>% select(-Mean) %>%
                            pivot_wider(names_from = c("Deployment.ID"),  values_from = c("SE")) %>%
                             rename(se2015=`2015`, se2016=`2016`, se2017=`2017`, se2018=`2018`, se2019=`2019`, se2020=`2020`)

mean.se.LUCsp.count <- cbind(mean.LUCSp.count, se.LUCsp.count) %>% select(-Land.Unit.Code1:-Classification1)
mean.se.LUCsp.count$Volancy <- mean.se.LUCsp.count$Volancy %>% recode_factor("Pre-volancy" = "Pre", "Post-volancy"="Post")

# function to separate out into year tables for knitr:kable
fn.mean.se.Sp.count.LUC <- function(LUC=LUC){
  
  df.LUC <- mean.se.LUCsp.count %>% filter(Land.Unit.Code==LUC)
  df.LUC <- df.LUC[c("Classification","Volancy",
                     "mean2015", "se2015","mean2016", "se2016","mean2017", "se2017",
                     "mean2018", "se2018","mean2019", "se2019","mean2020", "se2020")]
  
  colnames(df.LUC) <- c("Species", "Volancy","2015 Mean", "SE", "2016 Mean", "SE", "2017 Mean", "SE", "2018 Mean", "SE", "2019 Mean", "SE", "2020 Mean", "SE")
  return(df.LUC)
  
}
  
# create loop to run annual tables
call_LUC <- unique(call_count.LUC.Sp$Land.Unit.Code)
for (i in 1:length(call_LUC)) {
  opts <- options(knitr.kable.NA = "")
  df.LUC <- fn.mean.se.Sp.count.LUC(LUC=call_LUC[i])
  print(knitr::kable(df.LUC, 
             caption = paste(call_LUC[i],"mean ± 1 SE nightly bat calls by species / species group for year and volancy period.", sep=" "), 
             digits=1))
  cat('\n\n')
  options(opts)
}

```

``` {r temporal summaries, include=F}

# remove call files without time
dat_summaryT <- dat_summary[complete.cases(dat_summary$Timep),]

# now use Alberta eBat criteria to classify species
dat_sum_sub <- dat_summaryT[c("GRTS.Cell.ID","Location.Name","SurveyNight","Year","Month","jDay","Volancy","Timep",
                              "Filename","n_calls","prob1","sp1","prob2","sp2","prob3","sp3")]

# the Whitemud files that came from Kaleidoscope don't have probabilities so exclude them from this analysis
dat_sum_sub <- dat_sum_sub[complete.cases(dat_sum_sub$n_calls),] # now down to 285,596 files

# subset data to one month pre and one month post volancy (July 10)
dat_sum_sub <- dat_sum_sub %>% filter(between(jDay, 162,223))
nrow(dat_sum_sub) # now down to 277,785

# create thresholds for noise and bat classificaitons
threshold_noise <- 0.8; threshold_bat <- 0.5
# start with all call sequences as "unknown"
dat_sum_sub$category <- "unknown"

# Index cases to be categorized as noise
index_noise <- with(dat_sum_sub, (sp1 == "noise") & (prob1 > threshold_noise)) 
# Set the indexed categories to "noise" 
dat_sum_sub$category[index_noise] <- "noise"

# Index the noise values to be filtered out
index_remove_noise <- with(dat_sum_sub, (category == "unknown") & (sp1 == "noise")) 

# Note that there are 76 `NA` values
sum(is.na(index_remove_noise)) 
# These `NA` values are due to an `NA` in the `sp1` column which is the result of a tie in the random forest probabilities
dat_sum_sub[is.na(index_remove_noise),]

# Set any `NA` value to `FALSE` (i.e. not a noise value to be filtered out)
index_remove_noise[is.na(index_remove_noise)] <- FALSE 
# For the noise values to be filtered out, get the corresponding probabilities
prob_noise <- dat_sum_sub[index_remove_noise, "prob1"] 
# Record the state of the data frame before the filtering out of noise
dat_sum_sub_before_noise_removal <- dat_sum_sub

dat_sum_sub[index_remove_noise, "sp1"] <- dat_sum_sub[index_remove_noise, "sp2"]
dat_sum_sub[index_remove_noise,"sp2"] <- dat_sum_sub[index_remove_noise, "sp3"] 
dat_sum_sub[index_remove_noise, "prob1"] <- dat_sum_sub[index_remove_noise, "prob2"] 
dat_sum_sub[index_remove_noise,"prob2"] <- dat_sum_sub[index_remove_noise, "prob3"] 

# Now that the `prob3` value has been shifted to the `prob2` column, replace the `prob3` column with `NA` values, for the cases where noise has been filtered out
dat_sum_sub[index_remove_noise, "prob3"] <- NA 
dat_sum_sub[index_remove_noise, "sp3"] <- NA

dat_sum_sub[index_remove_noise, "prob1"] <- dat_sum_sub[index_remove_noise, "prob1"] / (1 - prob_noise) 
dat_sum_sub[index_remove_noise, "prob2"] <- dat_sum_sub[index_remove_noise, "prob2"] / (1 - prob_noise) 
# Compare "sp1" columns before and after filtering out noise
# table(dat_sum_sub_before_noise_removal$sp1)
# table(dat_sum_sub$sp1)

index_bat_species <- with(dat_sum_sub, (category == "unknown") & (n_calls >= 3) & (prob2 / prob1 <= 0.80)) 

index_bat_species[is.na(index_bat_species)] <- FALSE # to deal with the 5 NA values

dat_sum_sub$category[index_bat_species] <- dat_sum_sub$sp1[index_bat_species]
# dat_sum_sub$sp1 <- dat_sum_sub$sp1 %>% replace_na("unknown")
# dat_sum_sub$sp2 <- dat_sum_sub$sp2 %>% replace_na("unknown")
 
LABO.MYLU_index <- (dat_sum_sub$sp1 %in% c("LABO", "MYLU")) & (dat_sum_sub$sp2 %in% c("LABO", "MYLU")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

EPFU.LANO_index <- (dat_sum_sub$sp1 %in% c("EPFU", "LANO")) & (dat_sum_sub$sp2 %in% c("EPFU", "LANO")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

MYEV.MYSE_index <- (dat_sum_sub$sp1 %in% c("MYEV", "MYSE")) & (dat_sum_sub$sp2 %in% c("MYEV", "MYSE")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

My_40k_index <- (dat_sum_sub$sp1 %in% c("MYCI", "MYLU", "MYVO")) & (dat_sum_sub$sp2 %in% c("MYCI", "MYLU", "MYVO")) & (dat_sum_sub$prob1 < 0.4) & (dat_sum_sub$prob2 > 0.2)

#' Check the number of call sequences belonging to each category:
sum(LABO.MYLU_index, na.rm=T)
sum(EPFU.LANO_index, na.rm=T)
sum(MYEV.MYSE_index, na.rm=T)
sum(My_40k_index, na.rm=T)

#' Define a new data frame:  
dat_sum_sub_df <- dat_sum_sub
dat_sum_sub_df$category <- dat_sum_sub_df$sp1

dat_sum_sub_df$category[dat_sum_sub_df$n_calls < 3 | 
                          dat_sum_sub_df$prob1 < threshold_bat & dat_sum_sub_df$sp1 != "noise"] <- "unknown"
dat_sum_sub_df$category[LABO.MYLU_index] <- "LABO.MYLU"
dat_sum_sub_df$category[EPFU.LANO_index] <- "EPFU.LANO"
dat_sum_sub_df$category[My_40k_index] <- "My_40k"

###--- Visualization of overlapping species detection data
# uses camtrapR and dependent packages (overlap and plotrix)
# create new df with select columns, remove noise files and add in some covariates
dat_time <- dat_sum_sub_df %>% select(-Filename:-sp3) %>% filter(category!="noise")
# now down to 204,452 files

dat_time$Classification <- as.factor(dat_time$category %>% 
                                         recode(EPFU.LANO = "EPFU-LANO", LABO.MYLU = "LABO-MYLU",
                                                My_40k = "Myotis 40k", MYEV.MYSE = "MYEV-MYSE"))
dat_time$Classification <- factor(dat_time$Classification,
                                     levels = c("EPFU", "EPFU-LANO", "LANO", "LACI", "LABO", "LABO-MYLU", "MYLU",
                                                "MYCA", "MYCI", "MYEV","MYEV-MYSE", "MYSE", "MYVO", "Myotis 40k",
                                                "unknown"))
dat_time <- as.data.frame(dat_time) 
dat_time$Time <- format(dat_time$Timep, format = "%H:%M:%S")

# covariates for overlap
dat_time$Classification_Volancy <- as.factor(paste(dat_time$Classification, dat_time$Volancy))
dat_time$NP <- sta$NP[match(dat_time$Location.Name, sta$Location.Name,)]
dat_time$Land.Use.Type <- sta$Land.Use.Type[match(dat_time$Location.Name, sta$Location.Name,)]
dat_time$Natural.Region <- sta$Natural.Region[match(dat_time$Location.Name, sta$Location.Name,)]

# when graphing issues with time stamps in Barren Land and Grassland - need to remove these from the analyses
dat_time <- dat_time %>% filter(Land.Use.Type!="Barren Land" & Land.Use.Type!="Grassland") 
dat_time %>% count(Land.Use.Type)

# create activity overlap plots for all bats in/out of parks 
activityOverlap(recordTable = dat_time[dat_time$Volancy=="Post-volancy",],
                                 speciesA = "In",
                                 speciesB = "Out",
                                 speciesCol = "NP",
                                 recordDateTimeCol = "Time",
                                 recordDateTimeFormat = "%H:%M:%S",
                                 xcenter="midnight",
                                 overlapEstimator = "Dhat4",
                                 plotR = TRUE,
                                 writePNG = TRUE,
                                 plotDirectory = ("./Output"),
                                 addLegend = TRUE,
                                 legendPosition = "topleft",
                                 pngMaxPix   = 1000,
                                 linecol     = c("black", "blue"),
                                 linewidth   = c(3,3),
                                 linetype    = c(1, 2),
                                 olapcol     = "darkgrey",
                                 add.rug     = TRUE,
                                 extend      = "lightgrey",
                                # ylim = c(0,0.5),
                                 main = "Overall post-volancy bat activity overlap in/out of National Parks")


# create function to produce activity overlap for all classifications pre and post volancy 
volancy_overlap.fn <- function(Classification=Classification){
  activityOverlap(recordTable = dat_time,
                                 speciesA = paste(Classification,"Pre-volancy"),
                                 speciesB = paste(Classification,"Post-volancy"),
                                 speciesCol = "Classification_Volancy",
                                 recordDateTimeCol = "Time",
                                 recordDateTimeFormat = "%H:%M:%S",
                                 xcenter="midnight",
                                 overlapEstimator = "Dhat4",
                                 plotR = TRUE,
                                 writePNG = TRUE,
                                 plotDirectory = ("./Output"),
                                 addLegend = TRUE,
                                 legendPosition = "topleft",
                                 pngMaxPix   = 1000,
                                 linecol     = c("black", "blue"),
                                 linewidth   = c(3,3),
                                 linetype    = c(1, 2),
                                 olapcol     = "darkgrey",
                                 add.rug     = TRUE,
                                 extend      = "lightgrey",
                                 ylim = c(0,0.35),
                                 main = paste(Classification,"Pre and Post Volancy"))
}

# run through each classification, except "unknown" and any classification with <100 calls
class.list <- as.data.frame(dat_time %>% filter(Classification!="unknown") %>% count(Classification, sort=TRUE) %>% 
  filter(n>100) %>% select(Classification))
volancy.overlap <- volancy_overlap.fn(Classification=class.list[7,])

# create function to produce activity overlap for all bats pre and post volancy by land use type
dat_timeLUT <- dat_time[!is.na(dat_time$Land.Use.Type),]
dat_timeLUT %>% group_by(Volancy) %>% count(Land.Use.Type)

LUT.class <- c("EPFU", "LANO","LABO","LACI","MYLU") # too much to do LUT.class * LUT so stick with just LUT for now
LUT <- c("Agriculture","Developed","Forest","Shrubland","Water" )

LUT_overlap.fn <- function(LUT=LUT){ 
  activityOverlap(recordTable = dat_timeLUT[dat_timeLUT$Land.Use.Type==LUT,],
                                 speciesA = "Pre-volancy",
                                 speciesB = "Post-volancy",
                                 #allSpecies = FALSE,
                                 speciesCol = "Volancy",
                                 recordDateTimeCol = "Time",
                                 recordDateTimeFormat = "%H:%M:%S",
                                 xcenter="midnight",
                                 overlapEstimator = "Dhat4",
                                 plotR = TRUE,
                                 writePNG = TRUE,
                                 plotDirectory = ("./Output"),
                                 addLegend = TRUE,
                                 legendPosition = "topleft",
                                 pngMaxPix   = 1000,
                                 linecol     = c("black", "blue"),
                                 linewidth   = c(3,3),
                                 linetype    = c(1, 2),
                                 olapcol     = "darkgrey",
                                 add.rug     = TRUE,
                                 extend      = "lightgrey",
                                 #ylim = c(0,0.35),
                                 main = paste("Overall bat activity pre and post volancy overlap in",LUT))
}
# run through each land use type

LUT.overlap <- LUT_overlap.fn (LUT=LUT[3])

# create function to produce activity overlap for all bats pre and post volancy by Natural Region type
dat_time %>% group_by(Volancy) %>% count(Natural.Region)

NR <- c("Boreal","Foothills","Grassland","Parkland","Rocky Mountain" )

NR_overlap.fn <- function(NR=NR){ 
  activityOverlap(recordTable = dat_time[dat_time$Natural.Region==NR,],
                                 speciesA = "Pre-volancy",
                                 speciesB = "Post-volancy",
                                 #allSpecies = FALSE,
                                 speciesCol = "Volancy",
                                 recordDateTimeCol = "Time",
                                 recordDateTimeFormat = "%H:%M:%S",
                                 xcenter="midnight",
                                 overlapEstimator = "Dhat4",
                                 plotR = TRUE,
                                 writePNG = TRUE,
                                 plotDirectory = ("./Output"),
                                 addLegend = TRUE,
                                 legendPosition = "topleft",
                                 pngMaxPix   = 1000,
                                 linecol     = c("black", "blue"),
                                 linewidth   = c(3,3),
                                 linetype    = c(1, 2),
                                 olapcol     = "darkgrey",
                                 add.rug     = TRUE,
                                 extend      = "lightgrey",
                                 ylim = c(0,0.25),
                                 main = paste("Overall bat activity pre and post volancy overlap in",NR))
}
# run through each land use type

NR.overlap <- NR_overlap.fn (NR=NR[5])

```

```{r NP activity overlap, out.width="33%", out.height="20%",fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c("./Output/activity_overlap_In-Out_NP.png",
                          "./Output/activity_overlap_In-Out_NP_pre.png",
                          "./Output/activity_overlap_In-Out_NP_post.png"))
``` 

```{r LUT activity overlap, out.width="33%", out.height="20%",fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c("./Output/activity_overlap_Agriculture.png",
                          "./Output/activity_overlap_Developed.png",
                          "./Output/activity_overlap_Forest.png",
                          "./Output/activity_overlap_Shrubland.png",
                          "./Output/activity_overlap_Water.png"))
``` 

```{r NR activity overlap, out.width="33%", out.height="20%",fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c("./Output/activity_overlap_Boreal.png",
                          "./Output/activity_overlap_Foothills.png",
                          "./Output/activity_overlap_Grassland.png",
                          "./Output/activity_overlap_Parkland.png",
                          "./Output/activity_overlap_Rocky Mountain.png"))
``` 

```{r EFPU-LANO activity overlap, out.width="33%", out.height="20%", fig.cap="Nightly activity overlap one month pre and post volancy for EPFU, LANO and EPFU-LANO", fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c("./Output/activity_overlap_EPFU Pre-volancy-EPFU Post-volancy.png",
                          "./Output/activity_overlap_EPFU-LANO Pre-volancy-EPFU-LANO Post-volancy.png",
                          "./Output/activity_overlap_LANO Pre-volancy-LANO Post-volancy.png"))
``` 


```{r LABO-MYLU activity overlap, out.width="33%", out.height="20%", fig.cap="Nightly activity overlap one month pre and post volancy for LABO, MYLU and LABO-MYLU", fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c("./Output/activity_overlap_LABO Pre-volancy-LABO Post-volancy.png",
                          "./Output/activity_overlap_LABO-MYLU Pre-volancy-LABO-MYLU Post-volancy.png",
                          "./Output/activity_overlap_MYLU Pre-volancy-MYLU Post-volancy.png"))
``` 


```{r LACO-Myotis 40k activity overlap, out.width="33%", out.height="20%", fig.cap="Nightly activity overlap one month pre and post volancy for LACI and Myotis 40k", fig.show='hold', fig.align='center', echo=FALSE}

knitr::include_graphics(c("./Output/activity_overlap_LACI Pre-volancy-LACI Post-volancy.png",
                          "./Output/activity_overlap_Myotis 40k Pre-volancy-Myotis 40k Post-volancy.png"))
``` 

``` {r HMSC prep, include=F}
# Code to prepare data for HMSC analysis, do not include in rmarkdown output
# Objective: examine the data hierarchically.

# First an overall community composition analysis

# Data - use the call count data for aggregated nightly counts
glimpse(call_count)
dat <- call_count

# Exclude EINP as no station covariates
dat <- dat %>% filter(Land.Unit.Code!="EINP") 
dat %>% count(Location.Name) #113 stations, EINP has been removed

# Subset to the species / species groups with sufficient data (>100), exclude unknown
dat %>% count(Classification, sort=TRUE)
sp.to.include <- dat %>% filter(Classification!="unknown") %>% count(Classification, sort=TRUE) %>% filter(n>100) %>% select(Classification)

dat <- dat[dat$Classification %in% sp.to.include$Classification,]  
dat %>% count(Classification, sort=TRUE) # all here
dat$Species <- factor(dat$Classification)

# Effort data and covariates  
# Station (unique survey bat location) data
nrow(sta)
# remove EINP as doesn't have the GIS covaraites
sta <- sta %>% filter(Land.Unit.Code!="EINP") # now only 113 stations
nrow(sta) # EINP has been removed

plot(sta$Longitude , sta$Latitude, asp=1)

# modify the GIS covariates to suit the analysis
### Road type and distance
sta %>% count(Road.Type) # change road type to ordinal scale from 1 = no road to 8 = divided paved road
sta$Road.Type <- as.character(sta$Road.Type)

# if Road is within 100 m, keep as Road.Type.Ord but if >100 then change to 1, i.e., "No Road"
sta$Road.Type <- case_when(sta$Road.Distance > 100 ~ "No Road",
                               TRUE ~ as.character(sta$Road.Type))
# recode to ordinal scale, also changing NA to No Road as it meant no road within 1000 m
sta$Road.Type.Ord <- sta$Road.Type %>% replace_na("No Road") %>%
  recode("No Road"=1, "Driveway"=2,"Truck Trail"=2, "Unimproved Road"=3, "One Lane Gravel Road"=4, 
         "Two Lane Gravel Road"=5, "Two Lane Undivided Paved Road"=6, "Divided Paved Road"=7)

hist(sta$Road.Type.Ord)
# recode distance to ordinal scale 1-10 for 0-100 m being 1, 100-200 being 2 and so on
max(sta$Road.Distance, na.rm=T) # 759 m, so max category is 9 for roads > 800 and those with NA (none within 1000 m)
sta$Road.Distance.Ord <- ifelse(sta$Road.Distance<100, 1, 
                                ifelse(sta$Road.Distance<200, 2,
                                       ifelse(sta$Road.Distance<300, 3,
                                              ifelse(sta$Road.Distance<400, 4,
                                                     ifelse(sta$Road.Distance<500, 5,
                                                            ifelse(sta$Road.Distance<600, 6,
                                                                   ifelse(sta$Road.Distance<700, 7,
                                                                          ifelse(sta$Road.Distance<800, 8,
                                                                                 ifelse(sta$Road.Distance>800,9, NA)))))))))

sta$Road.Distance.Ord <- sta$Road.Distance.Ord %>% replace_na(9)
sta %>% group_by(Road.Distance.Ord) %>% summarise(min(Road.Distance), max(Road.Distance))

### Water type and distance
sta %>% filter(is.na(Waterbody.Distance)) %>% count(Stream.Type)
# some streams are closer than waterbodies, perhaps change waterbody type to stream?
# yes - stream and water have similar features - consolidate into one called "Water" with the closest distance being the type/distance attributed
sta %>% filter(Stream.Distance<100 | Waterbody.Distance<100) %>% group_by(Stream.Type, Waterbody.Type) %>% 
  select(Location.Name, Stream.Distance, Waterbody.Distance, Stream.Type, Waterbody.Type)

sta$Water.Distance <- with(sta, pmin(Stream.Distance, Waterbody.Distance, na.rm=T))
# to remove NAs, will change this to ordinal scale later as actually >1000 but unknown
sta$Water.Distance <- replace_na(sta$Water.Distance, 1001) 

sta$Water.Type <- case_when(sta$Waterbody.Distance < sta$Stream.Distance ~ as.character(sta$Waterbody.Type), 
                            TRUE ~ as.character(sta$Stream.Type))
as.data.frame(sta %>% group_by(Water.Type) %>% select(Waterbody.Type, Stream.Type, Waterbody.Distance, Stream.Distance, Water.Distance))

max(sta$Water.Distance, na.rm=T) # 1001 m, so max category is 11 for water with 11 being >1000 m

# recode to ordinal scale, also changing NA to No Water as it meant no waterbody or stream within 1000 m
sta$Water.Type.Ord <- sta$Water.Type %>% replace_na("No Water") %>%
  recode("No Stream"=1, "No Water"=1, "DITCH"=2,"FLOW-ARB-DEM"=2,"STR-INDEF"=3, "STR-PER"=3, "STR-RECUR"=3, "OXBOW-RECUR"=3, 
         "WETLAND"=4, "RIV-MAJ"=5, "QUARRY"=6, "RESERVOIR"=6, "LAKE-PER"=7, "LAKE-RECUR"=7)
sta %>% group_by(Water.Type) %>% count(Water.Type.Ord)
hist(sta$Water.Type.Ord)

sta$Water.Distance.Ord <- ifelse(sta$Water.Distance<100, 1, 
                                ifelse(sta$Water.Distance<200, 2,
                                       ifelse(sta$Water.Distance<300, 3,
                                              ifelse(sta$Water.Distance<400, 4,
                                                     ifelse(sta$Water.Distance<500, 5,
                                                            ifelse(sta$Water.Distance<600, 6,
                                                                   ifelse(sta$Water.Distance<700, 7,
                                                                          ifelse(sta$Water.Distance<800, 8,
                                                                                 ifelse(sta$Water.Distance<900,9, 
                                                                                        ifelse(sta$Water.Distance<1000,10,11))))))))))

sta %>% group_by(Water.Distance.Ord) %>% summarise(min(Water.Distance), max(Water.Distance))
hist(sta$Water.Distance.Ord)

### Human Footprint type and distance
as.data.frame(sta %>% count(Human.Footprint.Type)) # consolidate to sublayer from ABMI HF 2014 metadata, similar as 2018 report 
sta$Human.Footprint.Distance <- replace_na(sta$Human.Footprint.Distance, 1001)
sta$Human.Footprint.Sublayer <- ifelse(sta$Human.Footprint.Type %in% 
                                         c("BORROWPIT-DRY","BORROWPITS","CONVENTIONAL-SEISMIC","GRVL-SAND-PIT","PIPELINE",
                                           "RIS-RECLAIMED-PERMANENT","RIS-SOIL-REPLACED","WELL-ABAND","WELL-OIL"), "Industrial",
                                       ifelse(sta$Human.Footprint.Type %in% c("CROP","CULTIVATION_ABANDONED","ROUGH_PASTURE", "TAME_PASTURE"), "Agriculture",
                                              ifelse(sta$Human.Footprint.Type %in% c("CLEARING-UNKNOWN", "GREENSPACE","RECREATION", "FACILITY-UNKNOWN"), "Greenspace" ,
                                                     ifelse(sta$Human.Footprint.Type %in% c("HARVEST-AREA"), "Forestry",
                                                            ifelse(sta$Human.Footprint.Type %in% c("TRAIL", "VEGETATED-EDGE-ROADS"), "Transportation",
                                                                   ifelse(sta$Human.Footprint.Type %in% c("RURAL-RESIDENCE", "URBAN-RESIDENCE"), "Residence", NA))))))

sta$Human.Footprint.Sublayer <- case_when(sta$Human.Footprint.Distance > 100 ~ "No Human Footprint", 
                            TRUE ~ as.character(sta$Human.Footprint.Sublayer))

sta %>% count(Human.Footprint.Sublayer)
as.data.frame(sta %>% group_by(Human.Footprint.Sublayer) %>% count(Human.Footprint.Type))
summary(sta$Human.Footprint.Distance) # 94 are within 100 m of site, so let's change to log for ordinal scale
sta$Human.Footprint.Distance.log <- log(sta$Human.Footprint.Distance+1)
summary(sta$Human.Footprint.Distance.log) # now values range from 0-6.9
# recode distance to ordinal scale 1-10 using log transformed distance values
sta$Human.Footprint.Distance.Ord <- ifelse(sta$Human.Footprint.Distance.log<1, 1, 
                                ifelse(sta$Human.Footprint.Distance.log<2, 2,
                                       ifelse(sta$Human.Footprint.Distance.log<3, 3,
                                              ifelse(sta$Human.Footprint.Distance.log<4, 4,
                                                     ifelse(sta$Human.Footprint.Distance.log<5, 5,
                                                            ifelse(sta$Human.Footprint.Distance.log<6, 6,7))))))

sta %>% group_by(Human.Footprint.Distance.Ord) %>% summarise(min(Human.Footprint.Distance), max(Human.Footprint.Distance))

# now have ordinal scales for road distance, road type, water distance, water type and human footprint distance
# categorical scale for human footprint (no footprint will be reference value)
# other categorical scale covariates = NP, Natural Region, Land.Use.Type
# final station covariates = Latitude and Longitude
# temporal covariates = Year, Volancy
# environmental covariates = max temp, max humidity

# Need to determine lenght of surveys (effort) per location as not always what it says in eff df
dat %>% group_by(Deployment.ID) %>% summarise_at(c("SurveyNight"), list(min, mean, max))
# looks surveys started by June 9 (or earlier) most years, surveys ended by end of July or later
# go with June 9/10 - July 31 as the date range for HMSC
dat %>% group_by(Deployment.ID) %>% summarise_at(c("jDay"), list(min, mean, max))
# July 30 in julian day can be 211 or 212, go with 211 as some years only go till 211
# June 10 in julian day can be 161 or 162 depending on leap year so go with 162 to make even 7 weeks for weekly observations
# in the weekly obs, volancy will be considered as the start of week 28 (i.e., 190 days)
week("2020-07-10") # week 28 for all years (2015-2020)
yday("2020-07-10") # day 192 in 2020

dat <- dat %>% filter(between(jDay, 162, 211))

call_effort <- dat %>% group_by(Location.Name, Deployment.ID) %>% summarise_at(c("SurveyNight"), list(Min = min, Max=max))
call_effort$Location.Name.Year <- as.factor(paste(call_effort$Location.Name, call_effort$Deployment.ID,sep="_"))

call_effort_daily <- call_effort %>%
     # sequence of daily dates for each corresponding start, end elements
     transmute(Location.Name.Year, Date = map2(Min, Max, seq, by = "1 day")) %>%
     # unnest the list column
     unnest %>% 
     # remove any duplicate rows
     distinct

call_effort_daily$Year <- year(call_effort_daily$Date)
call_effort_daily %>% group_by(Year) %>% summarise(min(Date), max(Date))
call_effort_daily %>% group_by(Year) %>% count(Date)
call_effort_daily <- call_effort_daily %>% select(-Location.Name.Year)

##################################################################### 
# Build the analysis data      ######################################
#####################################################################

# Y data          

# Daily
  # Station / Date / Effort / Species      
    tmp <- call_effort_daily
    daily.obs <- tmp %>% 
      group_by(Location.Name,Date ) %>%
      summarise(Effort = n())
    
    daily.obs <- as.data.frame(daily.obs[!is.na(daily.obs$Date),]) # removing any NAs
    
    daily.obs[, levels(dat$Species)] <- NA
    i <-1
        for(i in 1:nrow(daily.obs)){
      tmp <- dat[dat$Location.Name==daily.obs$Location.Name[i] & dat$SurveyNight== daily.obs$Date[i],]
      for(j in 1:length(levels(dat$Species))){
        daily.obs[i,levels(dat$Species)[j]] <- length(tmp$Species[tmp$Species==levels(dat$Species)[j]])
      }
    }
        

# Weekly
  # Station / Month / Effort / Covariates / Species  
    dat$Location.Name_Yr <- paste(dat$Location.Name, dat$Year, sep="_")
    
    tmp <- call_effort_daily
    tmp$Week <- week(tmp$Date) 
    as.data.frame(tmp %>% group_by(Year, Week) %>% filter(Week == 24 |Week == 31) %>% summarise(min(Date), max(Date))) 
    # week 31 starts on July 29/30 so remove from df
    week.obs <- tmp %>% 
      filter(Week != 31) %>%
      group_by(Year, Location.Name, Week) %>%
      summarise(Effort = n())
    
    week.obs <- as.data.frame(week.obs)
    week.obs$Location.Name_Yr <- paste(week.obs$Location.Name, week.obs$Year, sep="_")
    
    week.obs[, levels(dat$Species)] <- NA
    i <-1
    for(i in 1:nrow(week.obs))
    {
      tmp <- dat[dat$Location.Name_Yr==week.obs$Location.Name_Yr[i] & week(dat$SurveyNight)== week.obs$Week[i],]
      for(j in 1:length(levels(dat$Species)))
      {
        week.obs[i,levels(dat$Species)[j]] <- length(tmp$Species[tmp$Species==levels(dat$Species)[j]])
      }
    }
 
# Save the Y data
write.csv(daily.obs,"Output/daily_observations.csv", row.names = F )    
write.csv(week.obs %>% select(-Location.Name_Yr),"Output/week_observations.csv", row.names = F )    
write.csv(call_effort_daily,"Output/row_lookup.csv", row.names = F )    

# Make the Y matrix
# week
# add in temporal covariates (Volancy only as Year already included); volancy starts in week 28
week.obs$Volancy <- as.factor(ifelse(week.obs$Week<28, "Pre-volancy","Post-volancy"))
week.obs %>% group_by(Week) %>% count(Volancy)

# Make the Y Matrix
Y <- week.obs
row.names(Y) <- paste(as.character(week.obs$Location.Name),week.obs$Week,week.obs$Year,sep="_") 
head(Y)

# clean up the matrix
Y$Location.Name <- NULL; Y$Effort <- NULL; Y$Location.Name_Yr <- NULL; Y$Year <- NULL ; Y$Volancy <- NULL; Y$Week <- NULL
head(Y)

Y <- as.matrix(Y)

# Make the XData
sta.week  <- week.obs[,c("Location.Name", "Week","Year", "Volancy","Effort")]
head(sta.week)
sta.week <- left_join(sta %>% select("Location.Name","NP","Natural.Region","Land.Unit.Code","Land.Use.Type","Latitude","Longitude","Road.Type.Ord","Road.Distance.Ord","Water.Type.Ord","Water.Distance.Ord","Human.Footprint.Sublayer","Human.Footprint.Distance.Ord"), sta.week)

XData <- data.frame(sta.week)
# remove the two surveys done outside the temporal window: 171651_NW_01 and 171651_NW_02
XData <- XData %>% filter(Location.Name!="171651_NW_01" & Location.Name!="171651_NW_02")

# Responses
tmp <- cor(Y)
corrplot.mixed(tmp)

# Predictors
tmp <- cor(XData[,c("Latitude","Longitude","Road.Type.Ord","Road.Distance.Ord","Water.Type.Ord","Water.Distance.Ord","Human.Footprint.Distance.Ord")])
corrplot.mixed(tmp)

nChains   = 2 # Ultimately use 4
thin      = 100 # build up from 100
samples   = 100 # #1000
transient = 100*thin
verbose   = T


# Add a station-level random effect (for the covariances)
studyDesign = data.frame(station = as.character(week.obs$Location.Name))

rL = HmscRandomLevel(units = studyDesign$station)

# Added poisson distributed

# No offline - Season as NDVI
m = Hmsc(Y = Y, XData = XData, XFormula = ~Year + Volancy + Effort +
           NP + Natural.Region + Land.Unit.Code + Land.Use.Type + Human.Footprint.Sublayer +
           Road.Type.Ord + Water.Type.Ord + Road.Distance.Ord + Water.Distance.Ord + Human.Footprint.Distance.Ord,
         studyDesign = studyDesign, ranLevels = list(station = rL), distr="poisson")

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
               nChains = nChains, verbose = verbose, nParallel = 2)


saveRDS(m,file="Output/MultiSpeciesHMSC.RData") 
#m <- readRDS("Output/MultiSpeciesHMSC.RData")

```

