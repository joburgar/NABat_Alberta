---
title: "NABat Collate and Summarise Data"
author: "Joanna Burgar"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  word_document: default
editor_options:
  chunk_output_type: console
always_allow_html: yes
---

```{r setup, echo=F, include=F}

#Load Packages
list.of.packages <- c("leaflet", "tidyverse", "lunar", "zoo", "colortools", "mapview", "data.table", "fs", "lubridate")
# Check you have them and load them
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

lapply(list.of.packages, require, character.only = TRUE)

# Timezone [Use UTC if your cameras do not correct for daylight saving time, if they do use the timezone where the data was collected]
tz <- "UTC"

# Set a single catagorical variable of interest from station covariates (`sta`) for summary graphs. If you do not have and appropriate category use "Project.ID".
category <- "PA"

# Define a colour from the R options to base the colourscheme
colour <- "lightseagreen"

# # Define the GRTS.Cell.ID and Year of interest if subsetting for year, studyarea
# GRTS_interest <- "922"
# Year_interest <- as.Date(2018)

## README FIRST ##
#Read and run this chunk of code line by line - there are some question below which you will have to answer/ logic tests to complete. Once you are happy with this, hit 'knit' above. 

# Create vector, and then dataframe, of processed data file names - not sure I need this...
# procdata.names <- list.files(path="./Input/NABat_ProcessedFiles", recursive = TRUE)
# 
# Proc.df <- as.data.frame(procdata.names)
# head(Proc.df)
# nrow(Proc.df)
# 
# # Populate dataframe with data structure as columns
# # Need GRTS.Cell.ID, Location.Name, Deployment.ID, Filename 
# Proc.df$GRTS.Cell.ID <- word(Proc.df$procdata.names,1,sep = "\\/")
# Proc.df$Location.Name <- word(Proc.df$procdata.names,2,sep = "\\/")
# Proc.df$Deployment.ID <- word(Proc.df$procdata.names,3,sep = "\\/")
# Proc.df$Filename <- word(Proc.df$procdata.names,-1,sep = "\\/")

# Import csv output files for all cells GRTS.Cell.ID of interest
fs::dir_ls(path="./Input/NABat_ProcessedFiles", recurse = TRUE)

# Import count files
dat_count <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles/"), 
                          regexp = "\\counts.csv$", recurse = TRUE) %>%
  map_dfr(read_csv, col_types = cols(.default = 'c'), .id = "source") %>% 
  type_convert() %>% 
  mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
  mutate(Year = year(SurveyNight), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-Date)

# # Import count files if selecting only GRTS.Cell.ID of interest
# make the changes below to add in GRTS_interest and/or Year_interest for other files as appropriate
# dat_count <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles",GRTS_interest,sep="/"), 
#                           regexp = "\\counts.csv$", recurse = TRUE) %>%
#   map_dfr(read_csv, .id = "source") %>% 
#   mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
#   mutate(Year = year(SurveyNight), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
#   filter(Year==Year_interest)

dat_count$Orig.Name <- as.factor(paste(dat_count$Site, dat_count$Detector, sep="_"))
dat_count$GRTS.Cell.ID <- as.factor(word(dat_count$source,4,sep = "\\/"))
dat_count$Location.Name <- as.factor(word(dat_count$source,5,sep = "\\/"))
dat_count$Deployment.ID <- as.factor(word(dat_count$source,6,sep = "\\/"))
dat_count$Classification <- as.factor(dat_count$Classification %>% 
                                        recode(EPFU.LANO = "EPFU-LANO", LABO.MYLU = "LABO-MYLU",
                                               My_40k = "Myotis 40k", MYEV.MYSE = "MYEV-MYSE"))
glimpse(dat_count)

# Import summary files
dat_summary <- fs::dir_ls(path=paste("./Input/NABat_ProcessedFiles/"), 
                            regexp = "\\_summary.csv$", recurse = TRUE) %>%
  map_dfr(~read_csv(.x, col_types = cols(.default = "c")), .id="source") %>% # issues with min_wind so read in as character
  type_convert() %>% # convert back to proper formats, min_wind still problematic so only use max and mean wind if available
  mutate(SurveyNight = ymd(Date, truncated = 1)) %>%
  mutate(Year = as.factor(year(SurveyNight)), Month = month(SurveyNight, label = T), jDay = yday(SurveyNight)) %>%
  select(-Date)%>%
  rename(Orig.Name=Site)

dat_summary$GRTS.Cell.ID <- as.factor(word(dat_summary$source,4,sep = "\\/"))
dat_summary$Location.Name <- as.factor(word(dat_summary$source,5,sep = "\\/"))
dat_summary$Deployment.ID <- as.factor(word(dat_summary$source,6,sep = "\\/"))

glimpse(dat_summary)

# Read deployment data csv for station covariates
eff <- read.csv("Input/NABat_Deployment_Data.csv", header=T) %>%
  mutate(Survey.Start.Time = ymd(Survey.Start.Time), Survey.End.Time = ymd(Survey.End.Time))
eff$Survey.End.Time - eff$Survey.Start.Time
glimpse(eff)

eff.names <- eff %>% group_by(Location.Name) %>% count(Orig.Name)
dat.names <- dat_count%>% group_by(Location.Name) %>% count(Orig.Name) 
dat.names.sum <- dat_summary %>% group_by(Location.Name) %>% count(Orig.Name)
names.join <- as.data.frame(full_join(eff.names, dat.names.sum, by="Location.Name"))
names.join[is.na(names.join$n.y),] # missing data for Sulphur 2a and 2b (waiting to hear back from Courtney) and Lineham Trail (no data)

# Read station covariates csv
sta <- read.csv("Input/NABat_Station_Covariates.csv", header=T, na.string=c("","NA",-1)) %>%
  dplyr::select(LandUnitCo, GRTSCellID, LocName, Orig_Name, NABat_Samp, YrsSurveye, LC_class, NSRNAME, NRNAME,
                WTRBODY_TY, DIST_WTRBD, STREAM_TYP, Dist_Strea, RoadType, DistRoad_M, HF_TYPE, Dist_to_HF, Latitude, Longitude) %>%
  rename(Land.Unit.Code=LandUnitCo, GRTS.Cell.ID=GRTSCellID, Location.Name=LocName, Orig.Name=Orig_Name, NABat.Sample=NABat_Samp,
         Yrs.Surveyed=YrsSurveye, Land.Cover=LC_class, Natural.Sub.Region=NSRNAME, Natural.Region=NRNAME, Waterbody.Type=WTRBODY_TY,          Waterbody.Distance=DIST_WTRBD, Stream.Type=STREAM_TYP, Stream.Distance=Dist_Strea, Road.Type=RoadType, Road.Distance=DistRoad_M,
         Human.Footprint.Type=HF_TYPE, Human.Footprint.Distance=Dist_to_HF)
sta$PA <- as.factor(ifelse(sta$Land.Unit.Code %in% c("BNP", "JNP", "WBNP", "WLNP", "EINP"), "In", "Out")) %>% relevel(ref="In")

sta$Land.Cover <- as.factor(sta$Land.Cover)
levels(sta$Land.Cover)
sta$Land.Cover <- sta$Land.Cover %>% recode("20"="Water", "33"="Exposed Land", "34"="Developed", "50"="Shrubland", "110"="Grassland",
                                            "120"="Agriculture", "210"="Coniferous Forest", "220"="Broadleaf Forest", "230"="Mixed Forest")

# Land-Use Type (Agriculture, Barren Land, Forest, Grassland, Shrubland, Urban, Water, Wetland)
# not enough Urban, keep as "Developed"
# not enough Wetland for separate category - combine Water and Wetland
# combine 3 forest types for Forest
# if waterbody < 10 m, then change Land Use Type to Water, otherwise go with ABMI class
# Consolidate Land.Cover to Agriculture, Barren (Exposed) Land, Forest, Grassland, Shrubland, Developed (incl Urban), Water/Wetland 
sta$Land.Use.Type <- sta$Land.Cover %>% recode("Coniferous Forest" = "Forest", "Broadleaf Forest" = "Forest", "Mixed Forest" = "Forest",
                                               "Exposed Land" = "Barren Land")
sta$Land.Use.Type <- case_when(sta$Waterbody.Distance < 10 ~ "Water", TRUE ~ as.character(sta$Land.Use.Type))
glimpse(sta)

##############################################################
##### DATA TESTS #############################################
##############################################################

# This code will not work unless your data passes the following checks

# 1) All dates must be in YYYY-MM-DD in 'eff' and YYYY-MM-DD HH:MM:SS in 'dat' 
# If either of the following return NA, you must change your formatting
strptime(eff$Survey.Start.Time[1], "%Y-%m-%d", tz="UTC")
strptime(dat_count$SurveyNight[1], "%Y-%m-%d", tz="UTC")
strptime(dat_summary$SurveyNight[1], "%Y-%m-%d", tz="UTC")

# 2) ensure all deployment starting dates are before deployment retreival dates
# Logic = are the stations active for 0 or more days -> all should read TRUE
table((strptime(eff$Survey.End.Time, "%Y-%m-%d", tz="UTC")-strptime(eff$Survey.Start.Time, "%Y-%m-%d", tz="UTC"))>=0)

# 3a) Do you have all stations represented in both the deployment data and station covariates? If yes, the value should be 0
# If length > 0, then you have some data missing!
length(setdiff(unique(eff$Location.Name), unique(sta$Location.Name))) # 0
# missing.eff.sta <- left_join(eff, sta, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
# missing.eff.sta[is.na(missing.eff.sta$Orig.Name.y),] # will show missing station covariates

# 3b) Do you have all data represented in the station covariates? If yes, the value should be 0
length(setdiff(unique(sta$Location.Name), unique(dat_count$Location.Name))) # 12
missing.dat.sta <- left_join(sta, dat_count, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.dat.sta[is.na(missing.dat.sta$Orig.Name.y),] # missing Sulphur 2a, Sulphur 2b, Lineham Trail (no actual data), and ABMI data

# 3c) Do you have all data represented in the deployment data? If yes, the value should be 0
length(setdiff(unique(eff$Location.Name), unique(dat_count$Location.Name))) # 3
missing.dat.eff <- left_join(eff, dat_count, by="Location.Name") %>% dplyr::select(Location.Name, Orig.Name.x, Orig.Name.y)
missing.dat.eff[is.na(missing.dat.eff$Orig.Name.y),] # missing the Sulphur 2a, 2b and Lineham data

# Need Sulphur data from Courtney and ABMI data from Cami - the Lineham Trail does not have data recorded (check with Helena)
# EINP can be included in the NABat 2020 report but does not include station covariates (no info from Rhonda), just PA, NR, NSR groupings


# If all of the above is satisfied -> press 'Knit' above ^
```

```{r non-adjustable options, echo=F, include=F}

# Count the total number of detector stations
n.stat <- length(unique(eff$Location.Name))
n.GRTS <- length(unique(eff$GRTS.Cell.ID))

# Generate colours to display the catagory levels - R needs them as a factor
sta[,category] <- factor(sta[,category])
col.cat <- wheel(colour, num = length(levels(sta[,category])))
sta$Cols <- col.cat[sta[,category]]

# Code to determine how large the figures will be (we need larger figures if we have more GRTS Cell locations)
eff.height <- 8
if(length(unique(eff$Location.Name))>80)
   {
     eff.height <- length(unique(eff$Location.Name))/10
   }

sp.height <- 7
if(length(unique(dat_count$Classification))>20)
   {
     sp.height <- 7+(length(unique(dat_count$Classification))/8)
   }

```

## NABat Grid Cell `r eff$GRTS.Cell.ID[1]` Surveys

### ARU locations

To date there have been passive ARU deployments at `r n.stat` unique locations within NABat Grid Cell `r GRTS_interest`.

```{r map, echo=F}


m <- leaflet() %>%
  addProviderTiles(providers$Esri.WorldImagery, group="Satellite") %>%  # Add satellite data
  addProviderTiles(providers$Esri.WorldTopoMap, group="Base") %>%     
  addCircleMarkers(lng=sta$Longitude, lat=sta$Latitude,
                   color=sta$Cols,
                   popup=paste(sta$Deployment.ID, sta[,category])) %>%
 addLegend("bottomleft", colors = col.cat,  labels = levels(sta[,category]),
    title = category,
    labFormat = labelFormat(prefix = "$"),
    opacity = 1
  ) %>%
  # Layers control
  addLayersControl(
    baseGroups = c("Satellite", "Base"),
    options = layersControlOptions(collapsed = FALSE)
  )
m


```
### ARU activity through time

# Table 1. ARU Recordings Summary
```{r recording summary, echo=T}
### add in SE function
se <- function(x) sd(x)/sqrt(length(x))

cdata <- left_join(dat_count, sta %>% dplyr::select(Location.Name, Land.Unit.Code, Orig.Name, NABat.Sample))

rec.days <- cdata %>% group_by(Land.Unit.Code, GRTS.Cell.ID, Location.Name) %>% 
  summarise(Nights.Surveyed = max(SurveyNight)-min(SurveyNight)+1)

rec.days %>% summarise_at(vars(Nights.Surveyed), list(Total = sum, Min = min, Mean = mean, Max = max, SE = se))
```

The break down of ARU activity is as follows:

```{r activity, echo=F, fig.height=eff.height}

glimpse(eff) # check for correct loading of deployment data
eff %>% dplyr::select(-Detector.Failure.Details, -Contact)
summary(eff) # overview of data and check for NAs

# Adjust layout
par(mar=c(2,6,1,1))
plot(c(min(eff$Survey.Start.Time, na.rm=T), max(eff$Survey.End.Time, na.rm=T)),
     c(1,n.stat), las=1, ylab="", xlab="", type="n", yaxt="n")

axis(2, at= 1:n.stat, labels= unique(eff$Location.Name), las=1, cex.axis=0.8)
mtext("ARU Station", 2, 4)
# Make lines for each of the cameras
for(i in 1:length(unique(eff$Location.Name)))
{
  abline(h=i, col=rgb(0,0,0,0.1))
  tmp <- eff[eff$Location.Name==unique(eff$Location.Name)[i],]
  for(j in 1:nrow(tmp))
    {
      lines(c(tmp$Survey.Start.Time[j],
                       tmp$Survey.End.Time[j]),
            c(i,i), lwd=2)
    }
  
}

```
#Figure 2: Where black lines denote an ARU which is active, white space indicates ARUs which are inactive. 

```{r, include=F}
# Make species colour codes
tmp3 <- data.frame("Species"=unique(dat_count$Classification),"Colour"= wheel("lightseagreen", num = length(unique(dat_count$Classification))))

```

# Table 2. Bat Call Summary
```{r call summary, echo=T}

cdata %>% group_by(Land.Unit.Code, GRTS.Cell.ID, Location.Name) %>% 
                filter(Classification!="noise") %>%
                summarise(Recording.Started = min(SurveyNight), Recording.Ended = max(SurveyNight), 
                          Count.Bat.Calls = sum(Count),
                          Bat.Calls.Per.Night = sum(Count)/as.numeric((max(SurveyNight)-min(SurveyNight))))

```

The `r n.stat` stations have resulted in a total of `r cdata %>% filter(Classification!="noise") %>% summarise(sum(Count))` bat call sequences and `r cdata %>% filter(Classification=="noise") %>% summarise(sum(Count))` noise sequences. 

# Table 3. Total Bat Call Counts by Species / Species Groups and Land Unit
```{r species summary, echo=T}

cdata %>% group_by(Classification, Land.Unit.Code) %>% summarise(sum(Count))
```


